# scEvalðŸ˜ˆ: A evaluation platform for single-cell Foundation Models (FMs)

This is the repo for our benchmarking and analysis project. All methods are collected until March 1st, 2024. 

# Install

To install our benchmarking environment based on [scGPT](https://scgpt.readthedocs.io/en/latest/), please use conda to create a environment based on this yml file in your own machine:
```
conda env create -n scgpt --file scgpt_bench.yml
```

For other methods we used, please refer their original project website for instruction. We recommend creating different environment for different methods.

These methods include: 

[tGPT](https://github.com/deeplearningplus/tGPT), [Geneformer](https://huggingface.co/ctheodoris/Geneformer), [scBERT](https://github.com/TencentAILabHealthcare/scBERT), [CellLM](https://github.com/BioFM/OpenBioMed/tree/main), [SCimilarity](https://github.com/Genentech/scimilarity), [scFoundation](https://github.com/biomap-research/scFoundation), [CellPLM](https://github.com/OmicsML/CellPLM), [UCE](https://github.com/snap-stanford/UCE). These are also single-cell FMs.

And

[TOSICA](https://github.com/JackieHanLab/TOSICA/tree/main), [scJoint](https://github.com/SydneyBioX/scJoint), [GLUE](https://github.com/gao-lab/GLUE), [ResPAN](https://github.com/AprilYuge/ResPAN/tree/main), [Harmony](https://scanpy.readthedocs.io/en/stable/generated/scanpy.external.pp.harmony_integrate.html), [scDesign3](https://github.com/SONGDONGYUAN1994/scDesign3), [Splatter](https://github.com/Oshlack/splatter), [scVI](https://scvi-tools.org/), [Tangram](https://github.com/broadinstitute/Tangram), [GEARS](https://github.com/snap-stanford/GEARS). These are task-specific models.


We need scIB for evaluation. Please use pip to install it:
```
pip install scib
```
We also provide a scib version with our new function in this repo. Please make sure you have **scib >=1.0.4** to run kBET correclty.

We will release a version of sceval with more functions in the future!


# Pre-training weights

Most of our experiments were finished based on weights under [scGPT_bc](https://drive.google.com/drive/folders/1S9B2QUvBAh_FxUNrWrLfsvsds1thF9ad?usp=share_link). [scGPT_full](https://drive.google.com/drive/folders/1eNdHu45uXDHOF4u0J1sYiBLZYN55yytS?usp=share_link) from scGPT v2 was also used in the batch effect correction evaluation.

Pre-training weights of scBERT can be found in [scBERT](https://github.com/TencentAILabHealthcare/scBERT). Pre-training weights of CellLM can be found in [cellLM](https://github.com/BioFM/OpenBioMed/tree/main). Pre-training weights of Geneformer can be found in [Geneformer](https://huggingface.co/ctheodoris/Geneformer). Pre-training weights of SCimilarity can be found in [SCimilarity](https://github.com/Genentech/scimilarity).

scFoundation relies on the APIs for access, please refer [scFoundation](https://github.com/biomap-research/scFoundation) for details.

# Benchmarking information

Please refer different folders for the codes of scEval and metrics we used to evaluate single-cell LLMs under different tasks. In general, we list the tasks and corresponding metrics here:

| Tasks                                                 | Metrics                                  |
|-------------------------------------------------------|------------------------------------------|
| Batch Effect Correction, Multi-omics Data Integration |
| and Simulation                                        | [scIB](https://github.com/theislab/scib)                                     |
| Cell-type Annotation and Gene Function Prediction     | Accuracy, Precision, Recall and F1 score |
| Imputation                                            | [scIB](https://github.com/theislab/scib), Correlation                        |
| Perturbation Prediction                               | Correlation                              |
| Gene Network Analysis                                 | Jaccard similarity                       |

The file 'sceval_lib.py' includes all of the metrics we used in this project.

To run the codes in different tasks, please use (we choose batch effect correction as an example here):

```
python sceval_batcheffect.py
```

We offer demo datasets for batch effect correction and cell type annotation. Such datasets can be found [here](https://drive.google.com/drive/folders/1YvBQ44H_jzhS8B35mPjpCMwQserLLhZs?usp=sharing).

To avoid using wandb, please set:

```
os.environ["WANDB_MODE"] = "offline"
```

# Results

We have an official website as the summary of our work. Please use this [link](https://sites.google.com/yale.edu/sceval) for access. 

# Contact

Please contact tianyu.liu@yale.edu if you have any questions about this project.

# Citation

```
@article{liu2023evaluating,
  title={Evaluating the Utilities of Foundation Models in Single-cell Data Analysis},
  author={Liu, Tianyu and Li, Kexing and Wang, Yuge and Li, Hongyu and Zhao, Hongyu},
  journal={bioRxiv},
  pages={2023--09},
  year={2023},
  publisher={Cold Spring Harbor Laboratory}
}
```