# scEvalðŸ˜ˆ: An evaluation platform for single-cell Foundation Models (FMs)

This is the repo for our benchmarking and analysis project. All methods are collected until March 1st, 2024. 

# Install

To install our benchmarking environment based on [scGPT](https://scgpt.readthedocs.io/en/latest/), please use conda to create an environment based on this yml file in your own machine:
```
conda env create -n scgpt --file scgpt_bench.yml
```

If you face any issues due to version conflicts, you can try to comment the problematic packages and try:

```
conda activate scgpt
conda env update --file scgpt_bench.yml
```

For other methods we used, please refer to their original project website for instructions. We recommend creating different environments for different methods. Considering the difficulties of installing different scFMs, we provide a list of yml files we used to install these models in the folder **installation_baselines**.

These methods include: 

[tGPT](https://github.com/deeplearningplus/tGPT), [Geneformer](https://huggingface.co/ctheodoris/Geneformer), [scBERT](https://github.com/TencentAILabHealthcare/scBERT), [CellLM](https://github.com/BioFM/OpenBioMed/tree/main), [SCimilarity](https://github.com/Genentech/scimilarity), [scFoundation](https://github.com/biomap-research/scFoundation), [CellPLM](https://github.com/OmicsML/CellPLM), [UCE](https://github.com/snap-stanford/UCE), [GeneCompass](https://github.com/xCompass-AI/GeneCompass/tree/main). These are also single-cell FMs.

And

[TOSICA](https://github.com/JackieHanLab/TOSICA/tree/main), [scJoint](https://github.com/SydneyBioX/scJoint), [GLUE](https://github.com/gao-lab/GLUE), [ResPAN](https://github.com/AprilYuge/ResPAN/tree/main), [Harmony](https://scanpy.readthedocs.io/en/stable/generated/scanpy.external.pp.harmony_integrate.html), [scDesign3](https://github.com/SONGDONGYUAN1994/scDesign3), [Splatter](https://github.com/Oshlack/splatter), [scVI](https://scvi-tools.org/), [Tangram](https://github.com/broadinstitute/Tangram), [GEARS](https://github.com/snap-stanford/GEARS). These are task-specific models.


We need scIB for evaluation. Please use pip to install it:
```
pip install scib
```
We also provide a scib version with our new function in this repo. Please make sure you have **scib >=1.0.4** to run kBET correctly.

We will release a version of scEval with more functions in the future!


# Pre-training weights

Most of our experiments were finished based on weights under [scGPT_bc](https://drive.google.com/drive/folders/1S9B2QUvBAh_FxUNrWrLfsvsds1thF9ad?usp=share_link). [scGPT_full](https://drive.google.com/drive/folders/1eNdHu45uXDHOF4u0J1sYiBLZYN55yytS?usp=share_link) from scGPT v2 was also used in the batch effect correction evaluation. Pre-training weights of scBERT can be found in [scBERT](https://github.com/TencentAILabHealthcare/scBERT). Pre-training weights of CellLM can be found in [cellLM](https://github.com/BioFM/OpenBioMed/tree/main). Pre-training weights of Geneformer can be found in [Geneformer](https://huggingface.co/ctheodoris/Geneformer). Pre-training weights of SCimilarity can be found in [SCimilarity](https://github.com/Genentech/scimilarity). Pre-training weights of UCE can be found in [UCE](https://github.com/snap-stanford/UCE). Pre-training weights of tGPT can be found in [tGPT](https://github.com/deeplearningplus/tGPT). Pre-training weights of CellPLM can be found in [CellPLM](https://github.com/OmicsML/CellPLM). 

scFoundation relies on the APIs or local sever for access, please refer [scFoundation](https://github.com/biomap-research/scFoundation) for details. Details of GeneCompas can be found in [GeneCompass](https://github.com/xCompass-AI/GeneCompass/tree/main)

# Benchmarking information

Please refer to different folders for the codes of scEval and metrics we used to evaluate single-cell LLMs under different tasks. In general, we list the tasks and corresponding metrics here:

| Tasks                                                 | Metrics                                  |
|-------------------------------------------------------|------------------------------------------|
| Batch Effect Correction, Multi-omics Data Integration |
| and Simulation                                        | [scIB](https://github.com/theislab/scib)                                     |
| Cell-type Annotation and Gene Function Prediction     | Accuracy, Precision, Recall and F1 score |
| Imputation                                            | [scIB](https://github.com/theislab/scib), Correlation                        |
| Perturbation Prediction                               | Correlation                              |
| Gene Network Analysis                                 | Jaccard similarity                       |

The file 'sceval_lib.py' includes all of the metrics we used in this project.

To run the codes in different tasks, please use (we choose batch effect correction of scGPT as an example here):

```
python sceval_batcheffect.py
```

We recommend directly evaluating the methods based on their outputs (as .h5ad file), which can be easily performed based on the codes in **sceval_method.py**.

We offer demo datasets for batch effect correction and cell type annotation. Such datasets can be found [here](https://drive.google.com/drive/folders/1YvBQ44H_jzhS8B35mPjpCMwQserLLhZs?usp=sharing).

To avoid using wandb, please set:

```
os.environ["WANDB_MODE"] = "offline"

```

We will upload our codes for benchmarking different foundation models soon.

# Devices

We recommend using sever to run benchmarked methods and scEval platform. To run single-cell Foundation Models, GPU cores (A100 or higher version) and 40+ GB memory are required. To run scEval (only the evaluation), 40+ GB memory is recommended.

# Results

We have an official website as the summary of our work. Please use this [link](https://sites.google.com/yale.edu/sceval) for access. 

# Contact

Please contact tianyu.liu@yale.edu if you have any questions about this project.

# Citation

```
@article{liu2023evaluating,
  title={Evaluating the Utilities of Foundation Models in Single-cell Data Analysis},
  author={Liu, Tianyu and Li, Kexing and Wang, Yuge and Li, Hongyu and Zhao, Hongyu},
  journal={bioRxiv},
  pages={2023--09},
  year={2023},
  publisher={Cold Spring Harbor Laboratory}
}
```