{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aad430d-7712-41fd-8c98-2c0433a2b971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/notebooks/lib/python3.9/site-packages/scanpy/_settings.py:447: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  IPython.display.set_matplotlib_formats(*ipython_format)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "/notebooks/lib/python3.9/site-packages/wandb/sdk/lib/ipython.py:70: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/scGPT/examples/wandb/run-20230629_172004-ect59rr5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zhao-lab/scGPT/runs/ect59rr5' target=\"_blank\">serene-puddle-597</a></strong> to <a href='https://wandb.ai/zhao-lab/scGPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zhao-lab/scGPT' target=\"_blank\">https://wandb.ai/zhao-lab/scGPT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zhao-lab/scGPT/runs/ect59rr5' target=\"_blank\">https://wandb.ai/zhao-lab/scGPT/runs/ect59rr5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 42, 'dataset_name': 'PBMC_10K', 'do_train': True, 'load_model': 'save/scGPT_bc', 'mask_ratio': 0.4, 'epochs': 10, 'n_bins': 51, 'GEPC': True, 'ecs_thres': 0.8, 'dab_weight': 1.0, 'lr': 0.0001, 'batch_size': 32, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'log_interval': 100, 'fast_transformer': True, 'pre_norm': False, 'amp': True}\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import numpy as np\n",
    "import wandb\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerModel, AdversarialDiscriminator\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    "    masked_ce_loss,\n",
    "    perturb_mse_loss\n",
    ")\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt import SubsetsBatchSampler\n",
    "from scgpt.utils import set_seed, category_str2int, eval_scib_metrics\n",
    "\n",
    "# import mmsdistance\n",
    "\n",
    "# from functions_group import DistanceLoss\n",
    "\n",
    "sc.set_figure_params(figsize=(4, 4))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "# os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "\n",
    "# modify original param\n",
    "hyperparameter_defaults = dict(\n",
    "    seed=42,\n",
    "    dataset_name=\"PBMC_10K\",\n",
    "    do_train=True,\n",
    "    load_model=\"save/scGPT_bc\",\n",
    "    mask_ratio=0.4,\n",
    "    epochs=10,\n",
    "    n_bins=51,\n",
    "    GEPC=True,  # Masked value prediction for cell embedding\n",
    "    ecs_thres=0.8,  # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "    dab_weight=1.0,\n",
    "    lr=1e-4,\n",
    "    batch_size=32,\n",
    "    layer_size=128,\n",
    "    nlayers=4,\n",
    "    nhead=4,\n",
    "    # if load model, batch_size, layer_size, nlayers, nhead will be ignored\n",
    "    dropout=0.2,\n",
    "    schedule_ratio=0.9,  # ratio of epochs for learning rate schedule\n",
    "    save_eval_interval=5,\n",
    "    log_interval=100,\n",
    "    fast_transformer=True,\n",
    "    pre_norm=False,\n",
    "    amp=True,  # Automatic Mixed Precision\n",
    ")\n",
    "\n",
    "run = wandb.init(\n",
    "    config=hyperparameter_defaults,\n",
    "    project=\"scGPT\",\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"fork\"),\n",
    ")\n",
    "config = wandb.config\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a21c50-23d4-4b1f-9f89-2f0a8b296b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_selection(adata, key='batch', orders=None):\n",
    "    '''Function used to determine the training sequence based on the variance across genes.\n",
    "        Args:\n",
    "            adata: The given dataset in AnnData form\n",
    "            key: The index name of batch information in the given dataset\n",
    "            orders: The batch sequence for training or none\n",
    "        Output:\n",
    "            orders: The batch sequence for training\n",
    "    '''\n",
    "    if orders == None:\n",
    "        batch_list = list(set(adata.obs[key].values))\n",
    "        adata_values = [np.array(adata.X[adata.obs[key] == batch]) for batch in batch_list]\n",
    "        std_ = [np.sum(np.std(item, axis=0)) for item in adata_values]\n",
    "        orders = np.argsort(std_)[::-1]\n",
    "        return [batch_list[i] for i in orders] \n",
    "    else:\n",
    "        return orders\n",
    "\n",
    "def generate_target_dataset(adata, batch_list):\n",
    "    \"\"\"A simple function used to rearrange the batch index of our given dataset.\n",
    "    \"\"\"\n",
    "    adata0 = adata[adata.obs['batch'] == batch_list[0]]\n",
    "    for i in batch_list[1:]:\n",
    "        adata0 = adata0.concatenate(adata[adata.obs['batch'] == i], batch_key='batch_key', index_unique=None)\n",
    "    return adata0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffad28de-4f21-4a8f-9ae6-31bc0132321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_dataset = \"single_per_d3\" #no mask\n",
    "use_dataset = \"adamson_perturb\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7e2e22-6fb8-4694-9a9b-e0c513048042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ee8ea3d-0f1c-4d96-ab76-e4b7dfab559b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to save/dev_PBMC_10K-Jun29-17-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2306/1085184116.py:56: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"str_batch\"] = adata.obs[ori_batch_col].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 4355/5060 genes in vocabulary of size 36574.\n",
      "scGPT - INFO - Resume model from save/scGPT_bc/best_model.pt, the model args will override the config save/scGPT_bc/args.json.\n"
     ]
    }
   ],
   "source": [
    "set_seed(config.seed)\n",
    "\n",
    "# %%\n",
    "# settings for input and preprocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_ratio = config.mask_ratio\n",
    "mask_value = -1\n",
    "pad_value = -2\n",
    "n_input_bins = config.n_bins\n",
    "\n",
    "n_hvg = 5000  # number of highly variable genes\n",
    "max_seq_len = n_hvg + 1\n",
    "per_seq_batch_sample = False\n",
    "DSBN = True  # Domain-spec batchnorm\n",
    "explicit_zero_prob = True  # whether explicit bernoulli for zeros\n",
    "\n",
    "# %%\n",
    "dataset_name = config.dataset_name\n",
    "save_dir = Path(f\"./save/dev_{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"save to {save_dir}\")\n",
    "# save the whole script to the dir\n",
    "# os.system(f\"cp {__file__} {save_dir}\")\n",
    "\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Loading and preparing data\n",
    "# if dataset_name == \"PBMC_10K\":\n",
    "#     adata = scvi.data.pbmc_dataset()  # 11990 × 3346\n",
    "#     ori_batch_col = \"batch\"\n",
    "#     adata.obs[\"celltype\"] = adata.obs[\"str_labels\"].astype(\"category\")\n",
    "#     adata.var = adata.var.set_index(\"gene_symbols\")\n",
    "#     data_is_raw = True\n",
    "\n",
    "\n",
    "\n",
    "adata = sc.read_h5ad(f\"./save/{use_dataset}.h5ad\")  # 11990 × 3346\n",
    "# adata.var_names = [i.upper() for i in adata.var_names]\n",
    "adata = sc.AnnData(adata.X.todense(), obs = adata.obs, var = adata.var)\n",
    "ori_batch_col = \"batch\"\n",
    "adata.obs['batch'] = ['1' for i in adata.obs_names]\n",
    "adata.obs['celltype'] = ['1' for i in adata.obs_names]\n",
    "\n",
    "data_is_raw = False\n",
    "\n",
    "order = order_selection(adata) \n",
    "adata = generate_target_dataset(adata, order)\n",
    "# adata = adata[[True if i in [\"Batch3_fluidigmc1\", \"Batch5_smartseq2\"] else False for i in adata.obs.batch]]\n",
    "\n",
    "# %%\n",
    "# make the batch category column\n",
    "adata.obs[\"str_batch\"] = adata.obs[ori_batch_col].astype(str)\n",
    "batch_id_labels = adata.obs[\"str_batch\"].astype(\"category\").cat.codes.values\n",
    "adata.obs[\"batch_id\"] = batch_id_labels\n",
    "\n",
    "adata.var[\"gene_name\"] = adata.var.index.tolist()\n",
    "\n",
    "if config.load_model is not None:\n",
    "    model_dir = Path(config.load_model)\n",
    "    model_config_file = model_dir / \"args.json\"\n",
    "    model_file = model_dir / \"best_model.pt\"\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "    # model\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "    logger.info(\n",
    "        f\"Resume model from {model_file}, the model args will override the \"\n",
    "        f\"config {model_config_file}.\"\n",
    "    )\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f26ced4-afa6-4481-8ac3-dbedbe950052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_perturb_train(train_obs_names, valid_obs_names, tok_train_data, tok_valid_data, adata):\n",
    "    \n",
    "    adata_train = adata[train_obs_names,:]\n",
    "\n",
    "    gene_list = list(adata_train.var_names)\n",
    "    for i,name in enumerate(train_obs_names):\n",
    "        genename = adata_train[name,:].obs['true_cond'][0]\n",
    "        \n",
    "        if '+' in genename:\n",
    "            gene_p1, gene_p2 = genename.split('+')[0], genename.split('+')[1]\n",
    "            if gene_p1 in adata_train.var_names:\n",
    "                maskid = gene_list.index(gene_p1)\n",
    "                tok_train_data[i,maskid+1] = mask_value\n",
    "            if gene_p2 in adata_train.var_names:\n",
    "                maskid = gene_list.index(gene_p2)\n",
    "                tok_train_data[i,maskid+1] = mask_value\n",
    "\n",
    "        else:\n",
    "            if genename in adata_train.var_names:\n",
    "                \n",
    "                maskid = gene_list.index(genename)\n",
    "                tok_train_data[i,maskid+1] = mask_value\n",
    "            \n",
    "    adata_valid = adata[valid_obs_names,:]\n",
    "\n",
    "    gene_list = list(adata_valid.var_names)\n",
    "    for i,name in enumerate(valid_obs_names):\n",
    "        genename = adata_valid[name,:].obs['true_cond'][0]\n",
    "        \n",
    "        if '+' in genename:\n",
    "            gene_p1, gene_p2 = genename.split('+')[0], genename.split('+')[1]\n",
    "            if gene_p1 in adata_valid.var_names:\n",
    "                maskid = gene_list.index(gene_p1)\n",
    "                tok_valid_data[i,maskid+1] = mask_value\n",
    "            if gene_p2 in adata_valid.var_names:\n",
    "                maskid = gene_list.index(gene_p2)\n",
    "                tok_valid_data[i,maskid+1] = mask_value\n",
    "        else:\n",
    "            if genename in adata_valid.var_names:\n",
    "                maskid = gene_list.index(genename)\n",
    "                tok_valid_data[i,maskid+1] = mask_value     \n",
    "\n",
    "    return tok_train_data, tok_valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "014ee6c2-cf5a-4656-bf11-2f6a84e9fc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Filtering genes by counts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/lib/python3.9/site-packages/scanpy/preprocessing/_simple.py:249: ImplicitModificationWarning: Trying to modify attribute `.var` of view, initializing view as actual.\n",
      "  adata.var['n_counts'] = number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Filtering cells by counts ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Subsetting highly variable genes ...\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2306/2718812547.py:35: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata_ctrl.obs['true_cond'] = list(adata_pert.obs.true_cond)\n",
      "/notebooks/lib/python3.9/site-packages/anndata/_core/anndata.py:1828: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# set up the preprocessor, use the args to config the workflow\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=3,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=n_hvg,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=config.n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "preprocessor(adata, batch_key=\"batch\" if dataset_name != \"heart_cell\" else None)\n",
    "\n",
    "# %%\n",
    "if per_seq_batch_sample:\n",
    "    # sort the adata by batch_id in advance\n",
    "    adata_sorted = adata[adata.obs[\"batch_id\"].argsort()].copy()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Tokenize input\n",
    "\n",
    "# %%\n",
    "\n",
    "adata_pert = adata[adata.obs.condition != 'ctrl'] \n",
    "adata_ctrl = adata[adata.obs.condition == 'ctrl']\n",
    "\n",
    "paired_ctrl = []\n",
    "for i in adata_pert.obs_names:\n",
    "    paired_ctrl.append(np.random.choice(adata_ctrl.obs_names))\n",
    "adata_ctrl = adata[paired_ctrl,:]\n",
    "adata_ctrl.obs['true_cond'] = list(adata_pert.obs.true_cond)\n",
    "adata_ctrl.obs_names = [str(i) for i in range(len(adata_ctrl))]\n",
    "\n",
    "\n",
    "input_layer_key = \"X_binned\"\n",
    "all_counts_input = (\n",
    "    adata_ctrl.layers[input_layer_key].A\n",
    "    if issparse(adata_ctrl.layers[input_layer_key])\n",
    "    else adata_ctrl.layers[input_layer_key]\n",
    ")\n",
    "\n",
    "all_counts_target = (\n",
    "    adata_pert.layers[input_layer_key].A\n",
    "    if issparse(adata_pert.layers[input_layer_key])\n",
    "    else adata_pert.layers[input_layer_key]\n",
    ")\n",
    "\n",
    "\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "celltypes_labels = adata_ctrl.obs[\"celltype\"].tolist()  # make sure count from 0\n",
    "num_types = len(set(celltypes_labels))\n",
    "celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "batch_ids = adata_ctrl.obs[\"batch_id\"].tolist()\n",
    "num_batch_types = len(set(batch_ids))\n",
    "batch_ids = np.array(batch_ids)\n",
    "\n",
    "(\n",
    "    train_data_input,\n",
    "    valid_data_input,\n",
    "    \n",
    "    train_data_target,\n",
    "    valid_data_target,\n",
    "    \n",
    "    train_celltype_labels,\n",
    "    valid_celltype_labels,\n",
    "    \n",
    "    train_batch_labels,\n",
    "    valid_batch_labels,\n",
    "    \n",
    "    input_train_obs_names,\n",
    "    input_valid_obs_names\n",
    ") = train_test_split(\n",
    "    all_counts_input, all_counts_target, celltypes_labels, batch_ids, adata_ctrl.obs_names, test_size=0.33, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b70d3131-dccf-43cc-a21a-37debca38a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "View of AnnData object with n_obs × n_vars = 29707 × 4345\n",
       "    obs: 'condition', 'cell_type', 'dose_val', 'control', 'condition_name', 'true_cond', 'batch', 'celltype', 'str_batch', 'batch_id', 'n_counts'\n",
       "    var: 'gene_name', 'ensid', 'id_in_vocab', 'n_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection'\n",
       "    uns: 'hvg'\n",
       "    obsm: 'bin_edges'\n",
       "    layers: 'X_normed', 'X_binned'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_ctrl[input_train_obs_names,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e82e25e-63cc-45b7-aac2-e0918c43c58a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3959e13-9ca4-43e5-9234-211c16f6a617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1bb51c4-0af7-4621-a055-a527bf824b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - train set number of samples: 29707, \n",
      "\t feature length: 5001\n",
      "scGPT - INFO - valid set number of samples: 14633, \n",
      "\t feature length: 5001\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "if config.load_model is None:\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(vocab(genes), dtype=int)\n",
    "\n",
    "# %%\n",
    "tokenized_train_input = tokenize_and_pad_batch(\n",
    "    train_data_input,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=True,\n",
    ")\n",
    "\n",
    "\n",
    "tokenized_valid_input = tokenize_and_pad_batch(\n",
    "    valid_data_input,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=True,\n",
    ")\n",
    "\n",
    "tokenized_train_target = tokenize_and_pad_batch(\n",
    "    train_data_target,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=True,\n",
    ")\n",
    "\n",
    "\n",
    "tokenized_valid_target = tokenize_and_pad_batch(\n",
    "    valid_data_target,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "logger.info(\n",
    "    f\"train set number of samples: {tokenized_train_input['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_train_input['genes'].shape[1]}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"valid set number of samples: {tokenized_valid_input['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_valid_input['genes'].shape[1]}\"\n",
    ")\n",
    "\n",
    "train_mask , valid_mask = generate_perturb_train(input_train_obs_names, input_valid_obs_names, tokenized_train_input[\"values\"], tokenized_valid_input[\"values\"], adata_ctrl)\n",
    "\n",
    "# %%\n",
    "def prepare_data(sort_seq_batch=False) -> Tuple[Dict[str, torch.Tensor]]:\n",
    "#     masked_values_train = random_mask_value(\n",
    "#         tokenized_train[\"values\"],\n",
    "#         mask_ratio=mask_ratio,\n",
    "#         mask_value=mask_value,\n",
    "#         pad_value=pad_value,\n",
    "#     )\n",
    "    \n",
    "#     masked_values_valid = random_mask_value(\n",
    "#         tokenized_valid[\"values\"],\n",
    "#         mask_ratio=mask_ratio,\n",
    "#         mask_value=mask_value,\n",
    "#         pad_value=pad_value,\n",
    "#     )\n",
    "    # print(\n",
    "    #     f\"random masking at epoch {epoch:3d}, ratio of masked values in train: \",\n",
    "    #     f\"{(masked_values_train == mask_value).sum() / (masked_values_train - pad_value).count_nonzero():.4f}\",\n",
    "    # )\n",
    "\n",
    "    input_gene_ids_train, input_gene_ids_valid = (\n",
    "        tokenized_train_input[\"genes\"],\n",
    "        tokenized_valid_input[\"genes\"],\n",
    "    )\n",
    "    input_values_train, input_values_valid = train_mask.float() , valid_mask.float()\n",
    "    target_values_train, target_values_valid = (\n",
    "        tokenized_train_target[\"values\"],\n",
    "        tokenized_valid_target[\"values\"],\n",
    "    )\n",
    "\n",
    "    tensor_batch_labels_train = torch.from_numpy(train_batch_labels).long()\n",
    "    tensor_batch_labels_valid = torch.from_numpy(valid_batch_labels).long()\n",
    "\n",
    "    if sort_seq_batch:\n",
    "        train_sort_ids = np.argsort(train_batch_labels)\n",
    "        input_gene_ids_train = input_gene_ids_train[train_sort_ids]\n",
    "        input_values_train = input_values_train[train_sort_ids]\n",
    "        target_values_train = target_values_train[train_sort_ids]\n",
    "        tensor_batch_labels_train = tensor_batch_labels_train[train_sort_ids]\n",
    "\n",
    "        valid_sort_ids = np.argsort(valid_batch_labels)\n",
    "        input_gene_ids_valid = input_gene_ids_valid[valid_sort_ids]\n",
    "        input_values_valid = input_values_valid[valid_sort_ids]\n",
    "        target_values_valid = target_values_valid[valid_sort_ids]\n",
    "        tensor_batch_labels_valid = tensor_batch_labels_valid[valid_sort_ids]\n",
    "\n",
    "    train_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_train,\n",
    "        \"values\": input_values_train,\n",
    "        \"target_values\": target_values_train,\n",
    "        \"batch_labels\": tensor_batch_labels_train,\n",
    "    }\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_valid,\n",
    "        \"values\": input_values_valid,\n",
    "        \"target_values\": target_values_valid,\n",
    "        \"batch_labels\": tensor_batch_labels_valid,\n",
    "    }\n",
    "\n",
    "    return train_data_pt, valid_data_pt\n",
    "\n",
    "\n",
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    intra_domain_shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    if per_seq_batch_sample:\n",
    "        # find the indices of samples in each seq batch\n",
    "        subsets = []\n",
    "        batch_labels_array = data_pt[\"batch_labels\"].numpy()\n",
    "        for batch_label in np.unique(batch_labels_array):\n",
    "            batch_indices = np.where(batch_labels_array == batch_label)[0].tolist()\n",
    "            subsets.append(batch_indices)\n",
    "        data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_sampler=SubsetsBatchSampler(\n",
    "                subsets,\n",
    "                batch_size,\n",
    "                intra_subset_shuffle=intra_domain_shuffle,\n",
    "                inter_subset_shuffle=shuffle,\n",
    "                drop_last=drop_last,\n",
    "            ),\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return data_loader\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80851885-3010-4bbc-a5f3-3097edb22a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "View of AnnData object with n_obs × n_vars = 44340 × 4345\n",
       "    obs: 'condition', 'cell_type', 'dose_val', 'control', 'condition_name', 'true_cond', 'batch', 'celltype', 'str_batch', 'batch_id', 'n_counts'\n",
       "    var: 'gene_name', 'ensid', 'id_in_vocab', 'n_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection'\n",
       "    uns: 'hvg'\n",
       "    obsm: 'bin_edges'\n",
       "    layers: 'X_normed', 'X_binned'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8dd97de-1e10-4f7d-8b39-25f4a3a9780e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use domain specific batchnorm with affine=False\n",
      "scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([36574, 512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.0.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.bias with shape torch.Size([1])\n",
      "scGPT - INFO - Loading params mvc_decoder.gene2query.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params mvc_decoder.gene2query.bias with shape torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Create and finetune scGPT\n",
    "\n",
    "# %%\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    vocab=vocab,\n",
    "    dropout=config.dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    do_mvc=config.GEPC,\n",
    "    do_dab=True,\n",
    "    use_batch_labels=True,\n",
    "    num_batch_labels=num_batch_types,\n",
    "    domain_spec_batchnorm=DSBN,\n",
    "    n_input_bins=n_input_bins,\n",
    "    ecs_threshold=config.ecs_thres,\n",
    "    explicit_zero_prob=explicit_zero_prob,\n",
    "    use_fast_transformer=True,\n",
    "    pre_norm=config.pre_norm,\n",
    ")\n",
    "if config.load_model is not None:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "        logger.info(f\"Loading all model params from {model_file}\")\n",
    "    except:\n",
    "        # only load params that are in the model and match the size\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_file)\n",
    "        pretrained_dict = {\n",
    "            k: v\n",
    "            for k, v in pretrained_dict.items()\n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        for k, v in pretrained_dict.items():\n",
    "            logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "model.to(device)\n",
    "# wandb.watch(model)\n",
    "\n",
    "\n",
    "criterion = masked_mse_loss\n",
    "\n",
    "perturb_criterion = perturb_mse_loss\n",
    "# criterion = masked_ce_loss\n",
    "\n",
    "\n",
    "criterion_dab = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=config.lr, eps=1e-4 if config.amp else 1e-8\n",
    ")\n",
    "\n",
    "# optimizer = SophiaG(model.parameters(), lr=config.lr, betas=(0.965, 0.99), rho = 0.01, weight_decay=1e-1)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=config.schedule_ratio)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89eb15f0-36a8-4ad6-a45e-928578a0ee11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (encoder): GeneEncoder(\n",
       "    (embedding): Embedding(36574, 512, padding_idx=36571)\n",
       "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (value_encoder): ContinuousValueEncoder(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
       "    (activation): ReLU()\n",
       "    (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (batch_encoder): BatchLabelEncoder(\n",
       "    (embedding): Embedding(1, 512)\n",
       "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (dsbn): DomainSpecificBatchNorm1d(\n",
       "    (bns): ModuleList(\n",
       "      (0): BatchNorm1d(512, eps=6.1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (1): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (2): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (3): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (4): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (5): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (6): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (7): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (8): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (9): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (10): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (11): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ExprDecoder(\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "    (zero_logit): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (cls_decoder): ClsDecoder(\n",
       "    (_decoder): ModuleList(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (out_layer): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (mvc_decoder): MVCDecoder(\n",
       "    (gene2query): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (query_activation): Sigmoid()\n",
       "    (W): Linear(in_features=512, out_features=1024, bias=False)\n",
       "    (W_zero_logit): Linear(in_features=512, out_features=1024, bias=True)\n",
       "  )\n",
       "  (grad_reverse_discriminator): AdversarialDiscriminator(\n",
       "    (_decoder): ModuleList(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (4): LeakyReLU(negative_slope=0.01)\n",
       "      (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (out_layer): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (sim): Similarity(\n",
       "    (cos): CosineSimilarity()\n",
       "  )\n",
       "  (creterion_cce): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f488693-3441-4470-8594-98f540245620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for param in model.transformer_encoder.parameters():\n",
    "#     param.requires_grad = True\n",
    "    \n",
    "# for param in model.decoder.parameters():\n",
    "#     param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb08703b-5ebb-4428-9815-5a2b81319bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model: nn.Module, loader: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_mse, total_gepc = 0.0, 0.0, 0.0\n",
    "    total_error = 0.0\n",
    "    log_interval = config.log_interval\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "        input_values = batch_data[\"values\"].to(device)\n",
    "        target_values = batch_data[\"target_values\"].to(device)\n",
    "        batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "        \n",
    "        src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=batch_labels if DSBN else None,\n",
    "                MVC=config.GEPC,\n",
    "                ECS=config.ecs_thres > 0,\n",
    "            )\n",
    "\n",
    "            masked_positions = input_values.eq(mask_value)  # the postions to predict\n",
    "            loss = loss_mse = perturb_criterion(\n",
    "                output_dict[\"mlm_output\"].float(), target_values.float()\n",
    "            )\n",
    "            # loss = loss_mse = criterion(\n",
    "            #     output_dict[\"mlm_output\"], target_values, masked_positions\n",
    "            # )\n",
    "            metrics_to_log = {\"train/mse\": loss_mse.item()}\n",
    "            if explicit_zero_prob:\n",
    "                loss_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "                    output_dict[\"mlm_zero_probs\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_zero_log_prob\n",
    "                metrics_to_log.update({\"train/nzlp\": loss_zero_log_prob.item()})\n",
    "            if config.GEPC:\n",
    "                loss_gepc = criterion(\n",
    "                    output_dict[\"mvc_output\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_gepc\n",
    "                metrics_to_log.update({\"train/mvc\": loss_gepc.item()})\n",
    "            if config.GEPC and explicit_zero_prob:\n",
    "                loss_gepc_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "                    output_dict[\"mvc_zero_probs\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_gepc_zero_log_prob\n",
    "                metrics_to_log.update(\n",
    "                    {\"train/mvc_nzlp\": loss_gepc_zero_log_prob.item()}\n",
    "                )\n",
    "            if config.ecs_thres > 0:\n",
    "                loss_ecs = 10 * output_dict[\"loss_ecs\"]\n",
    "                loss = loss + loss_ecs\n",
    "                metrics_to_log.update({\"train/ecs\": loss_ecs.item()})\n",
    "            loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "            #loss = loss + config.dab_weight * loss_dab\n",
    "            metrics_to_log.update({\"train/dab\": loss_dab.item()})\n",
    "\n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.filterwarnings(\"always\")\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                1.0,\n",
    "                error_if_nonfinite=False if scaler.is_enabled() else True,\n",
    "            )\n",
    "            if len(w) > 0:\n",
    "                logger.warning(\n",
    "                    f\"Found infinite gradient. This may be caused by the gradient \"\n",
    "                    f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n",
    "                    \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n",
    "                )\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        wandb.log(metrics_to_log)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mre = masked_relative_error(\n",
    "                output_dict[\"mlm_output\"], target_values, masked_positions\n",
    "            )\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_mse += loss_mse.item()\n",
    "        total_gepc += loss_gepc.item() if config.GEPC else 0.0\n",
    "        total_error += mre.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            cur_mse = total_mse / log_interval\n",
    "            cur_gepc = total_gepc / log_interval if config.GEPC else 0.0\n",
    "            cur_error = total_error / log_interval\n",
    "            # ppl = math.exp(cur_loss)\n",
    "            logger.info(\n",
    "                f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "                f\"lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | mse {cur_mse:5.2f} | mre {cur_error:5.2f} |\"\n",
    "                + (f\"gepc {cur_gepc:5.2f} |\" if config.GEPC else \"\")\n",
    "            )\n",
    "            total_loss = 0\n",
    "            total_mse = 0\n",
    "            total_gepc = 0\n",
    "            total_error = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def define_wandb_metrcis():\n",
    "    wandb.define_metric(\"valid/mse\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/mre\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/sum_mse_dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"test/avg_bio\", summary=\"max\")\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_error = 0.0\n",
    "    total_dab = 0.0\n",
    "    total_pertloss = 0.0\n",
    "    total_num = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            target_values = batch_data[\"target_values\"].to(device)\n",
    "            batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "\n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(\n",
    "                    input_gene_ids,\n",
    "                    input_values,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    batch_labels=batch_labels if DSBN else None,\n",
    "                )\n",
    "                output_values = output_dict[\"mlm_output\"]\n",
    "\n",
    "                masked_positions = input_values.eq(mask_value)\n",
    "                loss = criterion(output_values, target_values, masked_positions)\n",
    "                loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "                \n",
    "                perturb_loss = perturb_criterion(output_values, target_values)\n",
    "\n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            total_error += masked_relative_error(\n",
    "                output_values, target_values, masked_positions\n",
    "            ).item() * len(input_gene_ids)\n",
    "            total_dab += loss_dab.item() * len(input_gene_ids)\n",
    "            total_num += len(input_gene_ids)\n",
    "            total_pertloss += perturb_loss.item()* len(input_gene_ids)\n",
    "\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"valid/mse\": total_loss / total_num,\n",
    "            \"valid/mre\": total_error / total_num,\n",
    "            \"valid/dab\": total_dab / total_num,\n",
    "            \"valid/sum_mse_dab\": (total_loss + config.dab_weight * total_dab)\n",
    "            / total_num,\n",
    "            \"valid/perturb_loss\": total_pertloss / total_num,\n",
    "            \"epoch\": epoch,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return total_loss / total_num, total_error / total_num, total_pertloss / total_num\n",
    "\n",
    "\n",
    "def eval_testdata(\n",
    "    model: nn.Module,\n",
    "    adata_t: AnnData,\n",
    "    include_types: List[str] = [\"cls\"],\n",
    ") -> Optional[Dict]:\n",
    "    \"\"\"evaluate the model on test dataset of adata_t\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # copy adata_t to avoid reuse previously computed results stored in adata_t\n",
    "    adata_t = adata_t.copy()\n",
    "\n",
    "    all_counts = (\n",
    "        adata_t.layers[input_layer_key].A\n",
    "        if issparse(adata_t.layers[input_layer_key])\n",
    "        else adata_t.layers[input_layer_key]\n",
    "    )\n",
    "\n",
    "    celltypes_labels = adata_t.obs[\"celltype\"].tolist()\n",
    "    celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "    batch_ids = adata_t.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "\n",
    "    # Evaluate cls cell embeddings\n",
    "    if \"cls\" in include_types:\n",
    "        logger.info(\"Evaluating cls cell embeddings\")\n",
    "        tokenized_all = tokenize_and_pad_batch(\n",
    "            all_counts,\n",
    "            gene_ids,\n",
    "            max_len=max_seq_len,\n",
    "            vocab=vocab,\n",
    "            pad_token=pad_token,\n",
    "            pad_value=pad_value,\n",
    "            append_cls=True,  # append <cls> token at the beginning\n",
    "            include_zero_gene=True,\n",
    "        )\n",
    "        all_gene_ids, all_values = tokenized_all[\"genes\"], tokenized_all[\"values\"]\n",
    "        src_key_padding_mask = all_gene_ids.eq(vocab[pad_token])\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            cell_embeddings = model.encode_batch(\n",
    "                all_gene_ids,\n",
    "                all_values.float(),\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_size=config.batch_size,\n",
    "                batch_labels=torch.from_numpy(batch_ids).long() if DSBN else None,\n",
    "                time_step=0,\n",
    "                return_np=True,\n",
    "            )\n",
    "        cell_embeddings = cell_embeddings / np.linalg.norm(\n",
    "            cell_embeddings, axis=1, keepdims=True\n",
    "        )\n",
    "\n",
    "        adata_t.obsm[\"X_scGPT\"] = cell_embeddings\n",
    "\n",
    "        results = {}\n",
    "        try:\n",
    "            results = eval_scib_metrics(adata_t)\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            logger.error(e)\n",
    "\n",
    "        sc.pp.neighbors(adata_t, use_rep=\"X_scGPT\")\n",
    "        sc.tl.umap(adata_t, min_dist=0.3)\n",
    "        fig = sc.pl.umap(\n",
    "            adata_t,\n",
    "            color=[\"str_batch\"],\n",
    "            title=[f\"batch, avg_bio = {results.get('avg_bio', 0.0):.4f}\"],\n",
    "            frameon=False,\n",
    "            return_fig=True,\n",
    "            show=False,\n",
    "        )\n",
    "\n",
    "        results[\"batch_umap\"] = fig\n",
    "\n",
    "        sc.pp.neighbors(adata_t, use_rep=\"X_scGPT\")\n",
    "        sc.tl.umap(adata_t, min_dist=0.3)\n",
    "        fig = sc.pl.umap(\n",
    "            adata_t,\n",
    "            color=[\"celltype\"],\n",
    "            title=[\n",
    "                f\"celltype, avg_bio = {results.get('avg_bio', 0.0):.4f}\",\n",
    "            ],\n",
    "            frameon=False,\n",
    "            return_fig=True,\n",
    "            show=False,\n",
    "        )\n",
    "\n",
    "        results[\"celltype_umap\"] = fig\n",
    "\n",
    "    if len(include_types) == 1:\n",
    "        return results\n",
    "    \n",
    "    \n",
    "def return_testdata(\n",
    "    model: nn.Module,\n",
    "    adata_t: AnnData,\n",
    "    include_types: List[str] = [\"cls\"],\n",
    ") -> Optional[Dict]:\n",
    "    \"\"\"evaluate the model on test dataset of adata_t\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # copy adata_t to avoid reuse previously computed results stored in adata_t\n",
    "    adata_t = adata_t.copy()\n",
    "\n",
    "    all_counts = (\n",
    "        adata_t.layers[input_layer_key].A\n",
    "        if issparse(adata_t.layers[input_layer_key])\n",
    "        else adata_t.layers[input_layer_key]\n",
    "    )\n",
    "\n",
    "    celltypes_labels = adata_t.obs[\"celltype\"].tolist()\n",
    "    celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "    batch_ids = adata_t.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "\n",
    "    # Evaluate cls cell embeddings\n",
    "    if \"cls\" in include_types:\n",
    "        logger.info(\"Evaluating cls cell embeddings\")\n",
    "        tokenized_all = tokenize_and_pad_batch(\n",
    "            all_counts,\n",
    "            gene_ids,\n",
    "            max_len=max_seq_len,\n",
    "            vocab=vocab,\n",
    "            pad_token=pad_token,\n",
    "            pad_value=pad_value,\n",
    "            append_cls=True,  # append <cls> token at the beginning\n",
    "            include_zero_gene=True,\n",
    "        )\n",
    "        all_gene_ids, all_values = tokenized_all[\"genes\"], tokenized_all[\"values\"]\n",
    "        src_key_padding_mask = all_gene_ids.eq(vocab[pad_token])\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            cell_embeddings = model.encode_batch(\n",
    "                all_gene_ids,\n",
    "                all_values.float(),\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_size=config.batch_size,\n",
    "                batch_labels=torch.from_numpy(batch_ids).long() if DSBN else None,\n",
    "                time_step=0,\n",
    "                return_np=True,\n",
    "            )\n",
    "        cell_embeddings = cell_embeddings / np.linalg.norm(\n",
    "            cell_embeddings, axis=1, keepdims=True\n",
    "        )\n",
    "        \n",
    "    return cell_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c543dd5-d9be-49f4-8823-ba18a1d77bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #zero shot learning\n",
    "# best_val_loss = float(\"inf\")\n",
    "# best_avg_bio = 0.0\n",
    "# best_model = None\n",
    "# define_wandb_metrcis()\n",
    "\n",
    "# for epoch in range(1, config.epochs + 1):\n",
    "#     epoch_start_time = time.time()\n",
    "#     train_data_pt, valid_data_pt = prepare_data(sort_seq_batch=per_seq_batch_sample)\n",
    "#     train_loader = prepare_dataloader(\n",
    "#         train_data_pt,\n",
    "#         batch_size=config.batch_size,\n",
    "#         shuffle=True,\n",
    "#         intra_domain_shuffle=True,\n",
    "#         drop_last=False,\n",
    "#     )\n",
    "#     valid_loader = prepare_dataloader(\n",
    "#         valid_data_pt,\n",
    "#         batch_size=config.batch_size,\n",
    "#         shuffle=False,\n",
    "#         intra_domain_shuffle=False,\n",
    "#         drop_last=False,\n",
    "#     )\n",
    "    \n",
    "#     val_loss, val_mre, val_pertuberror = evaluate(\n",
    "#         model,\n",
    "#         loader=valid_loader,\n",
    "#     )\n",
    "#     elapsed = time.time() - epoch_start_time\n",
    "#     logger.info(\"-\" * 89)\n",
    "#     logger.info(\n",
    "#         f\"| zero shot learning | \"\n",
    "#         f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "#         f\"valid loss/mse {val_loss:5.4f} | mre {val_mre:5.4f} | valid loss/perturb {val_pertuberror:5.4f}\"\n",
    "#     )\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6210f5ce-1baf-4222-a9ee-cb1dc105fd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - | epoch   1 | 100/929 batches | lr 0.0001 | ms/batch 1499.88 | loss 198.90 | mse 121.27 | mre 7169293.72 |gepc 66.34 |\n",
      "scGPT - INFO - | epoch   1 | 200/929 batches | lr 0.0001 | ms/batch 1495.25 | loss 134.75 | mse 67.25 | mre 15351273.48 |gepc 56.65 |\n",
      "scGPT - INFO - | epoch   1 | 300/929 batches | lr 0.0001 | ms/batch 1486.51 | loss 133.64 | mse 62.92 | mre 15381977.02 |gepc 59.90 |\n",
      "scGPT - INFO - | epoch   1 | 400/929 batches | lr 0.0001 | ms/batch 1487.05 | loss 128.69 | mse 60.36 | mre 12593647.14 |gepc 57.56 |\n",
      "scGPT - INFO - | epoch   1 | 500/929 batches | lr 0.0001 | ms/batch 1487.64 | loss 123.43 | mse 57.90 | mre 11579570.48 |gepc 54.87 |\n",
      "scGPT - INFO - | epoch   1 | 600/929 batches | lr 0.0001 | ms/batch 1488.37 | loss 122.65 | mse 57.01 | mre 10518119.91 |gepc 54.90 |\n",
      "scGPT - INFO - | epoch   1 | 700/929 batches | lr 0.0001 | ms/batch 1487.00 | loss 116.72 | mse 56.25 | mre 10202907.63 |gepc 49.81 |\n",
      "scGPT - INFO - | epoch   1 | 800/929 batches | lr 0.0001 | ms/batch 1485.60 | loss 128.05 | mse 55.61 | mre 9523583.90 |gepc 61.66 |\n",
      "scGPT - INFO - | epoch   1 | 900/929 batches | lr 0.0001 | ms/batch 1483.80 | loss 127.71 | mse 55.25 | mre 8667936.27 |gepc 61.70 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   1 | time: 1585.82s | valid loss/mse 267.0546 | mre 9944352.5823 | valid loss/perturb 54.1813\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 267.0546\n",
      "scGPT - INFO - | epoch   2 | 100/929 batches | lr 0.0001 | ms/batch 1499.05 | loss 119.46 | mse 55.15 | mre 8795010.54 |gepc 53.53 |\n",
      "scGPT - INFO - | epoch   2 | 200/929 batches | lr 0.0001 | ms/batch 1484.85 | loss 119.57 | mse 54.54 | mre 8256265.54 |gepc 54.35 |\n",
      "scGPT - INFO - | epoch   2 | 300/929 batches | lr 0.0001 | ms/batch 1488.06 | loss 120.69 | mse 54.78 | mre 8500248.55 |gepc 55.26 |\n",
      "scGPT - INFO - | epoch   2 | 400/929 batches | lr 0.0001 | ms/batch 1488.30 | loss 120.54 | mse 54.21 | mre 8196456.96 |gepc 55.59 |\n",
      "scGPT - INFO - | epoch   2 | 500/929 batches | lr 0.0001 | ms/batch 1488.37 | loss 123.12 | mse 54.22 | mre 7692344.43 |gepc 58.08 |\n",
      "scGPT - INFO - | epoch   2 | 600/929 batches | lr 0.0001 | ms/batch 1484.14 | loss 121.79 | mse 54.06 | mre 7424276.01 |gepc 56.98 |\n",
      "scGPT - INFO - | epoch   2 | 700/929 batches | lr 0.0001 | ms/batch 1483.23 | loss 113.34 | mse 53.87 | mre 7029194.63 |gepc 48.81 |\n",
      "scGPT - INFO - | epoch   2 | 800/929 batches | lr 0.0001 | ms/batch 1485.16 | loss 113.48 | mse 53.63 | mre 6917514.44 |gepc 49.19 |\n",
      "scGPT - INFO - | epoch   2 | 900/929 batches | lr 0.0001 | ms/batch 1486.12 | loss 123.94 | mse 53.81 | mre 7280301.53 |gepc 59.31 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   2 | time: 1585.39s | valid loss/mse 174.0057 | mre 7056166.4545 | valid loss/perturb 53.4580\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 174.0057\n",
      "scGPT - INFO - | epoch   3 | 100/929 batches | lr 0.0001 | ms/batch 1500.32 | loss 118.68 | mse 54.36 | mre 6781319.43 |gepc 53.54 |\n",
      "scGPT - INFO - | epoch   3 | 200/929 batches | lr 0.0001 | ms/batch 1479.24 | loss 117.91 | mse 53.53 | mre 6535184.48 |gepc 53.70 |\n",
      "scGPT - INFO - | epoch   3 | 300/929 batches | lr 0.0001 | ms/batch 1482.82 | loss 119.26 | mse 53.46 | mre 6824637.39 |gepc 55.05 |\n",
      "scGPT - INFO - | epoch   3 | 400/929 batches | lr 0.0001 | ms/batch 1484.20 | loss 118.97 | mse 53.55 | mre 6088678.86 |gepc 54.73 |\n",
      "scGPT - INFO - | epoch   3 | 500/929 batches | lr 0.0001 | ms/batch 1492.18 | loss 117.83 | mse 53.25 | mre 5854697.40 |gepc 53.81 |\n",
      "scGPT - INFO - | epoch   3 | 600/929 batches | lr 0.0001 | ms/batch 1487.05 | loss 113.60 | mse 53.38 | mre 6058273.30 |gepc 49.59 |\n",
      "scGPT - INFO - | epoch   3 | 700/929 batches | lr 0.0001 | ms/batch 1491.21 | loss 118.24 | mse 53.59 | mre 5649425.09 |gepc 53.91 |\n",
      "scGPT - INFO - | epoch   3 | 800/929 batches | lr 0.0001 | ms/batch 1482.27 | loss 117.78 | mse 53.16 | mre 5716636.79 |gepc 53.88 |\n",
      "scGPT - INFO - | epoch   3 | 900/929 batches | lr 0.0001 | ms/batch 1481.15 | loss 113.97 | mse 53.30 | mre 5490262.20 |gepc 50.02 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   3 | time: 1586.37s | valid loss/mse 125.4457 | mre 5252268.3253 | valid loss/perturb 52.8108\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 125.4457\n",
      "scGPT - INFO - | epoch   4 | 100/929 batches | lr 0.0001 | ms/batch 1493.61 | loss 118.59 | mse 53.81 | mre 5504107.98 |gepc 53.97 |\n",
      "scGPT - INFO - | epoch   4 | 200/929 batches | lr 0.0001 | ms/batch 1479.63 | loss 118.72 | mse 53.45 | mre 5851924.02 |gepc 54.51 |\n",
      "scGPT - INFO - | epoch   4 | 300/929 batches | lr 0.0001 | ms/batch 1486.16 | loss 116.96 | mse 53.10 | mre 5144056.26 |gepc 53.13 |\n",
      "scGPT - INFO - | epoch   4 | 400/929 batches | lr 0.0001 | ms/batch 1488.25 | loss 113.72 | mse 52.98 | mre 5378439.85 |gepc 50.08 |\n",
      "scGPT - INFO - | epoch   4 | 500/929 batches | lr 0.0001 | ms/batch 1484.99 | loss 117.24 | mse 53.20 | mre 5172365.32 |gepc 53.37 |\n",
      "scGPT - INFO - | epoch   4 | 600/929 batches | lr 0.0001 | ms/batch 1480.85 | loss 117.80 | mse 52.99 | mre 4998942.67 |gepc 54.12 |\n",
      "scGPT - INFO - | epoch   4 | 700/929 batches | lr 0.0001 | ms/batch 1481.07 | loss 113.84 | mse 53.22 | mre 4678943.78 |gepc 49.99 |\n",
      "scGPT - INFO - | epoch   4 | 800/929 batches | lr 0.0001 | ms/batch 1482.04 | loss 112.21 | mse 52.93 | mre 4896470.75 |gepc 48.64 |\n",
      "scGPT - INFO - | epoch   4 | 900/929 batches | lr 0.0001 | ms/batch 1484.29 | loss 121.65 | mse 53.08 | mre 4847009.58 |gepc 57.79 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   4 | time: 1580.74s | valid loss/mse 106.3272 | mre 5053043.9099 | valid loss/perturb 52.9165\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 106.3272\n",
      "scGPT - INFO - | epoch   5 | 100/929 batches | lr 0.0001 | ms/batch 1501.78 | loss 112.20 | mse 53.60 | mre 4522026.90 |gepc 47.90 |\n",
      "scGPT - INFO - | epoch   5 | 200/929 batches | lr 0.0001 | ms/batch 1483.32 | loss 116.36 | mse 52.99 | mre 4501692.96 |gepc 52.77 |\n",
      "scGPT - INFO - | epoch   5 | 300/929 batches | lr 0.0001 | ms/batch 1482.66 | loss 118.17 | mse 53.13 | mre 4661399.16 |gepc 54.31 |\n",
      "scGPT - INFO - | epoch   5 | 400/929 batches | lr 0.0001 | ms/batch 1484.70 | loss 116.47 | mse 52.76 | mre 4549299.82 |gepc 53.03 |\n",
      "scGPT - INFO - | epoch   5 | 500/929 batches | lr 0.0001 | ms/batch 1487.92 | loss 114.81 | mse 52.82 | mre 4640820.87 |gepc 51.33 |\n",
      "scGPT - INFO - | epoch   5 | 600/929 batches | lr 0.0001 | ms/batch 1481.74 | loss 118.11 | mse 52.94 | mre 4370446.95 |gepc 54.41 |\n",
      "scGPT - INFO - | epoch   5 | 700/929 batches | lr 0.0001 | ms/batch 1485.99 | loss 116.59 | mse 52.86 | mre 4420201.54 |gepc 52.99 |\n",
      "scGPT - INFO - | epoch   5 | 800/929 batches | lr 0.0001 | ms/batch 1484.55 | loss 116.30 | mse 52.69 | mre 4354017.03 |gepc 52.83 |\n",
      "scGPT - INFO - | epoch   5 | 900/929 batches | lr 0.0001 | ms/batch 1484.38 | loss 109.94 | mse 52.76 | mre 4681842.38 |gepc 46.50 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   5 | time: 1586.21s | valid loss/mse 108.9017 | mre 5103061.0719 | valid loss/perturb 52.3827\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | epoch   6 | 100/929 batches | lr 0.0001 | ms/batch 1504.01 | loss 118.72 | mse 53.15 | mre 5023103.74 |gepc 54.75 |\n",
      "scGPT - INFO - | epoch   6 | 200/929 batches | lr 0.0001 | ms/batch 1484.33 | loss 117.86 | mse 52.54 | mre 5218184.03 |gepc 54.67 |\n",
      "scGPT - INFO - | epoch   6 | 300/929 batches | lr 0.0001 | ms/batch 1483.39 | loss 117.03 | mse 52.43 | mre 5445774.80 |gepc 53.99 |\n",
      "scGPT - INFO - | epoch   6 | 400/929 batches | lr 0.0001 | ms/batch 1483.53 | loss 117.22 | mse 52.56 | mre 5424603.22 |gepc 54.52 |\n",
      "scGPT - INFO - | epoch   6 | 500/929 batches | lr 0.0001 | ms/batch 1482.35 | loss 111.40 | mse 52.46 | mre 5400813.74 |gepc 48.81 |\n",
      "scGPT - INFO - | epoch   6 | 600/929 batches | lr 0.0001 | ms/batch 1480.52 | loss 111.27 | mse 52.62 | mre 4848061.13 |gepc 48.54 |\n",
      "scGPT - INFO - | epoch   6 | 700/929 batches | lr 0.0001 | ms/batch 1479.89 | loss 111.09 | mse 52.32 | mre 4897436.94 |gepc 48.69 |\n",
      "scGPT - INFO - | epoch   6 | 800/929 batches | lr 0.0001 | ms/batch 1478.75 | loss 111.97 | mse 52.57 | mre 4693437.22 |gepc 49.30 |\n",
      "scGPT - INFO - | epoch   6 | 900/929 batches | lr 0.0001 | ms/batch 1480.69 | loss 113.02 | mse 52.42 | mre 4357037.26 |gepc 50.49 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   6 | time: 1581.20s | valid loss/mse 81.0268 | mre 4182727.1015 | valid loss/perturb 52.0950\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 81.0268\n",
      "scGPT - INFO - | epoch   7 | 100/929 batches | lr 0.0001 | ms/batch 1500.86 | loss 117.36 | mse 53.04 | mre 4838023.97 |gepc 54.11 |\n",
      "scGPT - INFO - | epoch   7 | 200/929 batches | lr 0.0001 | ms/batch 1482.27 | loss 113.60 | mse 52.35 | mre 4536802.79 |gepc 51.18 |\n",
      "scGPT - INFO - | epoch   7 | 300/929 batches | lr 0.0001 | ms/batch 1480.37 | loss 108.47 | mse 52.40 | mre 4137801.07 |gepc 45.98 |\n",
      "scGPT - INFO - | epoch   7 | 400/929 batches | lr 0.0001 | ms/batch 1488.41 | loss 109.64 | mse 52.24 | mre 4502073.76 |gepc 47.31 |\n",
      "scGPT - INFO - | epoch   7 | 500/929 batches | lr 0.0001 | ms/batch 1477.28 | loss 117.91 | mse 52.41 | mre 4137041.29 |gepc 55.40 |\n",
      "scGPT - INFO - | epoch   7 | 600/929 batches | lr 0.0001 | ms/batch 1479.39 | loss 110.70 | mse 52.44 | mre 4081612.51 |gepc 48.20 |\n",
      "scGPT - INFO - | epoch   7 | 700/929 batches | lr 0.0001 | ms/batch 1485.09 | loss 111.81 | mse 52.13 | mre 4075045.63 |gepc 49.63 |\n",
      "scGPT - INFO - | epoch   7 | 800/929 batches | lr 0.0001 | ms/batch 1482.37 | loss 114.18 | mse 52.24 | mre 4208597.06 |gepc 51.89 |\n",
      "scGPT - INFO - | epoch   7 | 900/929 batches | lr 0.0001 | ms/batch 1479.63 | loss 116.80 | mse 52.15 | mre 4137326.74 |gepc 54.54 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   7 | time: 1580.94s | valid loss/mse 75.9648 | mre 3966625.4096 | valid loss/perturb 52.0282\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 75.9648\n",
      "scGPT - INFO - | epoch   8 | 100/929 batches | lr 0.0001 | ms/batch 1495.50 | loss 115.12 | mse 52.67 | mre 4124170.91 |gepc 52.27 |\n",
      "scGPT - INFO - | epoch   8 | 200/929 batches | lr 0.0001 | ms/batch 1480.05 | loss 111.63 | mse 52.48 | mre 3927323.38 |gepc 49.10 |\n",
      "scGPT - INFO - | epoch   8 | 300/929 batches | lr 0.0001 | ms/batch 1487.41 | loss 109.95 | mse 52.14 | mre 3786499.52 |gepc 47.73 |\n",
      "scGPT - INFO - | epoch   8 | 400/929 batches | lr 0.0001 | ms/batch 1477.30 | loss 113.74 | mse 52.30 | mre 4054963.73 |gepc 51.34 |\n",
      "scGPT - INFO - | epoch   8 | 500/929 batches | lr 0.0001 | ms/batch 1482.33 | loss 110.07 | mse 52.33 | mre 3375868.46 |gepc 47.67 |\n",
      "scGPT - INFO - | epoch   8 | 600/929 batches | lr 0.0001 | ms/batch 1480.11 | loss 112.65 | mse 52.25 | mre 3507304.74 |gepc 50.35 |\n",
      "scGPT - INFO - | epoch   8 | 700/929 batches | lr 0.0001 | ms/batch 1477.73 | loss 114.69 | mse 52.02 | mre 3836008.34 |gepc 52.61 |\n",
      "scGPT - INFO - | epoch   8 | 800/929 batches | lr 0.0001 | ms/batch 1478.23 | loss 118.78 | mse 52.25 | mre 3732279.20 |gepc 56.43 |\n",
      "scGPT - INFO - | epoch   8 | 900/929 batches | lr 0.0001 | ms/batch 1475.80 | loss 112.95 | mse 52.09 | mre 3837873.94 |gepc 50.77 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   8 | time: 1580.17s | valid loss/mse 64.0867 | mre 3381950.1715 | valid loss/perturb 52.0130\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 64.0867\n",
      "scGPT - INFO - | epoch   9 | 100/929 batches | lr 0.0000 | ms/batch 1497.09 | loss 112.88 | mse 52.48 | mre 3596048.55 |gepc 50.21 |\n",
      "scGPT - INFO - | epoch   9 | 200/929 batches | lr 0.0000 | ms/batch 1483.10 | loss 116.06 | mse 52.14 | mre 3762749.52 |gepc 53.80 |\n",
      "scGPT - INFO - | epoch   9 | 300/929 batches | lr 0.0000 | ms/batch 1476.33 | loss 110.37 | mse 52.27 | mre 3734316.90 |gepc 48.07 |\n",
      "scGPT - INFO - | epoch   9 | 400/929 batches | lr 0.0000 | ms/batch 1480.16 | loss 112.47 | mse 52.21 | mre 4084994.56 |gepc 50.20 |\n"
     ]
    }
   ],
   "source": [
    "### %%time\n",
    "best_val_loss = float(\"inf\")\n",
    "best_avg_bio = 0.0\n",
    "best_model = None\n",
    "define_wandb_metrcis()\n",
    "\n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_data_pt, valid_data_pt = prepare_data(sort_seq_batch=per_seq_batch_sample)\n",
    "    train_loader = prepare_dataloader(\n",
    "        train_data_pt,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        intra_domain_shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    valid_loader = prepare_dataloader(\n",
    "        valid_data_pt,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    if config.do_train:\n",
    "        train(\n",
    "            model,\n",
    "            loader=train_loader,\n",
    "        )\n",
    "    val_loss, val_mre, val_pertuberror = evaluate(\n",
    "        model,\n",
    "        loader=valid_loader,\n",
    "    )\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(\n",
    "        f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "        f\"valid loss/mse {val_loss:5.4f} | mre {val_mre:5.4f} | valid loss/perturb {val_pertuberror:5.4f}\"\n",
    "    )\n",
    "    logger.info(\"-\" * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "        logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "\n",
    "    if epoch % config.save_eval_interval == 0 or epoch == config.epochs:\n",
    "        continue\n",
    "        # logger.info(f\"Saving model to {save_dir}\")\n",
    "        # torch.save(best_model.state_dict(), save_dir / f\"model_e{best_model_epoch}.pt\")\n",
    "\n",
    "        # eval on testdata\n",
    "#         results = eval_testdata(\n",
    "#             best_model,\n",
    "#             adata_t=adata_sorted if per_seq_batch_sample else adata,\n",
    "#             include_types=[\"cls\"],\n",
    "#         )\n",
    "#         results[\"batch_umap\"].savefig(\n",
    "#             save_dir / f\"embeddings_batch_umap[cls]_e{best_model_epoch}.png\", dpi=300\n",
    "#         )\n",
    "\n",
    "#         results[\"celltype_umap\"].savefig(\n",
    "#             save_dir / f\"embeddings_celltype_umap[cls]_e{best_model_epoch}.png\", dpi=300\n",
    "#         )\n",
    "#         metrics_to_log = {\"test/\" + k: v for k, v in results.items()}\n",
    "#         metrics_to_log[\"test/batch_umap\"] = wandb.Image(\n",
    "#             str(save_dir / f\"embeddings_batch_umap[cls]_e{best_model_epoch}.png\"),\n",
    "#             caption=f\"celltype avg_bio epoch {best_model_epoch}\",\n",
    "#         )\n",
    "\n",
    "        # metrics_to_log[\"test/celltype_umap\"] = wandb.Image(\n",
    "        #     str(save_dir / f\"embeddings_celltype_umap[cls]_e{best_model_epoch}.png\"),\n",
    "        #     caption=f\"celltype avg_bio epoch {best_model_epoch}\",\n",
    "        # )\n",
    "        # # metrics_to_log[\"test/best_model_epoch\"] = best_model_epoch\n",
    "        # wandb.log(metrics_to_log)\n",
    "        # wandb.log({\"avg_bio\": results.get(\"avg_bio\", 0.0)})\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "# %%\n",
    "# save the best model\n",
    "torch.save(best_model.state_dict(), save_dir / \"best_model.pt\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Gene embeddings\n",
    "\n",
    "# %%\n",
    "artifact = wandb.Artifact(f\"best_model\", type=\"model\")\n",
    "glob_str = os.path.join(save_dir, \"best_model.pt\")\n",
    "artifact.add_file(glob_str)\n",
    "run.log_artifact(artifact)\n",
    "\n",
    "run.finish()\n",
    "wandb.finish()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce6a50-5b99-4c08-9497-c07482e89144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a671cf-284d-4594-becc-0ecee51da8e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876a7351-7d0e-4e1a-b02d-393132bb73aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06316d63-4d3a-47b0-81a1-7e7c67911a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5d3b78a-7a3e-4c75-925a-43fddfef46d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 159.736MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
