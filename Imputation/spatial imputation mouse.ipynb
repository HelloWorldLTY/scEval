{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f7d296c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/gpfs/gibbs/project/zhao/tl688/conda_envs/scgpt/lib/python3.8/site-packages/scanpy/_settings.py:447: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  IPython.display.set_matplotlib_formats(*ipython_format)\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtianyu-18\u001b[0m (\u001b[33mzhao-lab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/gpfs/gibbs/project/zhao/tl688/conda_envs/scgpt/lib/python3.8/site-packages/wandb/sdk/lib/ipython.py:70: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/gpfs/gibbs/pi/zhao/tl688/scGPT/examples/wandb/run-20230725_161241-1eebn4u2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zhao-lab/scGPT/runs/1eebn4u2' target=\"_blank\">lyric-blaze-1531</a></strong> to <a href='https://wandb.ai/zhao-lab/scGPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zhao-lab/scGPT' target=\"_blank\">https://wandb.ai/zhao-lab/scGPT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zhao-lab/scGPT/runs/1eebn4u2' target=\"_blank\">https://wandb.ai/zhao-lab/scGPT/runs/1eebn4u2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 42, 'dataset_name': 'PBMC_10K', 'do_train': True, 'load_model': 'save/scGPT_bc', 'mask_ratio': 0.4, 'epochs': 1, 'n_bins': 51, 'GEPC': True, 'ecs_thres': 0.8, 'dab_weight': 1.0, 'lr': 0.0001, 'batch_size': 32, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'log_interval': 100, 'fast_transformer': True, 'pre_norm': False, 'amp': True}\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import numpy as npddddd\n",
    "import wandb\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "\n",
    "from Sophia import SophiaG \n",
    "\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerModel, AdversarialDiscriminator\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    "    masked_ce_loss\n",
    ")\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt import SubsetsBatchSampler\n",
    "from scgpt.utils import set_seed, category_str2int, eval_scib_metrics\n",
    "\n",
    "# from functions_group import DistanceLoss\n",
    "\n",
    "sc.set_figure_params(figsize=(4, 4))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "# os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "\n",
    "# hyperparameter_defaults = dict(\n",
    "#     seed=42,\n",
    "#     dataset_name=\"PBMC_10K\",\n",
    "#     do_train=True,\n",
    "#     load_model=\"save/scGPT_bc\",\n",
    "#     mask_ratio=0.1,\n",
    "#     epochs=10,\n",
    "#     n_bins=10001,\n",
    "#     GEPC=True,  # Masked value prediction for cell embedding\n",
    "#     ecs_thres=0.8,  # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "#     dab_weight=10.0,\n",
    "#     lr=1e-4,\n",
    "#     batch_size=64,\n",
    "#     layer_size=128,\n",
    "#     nlayers=4,\n",
    "#     nhead=4,\n",
    "#     # if load model, batch_size, layer_size, nlayers, nhead will be ignored\n",
    "#     dropout=0.2,\n",
    "#     schedule_ratio=0.9,  # ratio of epochs for learning rate schedule\n",
    "#     save_eval_interval=5,\n",
    "#     log_interval=10,\n",
    "#     fast_transformer=True,\n",
    "#     pre_norm=False,\n",
    "#     amp=True,  # Automatic Mixed Precision\n",
    "# )\n",
    "\n",
    "# modify original param\n",
    "hyperparameter_defaults = dict(\n",
    "    seed=42,\n",
    "    dataset_name=\"PBMC_10K\",\n",
    "    do_train=True,\n",
    "    load_model=\"save/scGPT_bc\",\n",
    "    mask_ratio=0.4,\n",
    "    epochs=1,\n",
    "    n_bins=51,\n",
    "    GEPC=True,  # Masked value prediction for cell embedding\n",
    "    ecs_thres=0.8,  # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "    dab_weight=1.0,\n",
    "    lr=1e-4,\n",
    "    batch_size=32,\n",
    "    layer_size=128,\n",
    "    nlayers=4,\n",
    "    nhead=4,\n",
    "    # if load model, batch_size, layer_size, nlayers, nhead will be ignored\n",
    "    dropout=0.2,\n",
    "    schedule_ratio=0.9,  # ratio of epochs for learning rate schedule\n",
    "    save_eval_interval=5,\n",
    "    log_interval=100,\n",
    "    fast_transformer=True,\n",
    "    pre_norm=False,\n",
    "    amp=True,  # Automatic Mixed Precision\n",
    ")\n",
    "\n",
    "run = wandb.init(\n",
    "    config=hyperparameter_defaults,\n",
    "    project=\"scGPT\",\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"fork\"),\n",
    ")\n",
    "config = wandb.config\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ea5520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a888b017",
   "metadata": {},
   "source": [
    "# scRNA-seq imputation and scRNA-deq fine-tuning + spatial imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fd15979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_selection(adata, key='batch', orders=None):\n",
    "    '''Function used to determine the training sequence based on the variance across genes.\n",
    "        Args:\n",
    "            adata: The given dataset in AnnData form\n",
    "            key: The index name of batch information in the given dataset\n",
    "            orders: The batch sequence for training or none\n",
    "        Output:\n",
    "            orders: The batch sequence for training\n",
    "    '''\n",
    "    if orders == None:\n",
    "        batch_list = list(set(adata.obs[key].values))\n",
    "        adata_values = [np.array(adata.X[adata.obs[key] == batch]) for batch in batch_list]\n",
    "        std_ = [np.sum(np.std(item, axis=0)) for item in adata_values]\n",
    "        orders = np.argsort(std_)[::-1]\n",
    "        return [batch_list[i] for i in orders] \n",
    "    else:\n",
    "        return orders\n",
    "\n",
    "def generate_target_dataset(adata, batch_list):\n",
    "    \"\"\"A simple function used to rearrange the batch index of our given dataset.\n",
    "    \"\"\"\n",
    "    adata0 = adata[adata.obs['batch'] == batch_list[0]]\n",
    "    for i in batch_list[1:]:\n",
    "        adata0 = adata0.concatenate(adata[adata.obs['batch'] == i], batch_key='batch_key', index_unique=None)\n",
    "    return adata0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b38c9694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_dataset = \"HumanPBMC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd67da0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to save/dev_PBMC_10K-Jul25-16-12\n",
      "AnnData object with n_obs × n_vars = 21697 × 36826\n",
      "    obs: 'sample_name', 'organism', 'donor_sex', 'cell_class', 'cell_subclass', 'cell_cluster', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'n_counts', 'batch'\n",
      "    var: 'mt', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
      "    uns: 'cell_class_colors', 'cell_subclass_colors', 'hvg', 'neighbors', 'pca', 'umap'\n",
      "    obsm: 'X_pca', 'X_umap'\n",
      "    varm: 'PCs'\n",
      "    obsp: 'connectivities', 'distances'\n",
      "scGPT - INFO - match 14894/36826 genes in vocabulary of size 36574.\n",
      "scGPT - INFO - Resume model from save/scGPT_bc/best_model.pt, the model args will override the config save/scGPT_bc/args.json.\n"
     ]
    }
   ],
   "source": [
    "set_seed(config.seed)\n",
    "\n",
    "# %%\n",
    "# settings for input and preprocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_ratio = config.mask_ratio\n",
    "mask_value = -1\n",
    "pad_value = -2\n",
    "n_input_bins = config.n_bins\n",
    "\n",
    "# %%\n",
    "dataset_name = config.dataset_name\n",
    "save_dir = Path(f\"./save/dev_{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"save to {save_dir}\")\n",
    "# save the whole script to the dir\n",
    "# os.system(f\"cp {__file__} {save_dir}\")\n",
    "\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Loading and preparing data\n",
    "# if dataset_name == \"PBMC_10K\":\n",
    "#     adata = scvi.data.pbmc_dataset()  # 11990 × 3346\n",
    "#     ori_batch_col = \"batch\"\n",
    "#     adata.obs[\"celltype\"] = adata.obs[\"str_labels\"].astype(\"category\")\n",
    "#     adata.var = adata.var.set_index(\"gene_symbols\")\n",
    "#     data_is_raw = True\n",
    "\n",
    "\n",
    "\n",
    "# adata = sc.read_loom(f\"./save/{use_dataset}_raw.loom\")  # 11990 × 3346\n",
    "# # adata.var_names = [i.upper() for i in adata.var_names]\n",
    "\n",
    "# # adata = sc.read_h5ad(f\"./save/{use_dataset}_raw.loom\")  # 11990 × 3346\n",
    "\n",
    "# # adata = sc.read_h5ad(f\"./save/{use_dataset}.h5ad\")  # 11990 × 3346\n",
    "# adata = sc.AnnData(adata.X.todense(), obs = adata.obs, var = adata.var)\n",
    "# ori_batch_col = \"batch\"\n",
    "# adata.obs[\"celltype\"] = adata.obs[\"celltype\"].astype(\"category\")\n",
    "# # adata.obs[\"celltype\"] = list(adata.obs[\"final_annotation\"])\n",
    "# adata.obs['celltype'] = adata.obs['celltype'].astype('category')\n",
    "\n",
    "# adata = sc.read_h5ad(\"/gpfs/gibbs/pi/zhao/tl688/tangram/ST-LN-compressed.h5ad\")\n",
    "# print(adata)\n",
    "\n",
    "# ori_batch_col = \"batch\"\n",
    "# adata.obs['batch'] = list(adata.obs['in_tissue'])\n",
    "# adata.obs['celltype'] = list(adata.obs['lymph_node'])\n",
    "# adata.obs[\"celltype\"] = adata.obs[\"celltype\"].astype(\"category\")\n",
    "# adata.var_names = [i.upper() for i in adata.var_names]\n",
    "# data_is_raw = True\n",
    "\n",
    "adata = sc.read_h5ad(\"mouse_scrnaseq.h5ad\")\n",
    "adata.var_names = [i.upper() for i in adata.var_names]\n",
    "adata.obs['batch'] = list(adata.obs.sample_name)\n",
    "adata.obs['batch'] = adata.obs['batch'].astype('category')\n",
    "print(adata)\n",
    "\n",
    "ori_batch_col = \"batch\"\n",
    "adata.obs[\"celltype\"] = adata.obs[\"cell_subclass\"].astype(\"category\")\n",
    "data_is_raw = False\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# make the batch category column \n",
    "adata.obs[\"str_batch\"] = adata.obs[ori_batch_col].astype(str)\n",
    "batch_id_labels = adata.obs[\"str_batch\"].astype(\"category\").cat.codes.values\n",
    "adata.obs[\"batch_id\"] = batch_id_labels\n",
    "\n",
    "adata.var[\"gene_name\"] = adata.var.index.tolist()\n",
    "\n",
    "if config.load_model is not None:\n",
    "    model_dir = Path(config.load_model)\n",
    "    model_config_file = model_dir / \"args.json\"\n",
    "    model_file = model_dir / \"best_model.pt\"\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "    # model\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "    logger.info(\n",
    "        f\"Resume model from {model_file}, the model args will override the \"\n",
    "        f\"config {model_config_file}.\"\n",
    "    )\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6066665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_name</th>\n",
       "      <th>organism</th>\n",
       "      <th>donor_sex</th>\n",
       "      <th>cell_class</th>\n",
       "      <th>cell_subclass</th>\n",
       "      <th>cell_cluster</th>\n",
       "      <th>n_genes_by_counts</th>\n",
       "      <th>log1p_n_genes_by_counts</th>\n",
       "      <th>total_counts</th>\n",
       "      <th>log1p_total_counts</th>\n",
       "      <th>...</th>\n",
       "      <th>pct_counts_in_top_200_genes</th>\n",
       "      <th>pct_counts_in_top_500_genes</th>\n",
       "      <th>total_counts_mt</th>\n",
       "      <th>log1p_total_counts_mt</th>\n",
       "      <th>pct_counts_mt</th>\n",
       "      <th>n_counts</th>\n",
       "      <th>batch</th>\n",
       "      <th>celltype</th>\n",
       "      <th>str_batch</th>\n",
       "      <th>batch_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F2S4_151217_005_B01</th>\n",
       "      <td>F2S4_151217_005_B01</td>\n",
       "      <td>Mus musculus</td>\n",
       "      <td>M</td>\n",
       "      <td>GABAergic</td>\n",
       "      <td>Pvalb</td>\n",
       "      <td>Pvalb Tpbg</td>\n",
       "      <td>8542</td>\n",
       "      <td>9.052868</td>\n",
       "      <td>1282648.0</td>\n",
       "      <td>14.064438</td>\n",
       "      <td>...</td>\n",
       "      <td>30.921110</td>\n",
       "      <td>45.899343</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1282648.0</td>\n",
       "      <td>F2S4_151217_005_B01</td>\n",
       "      <td>Pvalb</td>\n",
       "      <td>F2S4_151217_005_B01</td>\n",
       "      <td>449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F2S4_151217_005_C01</th>\n",
       "      <td>F2S4_151217_005_C01</td>\n",
       "      <td>Mus musculus</td>\n",
       "      <td>M</td>\n",
       "      <td>Glutamatergic</td>\n",
       "      <td>L4</td>\n",
       "      <td>L4 IT VISp Rspo1</td>\n",
       "      <td>8111</td>\n",
       "      <td>9.001100</td>\n",
       "      <td>1129496.0</td>\n",
       "      <td>13.937283</td>\n",
       "      <td>...</td>\n",
       "      <td>25.959809</td>\n",
       "      <td>39.415102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1129496.0</td>\n",
       "      <td>F2S4_151217_005_C01</td>\n",
       "      <td>L4</td>\n",
       "      <td>F2S4_151217_005_C01</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F2S4_151217_005_E01</th>\n",
       "      <td>F2S4_151217_005_E01</td>\n",
       "      <td>Mus musculus</td>\n",
       "      <td>M</td>\n",
       "      <td>Glutamatergic</td>\n",
       "      <td>L4</td>\n",
       "      <td>L4 IT VISp Rspo1</td>\n",
       "      <td>8780</td>\n",
       "      <td>9.080346</td>\n",
       "      <td>1557742.0</td>\n",
       "      <td>14.258749</td>\n",
       "      <td>...</td>\n",
       "      <td>27.911361</td>\n",
       "      <td>42.299880</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1557742.0</td>\n",
       "      <td>F2S4_151217_005_E01</td>\n",
       "      <td>L4</td>\n",
       "      <td>F2S4_151217_005_E01</td>\n",
       "      <td>451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F2S4_151217_005_F01</th>\n",
       "      <td>F2S4_151217_005_F01</td>\n",
       "      <td>Mus musculus</td>\n",
       "      <td>M</td>\n",
       "      <td>Glutamatergic</td>\n",
       "      <td>L4</td>\n",
       "      <td>L4 IT VISp Rspo1</td>\n",
       "      <td>8498</td>\n",
       "      <td>9.047704</td>\n",
       "      <td>1306856.0</td>\n",
       "      <td>14.083136</td>\n",
       "      <td>...</td>\n",
       "      <td>26.350723</td>\n",
       "      <td>40.343236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1306856.0</td>\n",
       "      <td>F2S4_151217_005_F01</td>\n",
       "      <td>L4</td>\n",
       "      <td>F2S4_151217_005_F01</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F2S4_151217_005_G01</th>\n",
       "      <td>F2S4_151217_005_G01</td>\n",
       "      <td>Mus musculus</td>\n",
       "      <td>M</td>\n",
       "      <td>Glutamatergic</td>\n",
       "      <td>L4</td>\n",
       "      <td>L4 IT VISp Rspo1</td>\n",
       "      <td>7566</td>\n",
       "      <td>8.931552</td>\n",
       "      <td>1119824.0</td>\n",
       "      <td>13.928683</td>\n",
       "      <td>...</td>\n",
       "      <td>27.235976</td>\n",
       "      <td>41.983919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1119824.0</td>\n",
       "      <td>F2S4_151217_005_G01</td>\n",
       "      <td>L4</td>\n",
       "      <td>F2S4_151217_005_G01</td>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1S4_180124_317_D01</th>\n",
       "      <td>F1S4_180124_317_D01</td>\n",
       "      <td>Mus musculus</td>\n",
       "      <td>F</td>\n",
       "      <td>GABAergic</td>\n",
       "      <td>Lamp5</td>\n",
       "      <td>Lamp5 Lsp1</td>\n",
       "      <td>8601</td>\n",
       "      <td>9.059750</td>\n",
       "      <td>1022536.0</td>\n",
       "      <td>13.837797</td>\n",
       "      <td>...</td>\n",
       "      <td>32.826326</td>\n",
       "      <td>47.307968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1022536.0</td>\n",
       "      <td>F1S4_180124_317_D01</td>\n",
       "      <td>Lamp5</td>\n",
       "      <td>F1S4_180124_317_D01</td>\n",
       "      <td>444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1S4_180124_317_E01</th>\n",
       "      <td>F1S4_180124_317_E01</td>\n",
       "      <td>Mus musculus</td>\n",
       "      <td>F</td>\n",
       "      <td>GABAergic</td>\n",
       "      <td>Sncg</td>\n",
       "      <td>Sncg Slc17a8</td>\n",
       "      <td>9823</td>\n",
       "      <td>9.192584</td>\n",
       "      <td>724679.0</td>\n",
       "      <td>13.493485</td>\n",
       "      <td>...</td>\n",
       "      <td>34.913665</td>\n",
       "      <td>50.023942</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>724679.0</td>\n",
       "      <td>F1S4_180124_317_E01</td>\n",
       "      <td>Sncg</td>\n",
       "      <td>F1S4_180124_317_E01</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1S4_180124_317_F01</th>\n",
       "      <td>F1S4_180124_317_F01</td>\n",
       "      <td>Mus musculus</td>\n",
       "      <td>F</td>\n",
       "      <td>GABAergic</td>\n",
       "      <td>Sst</td>\n",
       "      <td>Sst Hpse Sema3c</td>\n",
       "      <td>8730</td>\n",
       "      <td>9.074635</td>\n",
       "      <td>980851.0</td>\n",
       "      <td>13.796177</td>\n",
       "      <td>...</td>\n",
       "      <td>33.757217</td>\n",
       "      <td>48.250346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>980851.0</td>\n",
       "      <td>F1S4_180124_317_F01</td>\n",
       "      <td>Sst</td>\n",
       "      <td>F1S4_180124_317_F01</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1S4_180124_317_G01</th>\n",
       "      <td>F1S4_180124_317_G01</td>\n",
       "      <td>Mus musculus</td>\n",
       "      <td>F</td>\n",
       "      <td>GABAergic</td>\n",
       "      <td>Sncg</td>\n",
       "      <td>Sncg Slc17a8</td>\n",
       "      <td>10633</td>\n",
       "      <td>9.271812</td>\n",
       "      <td>1029766.0</td>\n",
       "      <td>13.844843</td>\n",
       "      <td>...</td>\n",
       "      <td>33.706881</td>\n",
       "      <td>48.938594</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1029766.0</td>\n",
       "      <td>F1S4_180124_317_G01</td>\n",
       "      <td>Sncg</td>\n",
       "      <td>F1S4_180124_317_G01</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1S4_180124_317_H01</th>\n",
       "      <td>F1S4_180124_317_H01</td>\n",
       "      <td>Mus musculus</td>\n",
       "      <td>F</td>\n",
       "      <td>GABAergic</td>\n",
       "      <td>Sncg</td>\n",
       "      <td>Sncg Slc17a8</td>\n",
       "      <td>9773</td>\n",
       "      <td>9.187481</td>\n",
       "      <td>965509.0</td>\n",
       "      <td>13.780412</td>\n",
       "      <td>...</td>\n",
       "      <td>31.813479</td>\n",
       "      <td>46.717120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>965509.0</td>\n",
       "      <td>F1S4_180124_317_H01</td>\n",
       "      <td>Sncg</td>\n",
       "      <td>F1S4_180124_317_H01</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21697 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             sample_name      organism donor_sex  \\\n",
       "F2S4_151217_005_B01  F2S4_151217_005_B01  Mus musculus         M   \n",
       "F2S4_151217_005_C01  F2S4_151217_005_C01  Mus musculus         M   \n",
       "F2S4_151217_005_E01  F2S4_151217_005_E01  Mus musculus         M   \n",
       "F2S4_151217_005_F01  F2S4_151217_005_F01  Mus musculus         M   \n",
       "F2S4_151217_005_G01  F2S4_151217_005_G01  Mus musculus         M   \n",
       "...                                  ...           ...       ...   \n",
       "F1S4_180124_317_D01  F1S4_180124_317_D01  Mus musculus         F   \n",
       "F1S4_180124_317_E01  F1S4_180124_317_E01  Mus musculus         F   \n",
       "F1S4_180124_317_F01  F1S4_180124_317_F01  Mus musculus         F   \n",
       "F1S4_180124_317_G01  F1S4_180124_317_G01  Mus musculus         F   \n",
       "F1S4_180124_317_H01  F1S4_180124_317_H01  Mus musculus         F   \n",
       "\n",
       "                        cell_class cell_subclass      cell_cluster  \\\n",
       "F2S4_151217_005_B01      GABAergic         Pvalb        Pvalb Tpbg   \n",
       "F2S4_151217_005_C01  Glutamatergic            L4  L4 IT VISp Rspo1   \n",
       "F2S4_151217_005_E01  Glutamatergic            L4  L4 IT VISp Rspo1   \n",
       "F2S4_151217_005_F01  Glutamatergic            L4  L4 IT VISp Rspo1   \n",
       "F2S4_151217_005_G01  Glutamatergic            L4  L4 IT VISp Rspo1   \n",
       "...                            ...           ...               ...   \n",
       "F1S4_180124_317_D01      GABAergic         Lamp5        Lamp5 Lsp1   \n",
       "F1S4_180124_317_E01      GABAergic          Sncg      Sncg Slc17a8   \n",
       "F1S4_180124_317_F01      GABAergic           Sst   Sst Hpse Sema3c   \n",
       "F1S4_180124_317_G01      GABAergic          Sncg      Sncg Slc17a8   \n",
       "F1S4_180124_317_H01      GABAergic          Sncg      Sncg Slc17a8   \n",
       "\n",
       "                     n_genes_by_counts  log1p_n_genes_by_counts  total_counts  \\\n",
       "F2S4_151217_005_B01               8542                 9.052868     1282648.0   \n",
       "F2S4_151217_005_C01               8111                 9.001100     1129496.0   \n",
       "F2S4_151217_005_E01               8780                 9.080346     1557742.0   \n",
       "F2S4_151217_005_F01               8498                 9.047704     1306856.0   \n",
       "F2S4_151217_005_G01               7566                 8.931552     1119824.0   \n",
       "...                                ...                      ...           ...   \n",
       "F1S4_180124_317_D01               8601                 9.059750     1022536.0   \n",
       "F1S4_180124_317_E01               9823                 9.192584      724679.0   \n",
       "F1S4_180124_317_F01               8730                 9.074635      980851.0   \n",
       "F1S4_180124_317_G01              10633                 9.271812     1029766.0   \n",
       "F1S4_180124_317_H01               9773                 9.187481      965509.0   \n",
       "\n",
       "                     log1p_total_counts  ...  pct_counts_in_top_200_genes  \\\n",
       "F2S4_151217_005_B01           14.064438  ...                    30.921110   \n",
       "F2S4_151217_005_C01           13.937283  ...                    25.959809   \n",
       "F2S4_151217_005_E01           14.258749  ...                    27.911361   \n",
       "F2S4_151217_005_F01           14.083136  ...                    26.350723   \n",
       "F2S4_151217_005_G01           13.928683  ...                    27.235976   \n",
       "...                                 ...  ...                          ...   \n",
       "F1S4_180124_317_D01           13.837797  ...                    32.826326   \n",
       "F1S4_180124_317_E01           13.493485  ...                    34.913665   \n",
       "F1S4_180124_317_F01           13.796177  ...                    33.757217   \n",
       "F1S4_180124_317_G01           13.844843  ...                    33.706881   \n",
       "F1S4_180124_317_H01           13.780412  ...                    31.813479   \n",
       "\n",
       "                     pct_counts_in_top_500_genes  total_counts_mt  \\\n",
       "F2S4_151217_005_B01                    45.899343              0.0   \n",
       "F2S4_151217_005_C01                    39.415102              0.0   \n",
       "F2S4_151217_005_E01                    42.299880              0.0   \n",
       "F2S4_151217_005_F01                    40.343236              0.0   \n",
       "F2S4_151217_005_G01                    41.983919              0.0   \n",
       "...                                          ...              ...   \n",
       "F1S4_180124_317_D01                    47.307968              0.0   \n",
       "F1S4_180124_317_E01                    50.023942              0.0   \n",
       "F1S4_180124_317_F01                    48.250346              0.0   \n",
       "F1S4_180124_317_G01                    48.938594              0.0   \n",
       "F1S4_180124_317_H01                    46.717120              0.0   \n",
       "\n",
       "                     log1p_total_counts_mt  pct_counts_mt   n_counts  \\\n",
       "F2S4_151217_005_B01                    0.0            0.0  1282648.0   \n",
       "F2S4_151217_005_C01                    0.0            0.0  1129496.0   \n",
       "F2S4_151217_005_E01                    0.0            0.0  1557742.0   \n",
       "F2S4_151217_005_F01                    0.0            0.0  1306856.0   \n",
       "F2S4_151217_005_G01                    0.0            0.0  1119824.0   \n",
       "...                                    ...            ...        ...   \n",
       "F1S4_180124_317_D01                    0.0            0.0  1022536.0   \n",
       "F1S4_180124_317_E01                    0.0            0.0   724679.0   \n",
       "F1S4_180124_317_F01                    0.0            0.0   980851.0   \n",
       "F1S4_180124_317_G01                    0.0            0.0  1029766.0   \n",
       "F1S4_180124_317_H01                    0.0            0.0   965509.0   \n",
       "\n",
       "                                   batch  celltype            str_batch  \\\n",
       "F2S4_151217_005_B01  F2S4_151217_005_B01     Pvalb  F2S4_151217_005_B01   \n",
       "F2S4_151217_005_C01  F2S4_151217_005_C01        L4  F2S4_151217_005_C01   \n",
       "F2S4_151217_005_E01  F2S4_151217_005_E01        L4  F2S4_151217_005_E01   \n",
       "F2S4_151217_005_F01  F2S4_151217_005_F01        L4  F2S4_151217_005_F01   \n",
       "F2S4_151217_005_G01  F2S4_151217_005_G01        L4  F2S4_151217_005_G01   \n",
       "...                                  ...       ...                  ...   \n",
       "F1S4_180124_317_D01  F1S4_180124_317_D01     Lamp5  F1S4_180124_317_D01   \n",
       "F1S4_180124_317_E01  F1S4_180124_317_E01      Sncg  F1S4_180124_317_E01   \n",
       "F1S4_180124_317_F01  F1S4_180124_317_F01       Sst  F1S4_180124_317_F01   \n",
       "F1S4_180124_317_G01  F1S4_180124_317_G01      Sncg  F1S4_180124_317_G01   \n",
       "F1S4_180124_317_H01  F1S4_180124_317_H01      Sncg  F1S4_180124_317_H01   \n",
       "\n",
       "                    batch_id  \n",
       "F2S4_151217_005_B01      449  \n",
       "F2S4_151217_005_C01      450  \n",
       "F2S4_151217_005_E01      451  \n",
       "F2S4_151217_005_F01      452  \n",
       "F2S4_151217_005_G01      453  \n",
       "...                      ...  \n",
       "F1S4_180124_317_D01      444  \n",
       "F1S4_180124_317_E01      445  \n",
       "F1S4_180124_317_F01      446  \n",
       "F1S4_180124_317_G01      447  \n",
       "F1S4_180124_317_H01      448  \n",
       "\n",
       "[21697 rows x 22 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "117b1e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Filtering genes by counts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/gibbs/project/zhao/tl688/conda_envs/scgpt/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py:249: ImplicitModificationWarning: Trying to modify attribute `.var` of view, initializing view as actual.\n",
      "  adata.var['n_counts'] = number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Filtering cells by counts ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# set up the preprocessor, use the args to config the workflow\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=3,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=config.n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "preprocessor(adata, batch_key=\"batch\" if dataset_name != \"heart_cell\" else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d668dfa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 21697 × 14894\n",
       "    obs: 'sample_name', 'organism', 'donor_sex', 'cell_class', 'cell_subclass', 'cell_cluster', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'n_counts', 'batch', 'celltype', 'str_batch', 'batch_id'\n",
       "    var: 'mt', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm', 'gene_name', 'id_in_vocab', 'n_counts'\n",
       "    uns: 'cell_class_colors', 'cell_subclass_colors', 'hvg', 'neighbors', 'pca', 'umap'\n",
       "    obsm: 'X_pca', 'X_umap', 'bin_edges'\n",
       "    varm: 'PCs'\n",
       "    layers: 'X_normed', 'X_binned'\n",
       "    obsp: 'connectivities', 'distances'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fdb6608",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hvg = len(adata.var_names) # number of highly variable genes\n",
    "max_seq_len = n_hvg + 1\n",
    "per_seq_batch_sample = False\n",
    "DSBN = True  # Domain-spec batchnorm\n",
    "explicit_zero_prob = True  # whether explicit bernoulli for zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a21e6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - train set number of samples: 21675, \n",
      "\t feature length: 14895\n",
      "scGPT - INFO - valid set number of samples: 22, \n",
      "\t feature length: 14895\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "if per_seq_batch_sample:\n",
    "    # sort the adata by batch_id in advance\n",
    "    adata_sorted = adata[adata.obs[\"batch_id\"].argsort()].copy()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Tokenize input\n",
    "\n",
    "# %%\n",
    "input_layer_key = \"X_binned\"\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].A\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "celltypes_labels = adata.obs[\"celltype\"].tolist()  # make sure count from 0\n",
    "num_types = len(set(celltypes_labels))\n",
    "celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "num_batch_types = len(set(batch_ids))\n",
    "batch_ids = np.array(batch_ids)\n",
    "\n",
    "(\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_celltype_labels,\n",
    "    valid_celltype_labels,\n",
    "    train_batch_labels,\n",
    "    valid_batch_labels,\n",
    ") = train_test_split(\n",
    "    all_counts, celltypes_labels, batch_ids, test_size=0.001, shuffle=True\n",
    ")\n",
    "\n",
    "# %%\n",
    "if config.load_model is None:\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(vocab(genes), dtype=int)\n",
    "\n",
    "# %%\n",
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=True,\n",
    ")\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=True,\n",
    ")\n",
    "logger.info(\n",
    "    f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    ")\n",
    "\n",
    "\n",
    "# %%\n",
    "def prepare_data(sort_seq_batch=False) -> Tuple[Dict[str, torch.Tensor]]:\n",
    "    masked_values_train = random_mask_value(\n",
    "        tokenized_train[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    masked_values_valid = random_mask_value(\n",
    "        tokenized_valid[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    print(\n",
    "        f\"random masking at epoch {epoch:3d}, ratio of masked values in train: \",\n",
    "        f\"{(masked_values_train == mask_value).sum() / (masked_values_train - pad_value).count_nonzero():.4f}\",\n",
    "    )\n",
    "\n",
    "    input_gene_ids_train, input_gene_ids_valid = (\n",
    "        tokenized_train[\"genes\"],\n",
    "        tokenized_valid[\"genes\"],\n",
    "    )\n",
    "    input_values_train, input_values_valid = masked_values_train, masked_values_valid\n",
    "    target_values_train, target_values_valid = (\n",
    "        tokenized_train[\"values\"],\n",
    "        tokenized_valid[\"values\"],\n",
    "    )\n",
    "\n",
    "    tensor_batch_labels_train = torch.from_numpy(train_batch_labels).long()\n",
    "    tensor_batch_labels_valid = torch.from_numpy(valid_batch_labels).long()\n",
    "\n",
    "    if sort_seq_batch:\n",
    "        train_sort_ids = np.argsort(train_batch_labels)\n",
    "        input_gene_ids_train = input_gene_ids_train[train_sort_ids]\n",
    "        input_values_train = input_values_train[train_sort_ids]\n",
    "        target_values_train = target_values_train[train_sort_ids]\n",
    "        tensor_batch_labels_train = tensor_batch_labels_train[train_sort_ids]\n",
    "\n",
    "        valid_sort_ids = np.argsort(valid_batch_labels)\n",
    "        input_gene_ids_valid = input_gene_ids_valid[valid_sort_ids]\n",
    "        input_values_valid = input_values_valid[valid_sort_ids]\n",
    "        target_values_valid = target_values_valid[valid_sort_ids]\n",
    "        tensor_batch_labels_valid = tensor_batch_labels_valid[valid_sort_ids]\n",
    "\n",
    "    train_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_train,\n",
    "        \"values\": input_values_train,\n",
    "        \"target_values\": target_values_train,\n",
    "        \"batch_labels\": tensor_batch_labels_train,\n",
    "    }\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_valid,\n",
    "        \"values\": input_values_valid,\n",
    "        \"target_values\": target_values_valid,\n",
    "        \"batch_labels\": tensor_batch_labels_valid,\n",
    "    }\n",
    "\n",
    "    return train_data_pt, valid_data_pt\n",
    "\n",
    "\n",
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    intra_domain_shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    if per_seq_batch_sample:\n",
    "        # find the indices of samples in each seq batch\n",
    "        subsets = []\n",
    "        batch_labels_array = data_pt[\"batch_labels\"].numpy()\n",
    "        for batch_label in np.unique(batch_labels_array):\n",
    "            batch_indices = np.where(batch_labels_array == batch_label)[0].tolist()\n",
    "            subsets.append(batch_indices)\n",
    "        data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_sampler=SubsetsBatchSampler(\n",
    "                subsets,\n",
    "                batch_size,\n",
    "                intra_subset_shuffle=intra_domain_shuffle,\n",
    "                inter_subset_shuffle=shuffle,\n",
    "                drop_last=drop_last,\n",
    "            ),\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return data_loader\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143a09dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7998a764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use domain specific batchnorm with affine=False\n",
      "scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([36574, 512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.0.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.bias with shape torch.Size([1])\n",
      "scGPT - INFO - Loading params mvc_decoder.gene2query.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params mvc_decoder.gene2query.bias with shape torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Create and finetune scGPT\n",
    "\n",
    "# %%\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    vocab=vocab,\n",
    "    dropout=config.dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    do_mvc=config.GEPC,\n",
    "    do_dab=True,\n",
    "    use_batch_labels=True,\n",
    "    num_batch_labels=num_batch_types,\n",
    "    domain_spec_batchnorm=DSBN,\n",
    "    n_input_bins=n_input_bins,\n",
    "    ecs_threshold=config.ecs_thres,\n",
    "    explicit_zero_prob=explicit_zero_prob,\n",
    "    use_fast_transformer=True,\n",
    "    pre_norm=config.pre_norm,\n",
    ")\n",
    "if config.load_model is not None:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "        logger.info(f\"Loading all model params from {model_file}\")\n",
    "    except:\n",
    "        # only load params that are in the model and match the size\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_file)\n",
    "        pretrained_dict = {\n",
    "            k: v\n",
    "            for k, v in pretrained_dict.items()\n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        for k, v in pretrained_dict.items():\n",
    "            logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "model.to(device)\n",
    "# wandb.watch(model)\n",
    "\n",
    "\n",
    "# criterion = masked_mse_loss\n",
    "criterion = masked_ce_loss\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.transformer_encoder.layers[-2].parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "criterion_dab = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(\n",
    "#     model.parameters(), lr=config.lr,\n",
    "# )\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=config.lr, eps=1e-4 if config.amp else 1e-8\n",
    ")\n",
    "\n",
    "# optimizer = SophiaG(model.parameters(), lr=config.lr, betas=(0.965, 0.99), rho = 0.01, weight_decay=1e-1)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=config.schedule_ratio)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e1a2518",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model: nn.Module, loader: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_mse, total_gepc = 0.0, 0.0, 0.0\n",
    "    total_error = 0.0\n",
    "    log_interval = config.log_interval\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "        input_values = batch_data[\"values\"].to(device)\n",
    "        target_values = batch_data[\"target_values\"].to(device)\n",
    "        batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "        \n",
    "        src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "        with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=batch_labels if DSBN else None,\n",
    "                MVC=config.GEPC,\n",
    "                ECS=config.ecs_thres > 0,\n",
    "            )\n",
    "\n",
    "            masked_positions = input_values.eq(mask_value)  # the postions to predict\n",
    "            loss = loss_mse = criterion(\n",
    "                output_dict[\"mlm_output\"], target_values, masked_positions\n",
    "            )\n",
    "            \n",
    "            \n",
    "            # loss = loss_mse = torch.FloatTensor([0.0]).cuda()\n",
    "            \n",
    "            metrics_to_log = {\"train/mse\": loss_mse.item()}\n",
    "            \n",
    "            if explicit_zero_prob:\n",
    "                loss_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "                    output_dict[\"mlm_zero_probs\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_zero_log_prob\n",
    "                metrics_to_log.update({\"train/nzlp\": loss_zero_log_prob.item()})\n",
    "                \n",
    "            if config.GEPC:\n",
    "                loss_gepc = criterion(\n",
    "                    output_dict[\"mvc_output\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_gepc\n",
    "                metrics_to_log.update({\"train/mvc\": loss_gepc.item()})\n",
    "                \n",
    "            if config.GEPC and explicit_zero_prob:\n",
    "                loss_gepc_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "                    output_dict[\"mvc_zero_probs\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_gepc_zero_log_prob\n",
    "                metrics_to_log.update(\n",
    "                    {\"train/mvc_nzlp\": loss_gepc_zero_log_prob.item()}\n",
    "                )\n",
    "                \n",
    "            if config.ecs_thres > 0:\n",
    "                loss_ecs = 10 * output_dict[\"loss_ecs\"]\n",
    "                loss = loss + loss_ecs\n",
    "                metrics_to_log.update({\"train/ecs\": loss_ecs.item()})\n",
    "                \n",
    "                \n",
    "#             loss_gepc = torch.FloatTensor([0.0])\n",
    "            \n",
    "            loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "#             loss = loss + config.dab_weight * loss_dab\n",
    "            loss_dab = torch.FloatTensor([0.0])\n",
    "            metrics_to_log.update({\"train/dab\": loss_dab.item()})\n",
    "\n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.filterwarnings(\"always\")\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                1.0,\n",
    "                error_if_nonfinite=False if scaler.is_enabled() else True,\n",
    "            )\n",
    "            if len(w) > 0:\n",
    "                logger.warning(\n",
    "                    f\"Found infinite gradient. This may be caused by the gradient \"\n",
    "                    f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n",
    "                    \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n",
    "                )\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        wandb.log(metrics_to_log)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mre = masked_relative_error(\n",
    "                output_dict[\"mlm_output\"], target_values, masked_positions\n",
    "            )\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_mse += loss_mse.item()\n",
    "        total_gepc += loss_gepc.item() if config.GEPC else 0.0\n",
    "        total_error += mre.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            cur_mse = total_mse / log_interval\n",
    "            cur_gepc = total_gepc / log_interval if config.GEPC else 0.0\n",
    "            cur_error = total_error / log_interval\n",
    "            # ppl = math.exp(cur_loss)\n",
    "            logger.info(\n",
    "                f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "                f\"lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | mse {cur_mse:5.2f} | mre {cur_error:5.2f} |\"\n",
    "                + (f\"gepc {cur_gepc:5.2f} |\" if config.GEPC else \"\")\n",
    "            )\n",
    "            total_loss = 0\n",
    "            total_mse = 0\n",
    "            total_gepc = 0\n",
    "            total_error = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def define_wandb_metrcis():\n",
    "    wandb.define_metric(\"valid/mse\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/mre\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/sum_mse_dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"test/avg_bio\", summary=\"max\")\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_error = 0.0\n",
    "    total_dab = 0.0\n",
    "    total_num = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            target_values = batch_data[\"target_values\"].to(device)\n",
    "            batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "\n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(\n",
    "                    input_gene_ids,\n",
    "                    input_values,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    batch_labels=batch_labels if DSBN else None,\n",
    "                )\n",
    "                output_values = output_dict[\"mlm_output\"]\n",
    "\n",
    "                masked_positions = input_values.eq(mask_value)\n",
    "                loss = criterion(output_values, target_values, masked_positions)\n",
    "                loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "\n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            total_error += masked_relative_error(\n",
    "                output_values, target_values, masked_positions\n",
    "            ).item() * len(input_gene_ids)\n",
    "            total_dab += loss_dab.item() * len(input_gene_ids)\n",
    "            total_num += len(input_gene_ids)\n",
    "\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"valid/mse\": total_loss / total_num,\n",
    "            \"valid/mre\": total_error / total_num,\n",
    "            \"valid/dab\": total_dab / total_num,\n",
    "            \"valid/sum_mse_dab\": (total_loss + config.dab_weight * total_dab)\n",
    "            / total_num,\n",
    "            \"epoch\": epoch,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return total_loss / total_num, total_error / total_num\n",
    "\n",
    "\n",
    "def eval_testdata(\n",
    "    model: nn.Module,\n",
    "    adata_t: AnnData,\n",
    "    include_types: List[str] = [\"cls\"],\n",
    ") -> Optional[Dict]:\n",
    "    \"\"\"evaluate the model on test dataset of adata_t\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # copy adata_t to avoid reuse previously computed results stored in adata_t\n",
    "    adata_t = adata_t.copy()\n",
    "\n",
    "    all_counts = (\n",
    "        adata_t.layers[input_layer_key].A\n",
    "        if issparse(adata_t.layers[input_layer_key])\n",
    "        else adata_t.layers[input_layer_key]\n",
    "    )\n",
    "\n",
    "    celltypes_labels = adata_t.obs[\"celltype\"].tolist()\n",
    "    celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "    batch_ids = adata_t.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "\n",
    "    # Evaluate cls cell embeddings\n",
    "    if \"cls\" in include_types:\n",
    "        logger.info(\"Evaluating cls cell embeddings\")\n",
    "        tokenized_all = tokenize_and_pad_batch(\n",
    "            all_counts,\n",
    "            gene_ids,\n",
    "            max_len=max_seq_len,\n",
    "            vocab=vocab,\n",
    "            pad_token=pad_token,\n",
    "            pad_value=pad_value,\n",
    "            append_cls=True,  # append <cls> token at the beginning\n",
    "            include_zero_gene=True,\n",
    "        )\n",
    "        all_gene_ids, all_values = tokenized_all[\"genes\"], tokenized_all[\"values\"]\n",
    "        src_key_padding_mask = all_gene_ids.eq(vocab[pad_token])\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            cell_embeddings = model.encode_batch(\n",
    "                all_gene_ids,\n",
    "                all_values.float(),\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_size=config.batch_size,\n",
    "                batch_labels=torch.from_numpy(batch_ids).long() if DSBN else None,\n",
    "                time_step=0,\n",
    "                return_np=True,\n",
    "            )\n",
    "        cell_embeddings = cell_embeddings / np.linalg.norm(\n",
    "            cell_embeddings, axis=1, keepdims=True\n",
    "        )\n",
    "\n",
    "        adata_t.obsm[\"X_scGPT\"] = cell_embeddings\n",
    "\n",
    "        results = {}\n",
    "        try:\n",
    "            results = eval_scib_metrics(adata_t)\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            logger.error(e)\n",
    "\n",
    "        sc.pp.neighbors(adata_t, use_rep=\"X_scGPT\")\n",
    "        sc.tl.umap(adata_t, min_dist=0.3)\n",
    "        fig = sc.pl.umap(\n",
    "            adata_t,\n",
    "            color=[\"str_batch\"],\n",
    "            title=[f\"batch, avg_bio = {results.get('avg_bio', 0.0):.4f}\"],\n",
    "            frameon=False,\n",
    "            return_fig=True,\n",
    "            show=False,\n",
    "        )\n",
    "\n",
    "        results[\"batch_umap\"] = fig\n",
    "\n",
    "        sc.pp.neighbors(adata_t, use_rep=\"X_scGPT\")\n",
    "        sc.tl.umap(adata_t, min_dist=0.3)\n",
    "        fig = sc.pl.umap(\n",
    "            adata_t,\n",
    "            color=[\"celltype\"],\n",
    "            title=[\n",
    "                f\"celltype, avg_bio = {results.get('avg_bio', 0.0):.4f}\",\n",
    "            ],\n",
    "            frameon=False,\n",
    "            return_fig=True,\n",
    "            show=False,\n",
    "        )\n",
    "\n",
    "        results[\"celltype_umap\"] = fig\n",
    "\n",
    "    if len(include_types) == 1:\n",
    "        return results\n",
    "    \n",
    "    \n",
    "def return_testdata(\n",
    "    model: nn.Module,\n",
    "    adata_t: AnnData,\n",
    "    include_types: List[str] = [\"cls\"],\n",
    ") -> Optional[Dict]:\n",
    "    \"\"\"evaluate the model on test dataset of adata_t\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # copy adata_t to avoid reuse previously computed results stored in adata_t\n",
    "    adata_t = adata_t.copy()\n",
    "\n",
    "    all_counts = (\n",
    "        adata_t.layers[input_layer_key].A\n",
    "        if issparse(adata_t.layers[input_layer_key])\n",
    "        else adata_t.layers[input_layer_key]\n",
    "    )\n",
    "\n",
    "    celltypes_labels = adata_t.obs[\"celltype\"].tolist()\n",
    "    celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "    batch_ids = adata_t.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "\n",
    "    # Evaluate cls cell embeddings\n",
    "    if \"cls\" in include_types:\n",
    "        logger.info(\"Evaluating cls cell embeddings\")\n",
    "        tokenized_all = tokenize_and_pad_batch(\n",
    "            all_counts,\n",
    "            gene_ids,\n",
    "            max_len=max_seq_len,\n",
    "            vocab=vocab,\n",
    "            pad_token=pad_token,\n",
    "            pad_value=pad_value,\n",
    "            append_cls=True,  # append <cls> token at the beginning\n",
    "            include_zero_gene=True,\n",
    "        )\n",
    "        all_gene_ids, all_values = tokenized_all[\"genes\"], tokenized_all[\"values\"]\n",
    "        src_key_padding_mask = all_gene_ids.eq(vocab[pad_token])\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            cell_embeddings = model.encode_batch(\n",
    "                all_gene_ids,\n",
    "                all_values.float(),\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_size=config.batch_size,\n",
    "                batch_labels=torch.from_numpy(batch_ids).long() if DSBN else None,\n",
    "                time_step=0,\n",
    "                return_np=True,\n",
    "            )\n",
    "        cell_embeddings = cell_embeddings / np.linalg.norm(\n",
    "            cell_embeddings, axis=1, keepdims=True\n",
    "        )\n",
    "        \n",
    "    return cell_embeddings\n",
    "\n",
    "\n",
    "def simulation_data(\n",
    "    model: nn.Module,\n",
    "    adata_t: AnnData,\n",
    "    include_types: List[str] = [\"cls\"],\n",
    ") -> Optional[Dict]:\n",
    "    \"\"\"evaluate the model on test dataset of adata_t\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    mlm_out = []\n",
    "    \n",
    "    # copy adata_t to avoid reuse previously computed results stored in adata_t\n",
    "    adata_t = adata_t.copy()\n",
    "\n",
    "    for i in adata_t.obs_names:\n",
    "        adata_new = adata_t[i,:]\n",
    "\n",
    "        all_counts = (\n",
    "            adata_new.layers[input_layer_key].A\n",
    "            if issparse(adata_new.layers[input_layer_key])\n",
    "            else adata_new.layers[input_layer_key]\n",
    "        )\n",
    "\n",
    "        celltypes_labels = adata_new.obs[\"celltype\"].tolist()\n",
    "        celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "        batch_ids = adata_new.obs[\"batch_id\"].tolist()\n",
    "        batch_ids = np.array(batch_ids)\n",
    "\n",
    "        # Evaluate cls cell embeddings\n",
    "        if \"cls\" in include_types:\n",
    "            # logger.info(\"Evaluating cls cell embeddings\")\n",
    "            tokenized_all = tokenize_and_pad_batch(\n",
    "                all_counts,\n",
    "                gene_ids,\n",
    "                max_len=max_seq_len,\n",
    "                vocab=vocab,\n",
    "                pad_token=pad_token,\n",
    "                pad_value=pad_value,\n",
    "                append_cls=True,  # append <cls> token at the beginning\n",
    "                include_zero_gene=True,\n",
    "            )\n",
    "            all_gene_ids, all_values = tokenized_all[\"genes\"], tokenized_all[\"values\"]\n",
    "            src_key_padding_mask = all_gene_ids.eq(vocab[pad_token])\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(\n",
    "                    all_gene_ids.cuda(),\n",
    "                    all_values.float().cuda(),\n",
    "                    src_key_padding_mask=src_key_padding_mask.cuda(),\n",
    "                    batch_labels=torch.from_numpy(batch_ids).long().cuda()  if DSBN else None,\n",
    "                    MVC=config.GEPC,\n",
    "                    ECS=config.ecs_thres > 0,\n",
    "                )\n",
    "                \n",
    "                mlm_out.append(output_dict[\"mlm_output\"].cpu().detach().numpy()[0] \n",
    "                               * ((output_dict[\"mlm_zero_probs\"]>0.5)*1).cpu().detach().numpy()[0])\n",
    "    \n",
    "    return mlm_out\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bc7b195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random masking at epoch   1, ratio of masked values in train:  0.4000\n",
      "scGPT - INFO - | epoch   1 | 100/678 batches | lr 0.0001 | ms/batch 4121.25 | loss 276.86 | mse 100.67 | mre 2312504.62 |gepc 163.73 |\n",
      "scGPT - INFO - | epoch   1 | 200/678 batches | lr 0.0001 | ms/batch 4093.47 | loss 264.59 | mse 99.71 | mre 2342625.58 |gepc 152.59 |\n",
      "scGPT - INFO - | epoch   1 | 300/678 batches | lr 0.0001 | ms/batch 4092.37 | loss 262.95 | mse 99.43 | mre 2415035.02 |gepc 151.23 |\n",
      "scGPT - INFO - | epoch   1 | 400/678 batches | lr 0.0001 | ms/batch 4092.19 | loss 262.48 | mse 99.32 | mre 2426521.65 |gepc 150.90 |\n",
      "scGPT - INFO - | epoch   1 | 500/678 batches | lr 0.0001 | ms/batch 4092.82 | loss 261.88 | mse 98.95 | mre 2451068.65 |gepc 150.73 |\n",
      "scGPT - INFO - | epoch   1 | 600/678 batches | lr 0.0001 | ms/batch 4092.97 | loss 260.31 | mse 98.06 | mre 2449176.47 |gepc 150.04 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   1 | time: 2782.34s | valid loss/mse 102.9176 | mre 2258901.2500\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 102.9176\n",
      "scGPT - INFO - Evaluating cls cell embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 679/679 [28:52<00:00,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI...\n",
      "ARI...\n",
      "Silhouette score...\n",
      "scGPT - ERROR - No objects to concatenate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1933574/3680721097.py\", line 241, in eval_testdata\n",
      "    results = eval_scib_metrics(adata_t)\n",
      "  File \"/gpfs/gibbs/project/zhao/tl688/conda_envs/scgpt/lib/python3.8/site-packages/scgpt/utils/util.py\", line 273, in eval_scib_metrics\n",
      "    results = scib.metrics.metrics(\n",
      "  File \"/gpfs/gibbs/project/zhao/tl688/conda_envs/scgpt/lib/python3.8/site-packages/scib/metrics/metrics.py\", line 333, in metrics\n",
      "    asw_batch = silhouette_batch(\n",
      "  File \"/gpfs/gibbs/project/zhao/tl688/conda_envs/scgpt/lib/python3.8/site-packages/scib/metrics/silhouette.py\", line 113, in silhouette_batch\n",
      "    sil_df = pd.concat(sil_dfs).reset_index(drop=True)\n",
      "  File \"/gpfs/gibbs/project/zhao/tl688/conda_envs/scgpt/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/gpfs/gibbs/project/zhao/tl688/conda_envs/scgpt/lib/python3.8/site-packages/pandas/core/reshape/concat.py\", line 294, in concat\n",
      "    op = _Concatenator(\n",
      "  File \"/gpfs/gibbs/project/zhao/tl688/conda_envs/scgpt/lib/python3.8/site-packages/pandas/core/reshape/concat.py\", line 351, in __init__\n",
      "    raise ValueError(\"No objects to concatenate\")\n",
      "ValueError: No objects to concatenate\n",
      "/gpfs/gibbs/project/zhao/tl688/conda_envs/scgpt/lib/python3.8/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/gpfs/gibbs/project/zhao/tl688/conda_envs/scgpt/lib/python3.8/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/gpfs/gibbs/project/zhao/tl688/conda_envs/scgpt/lib/python3.8/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/gpfs/gibbs/project/zhao/tl688/conda_envs/scgpt/lib/python3.8/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/gpfs/gibbs/project/zhao/tl688/conda_envs/scgpt/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:163: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = copy(get_cmap(cmap))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m adata\u001b[38;5;241m.\u001b[39mobs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m adata\u001b[38;5;241m.\u001b[39mobs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m adata\u001b[38;5;241m.\u001b[39mobs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcelltype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m adata\u001b[38;5;241m.\u001b[39mobs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcelltype\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43meval_testdata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43madata_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madata_sorted\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mper_seq_batch_sample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_umap\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msavefig(\n\u001b[1;32m     63\u001b[0m     save_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings_batch_umap[cls]_e\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     66\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcelltype_umap\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msavefig(\n\u001b[1;32m     67\u001b[0m     save_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings_celltype_umap[cls]_e\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m\n\u001b[1;32m     68\u001b[0m )\n",
      "Cell \u001b[0;32mIn[11], line 248\u001b[0m, in \u001b[0;36meval_testdata\u001b[0;34m(model, adata_t, include_types)\u001b[0m\n\u001b[1;32m    246\u001b[0m sc\u001b[38;5;241m.\u001b[39mpp\u001b[38;5;241m.\u001b[39mneighbors(adata_t, use_rep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_scGPT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    247\u001b[0m sc\u001b[38;5;241m.\u001b[39mtl\u001b[38;5;241m.\u001b[39mumap(adata_t, min_dist\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)\n\u001b[0;32m--> 248\u001b[0m fig \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mumap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43madata_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstr_batch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch, avg_bio = \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mavg_bio\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m.4f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframeon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_fig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_umap\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m fig\n\u001b[1;32m    259\u001b[0m sc\u001b[38;5;241m.\u001b[39mpp\u001b[38;5;241m.\u001b[39mneighbors(adata_t, use_rep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_scGPT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/gpfs/gibbs/project/zhao/tl688/conda_envs/scgpt/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:669\u001b[0m, in \u001b[0;36mumap\u001b[0;34m(adata, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;129m@_wraps_plot_scatter\u001b[39m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;129m@_doc_params\u001b[39m(\n\u001b[1;32m    612\u001b[0m     adata_color_etc\u001b[38;5;241m=\u001b[39mdoc_adata_color_etc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    616\u001b[0m )\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mumap\u001b[39m(adata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Axes, List[Axes], \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[1;32m    618\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\\\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;124;03m    Scatter plot in UMAP basis.\u001b[39;00m\n\u001b[1;32m    620\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;124;03m    tl.umap\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 669\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43madata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mumap\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/gibbs/project/zhao/tl688/conda_envs/scgpt/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:277\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, dimensions, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, colorbar_loc, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sort_order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m value_to_plot \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m categorical \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;66;03m# Higher values plotted on top, null values on bottom\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     order \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mcolor_vector\u001b[49m, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstable\u001b[39m\u001b[38;5;124m\"\u001b[39m)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sort_order \u001b[38;5;129;01mand\u001b[39;00m categorical:\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;66;03m# Null points go on bottom\u001b[39;00m\n\u001b[1;32m    280\u001b[0m     order \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(\u001b[38;5;241m~\u001b[39mpd\u001b[38;5;241m.\u001b[39misnull(color_source_vector), kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary -: 'str'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 320x320 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 293,
       "width": 316
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### %%time\n",
    "best_val_loss = float(\"inf\")\n",
    "best_avg_bio = 0.0\n",
    "best_model = None\n",
    "define_wandb_metrcis()\n",
    "\n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_data_pt, valid_data_pt = prepare_data(sort_seq_batch=per_seq_batch_sample)\n",
    "    train_loader = prepare_dataloader(\n",
    "        train_data_pt,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        intra_domain_shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    valid_loader = prepare_dataloader(\n",
    "        valid_data_pt,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    if config.do_train:\n",
    "        train(\n",
    "            model,\n",
    "            loader=train_loader,\n",
    "        )\n",
    "    val_loss, val_mre = evaluate(\n",
    "        model,\n",
    "        loader=valid_loader,\n",
    "    )\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(\n",
    "        f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "        f\"valid loss/mse {val_loss:5.4f} | mre {val_mre:5.4f}\"\n",
    "    )\n",
    "    logger.info(\"-\" * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "        logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "\n",
    "    if epoch % config.save_eval_interval == 0 or epoch == config.epochs:\n",
    "        # logger.info(f\"Saving model to {save_dir}\")\n",
    "        # torch.save(best_model.state_dict(), save_dir / f\"model_e{best_model_epoch}.pt\")\n",
    "\n",
    "        # eval on testdata\n",
    "        \n",
    "        adata.obs['batch'] = adata.obs['batch'].astype(\"category\")\n",
    "        adata.obs['celltype'] = adata.obs['celltype'].astype(\"category\")\n",
    "        \n",
    "        results = eval_testdata(\n",
    "            best_model,\n",
    "            adata_t=adata_sorted if per_seq_batch_sample else adata,\n",
    "            include_types=[\"cls\"],\n",
    "        )\n",
    "        results[\"batch_umap\"].savefig(\n",
    "            save_dir / f\"embeddings_batch_umap[cls]_e{best_model_epoch}.png\", dpi=300\n",
    "        )\n",
    "\n",
    "        results[\"celltype_umap\"].savefig(\n",
    "            save_dir / f\"embeddings_celltype_umap[cls]_e{best_model_epoch}.png\", dpi=300\n",
    "        )\n",
    "        metrics_to_log = {\"test/\" + k: v for k, v in results.items()}\n",
    "        metrics_to_log[\"test/batch_umap\"] = wandb.Image(\n",
    "            str(save_dir / f\"embeddings_batch_umap[cls]_e{best_model_epoch}.png\"),\n",
    "            caption=f\"celltype avg_bio epoch {best_model_epoch}\",\n",
    "        )\n",
    "\n",
    "        # metrics_to_log[\"test/celltype_umap\"] = wandb.Image(\n",
    "        #     str(save_dir / f\"embeddings_celltype_umap[cls]_e{best_model_epoch}.png\"),\n",
    "        #     caption=f\"celltype avg_bio epoch {best_model_epoch}\",\n",
    "        # )\n",
    "        # # metrics_to_log[\"test/best_model_epoch\"] = best_model_epoch\n",
    "        # wandb.log(metrics_to_log)\n",
    "        # wandb.log({\"avg_bio\": results.get(\"avg_bio\", 0.0)})\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "# %%\n",
    "# save the best model\n",
    "torch.save(best_model.state_dict(), save_dir / \"best_model.pt\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Gene embeddings\n",
    "\n",
    "# %%\n",
    "artifact = wandb.Artifact(f\"best_model\", type=\"model\")\n",
    "glob_str = os.path.join(save_dir, \"best_model.pt\")\n",
    "artifact.add_file(glob_str)\n",
    "run.log_artifact(artifact)\n",
    "\n",
    "run.finish()\n",
    "wandb.finish()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "833d4d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "898"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d7136d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b31014f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 21697 × 14894\n",
       "    obs: 'sample_name', 'organism', 'donor_sex', 'cell_class', 'cell_subclass', 'cell_cluster', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'n_counts', 'batch', 'celltype', 'str_batch', 'batch_id'\n",
       "    var: 'mt', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm', 'gene_name', 'id_in_vocab', 'n_counts'\n",
       "    uns: 'cell_class_colors', 'cell_subclass_colors', 'hvg', 'neighbors', 'pca', 'umap'\n",
       "    obsm: 'X_pca', 'X_umap', 'bin_edges'\n",
       "    varm: 'PCs'\n",
       "    layers: 'X_normed', 'X_binned'\n",
       "    obsp: 'connectivities', 'distances'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e35f50b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_old = adata.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acd48443",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdict = simulation_data(best_model,\n",
    "    adata_t=adata_sorted if per_seq_batch_sample else adata,\n",
    "    include_types=[\"cls\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18d7886a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21697, 14894)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(outdict)[:,1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5218847",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_new = sc.AnnData(np.array(outdict)[:,1:], obs=adata.obs, var = adata.var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c64d4bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 21697 × 14894\n",
       "    obs: 'sample_name', 'organism', 'donor_sex', 'cell_class', 'cell_subclass', 'cell_cluster', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'n_counts', 'batch', 'celltype', 'str_batch', 'batch_id'\n",
       "    var: 'mt', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm', 'gene_name', 'id_in_vocab', 'n_counts'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a7f12e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_new.write_h5ad(\"scRNA_seq_imputation.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27778154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f4262a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8dce092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import count_nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d56ea012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.7890625 , 3.87109375, 3.91796875, ..., 4.5234375 , 4.23046875,\n",
       "        4.65625   ],\n",
       "       [4.953125  , 5.06640625, 5.109375  , ..., 5.21484375, 5.09375   ,\n",
       "        5.51171875],\n",
       "       [3.79882812, 3.90820312, 3.88867188, ..., 4.5234375 , 3.86914062,\n",
       "        4.0703125 ],\n",
       "       ...,\n",
       "       [4.05078125, 4.0859375 , 4.20703125, ..., 4.25      , 4.33203125,\n",
       "        4.3828125 ],\n",
       "       [4.13671875, 4.1171875 , 4.2109375 , ..., 4.24609375, 4.12109375,\n",
       "        4.65234375],\n",
       "       [3.9609375 , 3.91210938, 4.03125   , ..., 4.05078125, 4.0546875 ,\n",
       "        4.20703125]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_new.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbdf168f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01412883084834815"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - count_nonzero(adata_new.X) / float(adata_new.X.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d41ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70337adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bf5c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_list = []\n",
    "pval_list = []\n",
    "for item in adata.var_names:\n",
    "    adata1 = adata[:,item]\n",
    "    adata2 = adata_new[:,item]\n",
    "    cor, pval = scipy.stats.pearsonr(np.array(adata1.X.todense().T)[0], np.array(adata2.X.T)[0])\n",
    "    cor_list.append(cor)\n",
    "    pval_list.append(pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0c4bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425e066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f017a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5627443c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9b0b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e920b5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 324 × 16562\n",
      "    obs: 'in_tissue', 'array_row', 'array_col', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes', 'total_counts_MT', 'log1p_total_counts_MT', 'pct_counts_MT', 'n_counts', 'leiden', 'cluster'\n",
      "    var: 'gene_ids', 'feature_types', 'genome', 'MT', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
      "    uns: 'cluster_colors', 'hvg', 'leiden', 'leiden_colors', 'neighbors', 'pca', 'spatial', 'umap'\n",
      "    obsm: 'X_pca', 'X_umap', 'spatial'\n",
      "    varm: 'PCs'\n",
      "    obsp: 'connectivities', 'distances'\n",
      "scGPT - INFO - match 13444/16562 genes in vocabulary of size 36574.\n",
      "scGPT - INFO - Resume model from save/scGPT_bc/best_model.pt, the model args will override the config save/scGPT_bc/args.json.\n"
     ]
    }
   ],
   "source": [
    "# adata = sc.read_h5ad(\"/gpfs/gibbs/pi/zhao/tl688/tangram/ST-LN-compressed.h5ad\")\n",
    "adata = sc.read_h5ad(\"/gpfs/gibbs/pi/zhao/tl688/scGPT/examples/mouse_spatial.h5ad\")\n",
    "print(adata)\n",
    "\n",
    "ori_batch_col = \"batch\"\n",
    "adata.obs['batch'] = list(adata.obs['in_tissue'])\n",
    "adata.obs['celltype'] = list(adata.obs['cluster'])\n",
    "adata.obs[\"celltype\"] = adata.obs[\"celltype\"].astype(\"category\")\n",
    "adata.var_names = [i.upper() for i in adata.var_names]\n",
    "data_is_raw = True\n",
    "\n",
    "\n",
    "# %%\n",
    "# make the batch category column \n",
    "adata.obs[\"str_batch\"] = adata.obs[ori_batch_col].astype(str)\n",
    "batch_id_labels = adata.obs[\"str_batch\"].astype(\"category\").cat.codes.values\n",
    "adata.obs[\"batch_id\"] = batch_id_labels\n",
    "\n",
    "adata.var[\"gene_name\"] = adata.var.index.tolist()\n",
    "\n",
    "if config.load_model is not None:\n",
    "    model_dir = Path(config.load_model)\n",
    "    model_config_file = model_dir / \"args.json\"\n",
    "    model_file = model_dir / \"best_model.pt\"\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "    # model\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "    logger.info(\n",
    "        f\"Resume model from {model_file}, the model args will override the \"\n",
    "        f\"config {model_config_file}.\"\n",
    "    )\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce9fe49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in_tissue</th>\n",
       "      <th>array_row</th>\n",
       "      <th>array_col</th>\n",
       "      <th>n_genes_by_counts</th>\n",
       "      <th>log1p_n_genes_by_counts</th>\n",
       "      <th>total_counts</th>\n",
       "      <th>log1p_total_counts</th>\n",
       "      <th>pct_counts_in_top_50_genes</th>\n",
       "      <th>pct_counts_in_top_100_genes</th>\n",
       "      <th>pct_counts_in_top_200_genes</th>\n",
       "      <th>...</th>\n",
       "      <th>total_counts_MT</th>\n",
       "      <th>log1p_total_counts_MT</th>\n",
       "      <th>pct_counts_MT</th>\n",
       "      <th>n_counts</th>\n",
       "      <th>leiden</th>\n",
       "      <th>cluster</th>\n",
       "      <th>batch</th>\n",
       "      <th>celltype</th>\n",
       "      <th>str_batch</th>\n",
       "      <th>batch_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAATGGCATGTCTTGT-1</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>69</td>\n",
       "      <td>5191</td>\n",
       "      <td>8.554874</td>\n",
       "      <td>15977.0</td>\n",
       "      <td>9.678968</td>\n",
       "      <td>20.629655</td>\n",
       "      <td>26.757213</td>\n",
       "      <td>34.743694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15977.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Cortex_1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cortex_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AACAACTGGTAGTTGC-1</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>42</td>\n",
       "      <td>5252</td>\n",
       "      <td>8.566555</td>\n",
       "      <td>16649.0</td>\n",
       "      <td>9.720165</td>\n",
       "      <td>20.481711</td>\n",
       "      <td>26.277855</td>\n",
       "      <td>34.092138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16649.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Cortex_1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cortex_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AACAGGAAATCGAATA-1</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>67</td>\n",
       "      <td>6320</td>\n",
       "      <td>8.751633</td>\n",
       "      <td>23375.0</td>\n",
       "      <td>10.059465</td>\n",
       "      <td>17.929412</td>\n",
       "      <td>23.850267</td>\n",
       "      <td>32.077005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23375.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Cortex_1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cortex_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AACCCAGAGACGGAGA-1</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>39</td>\n",
       "      <td>3659</td>\n",
       "      <td>8.205218</td>\n",
       "      <td>9229.0</td>\n",
       "      <td>9.130215</td>\n",
       "      <td>25.939972</td>\n",
       "      <td>31.964460</td>\n",
       "      <td>39.885145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9229.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Cortex_2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cortex_2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AACCGTTGTGTTTGCT-1</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>64</td>\n",
       "      <td>4512</td>\n",
       "      <td>8.414717</td>\n",
       "      <td>12679.0</td>\n",
       "      <td>9.447782</td>\n",
       "      <td>21.839262</td>\n",
       "      <td>28.038489</td>\n",
       "      <td>36.209480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12679.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Cortex_1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cortex_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTGGATTGGGTACCAC-1</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>55</td>\n",
       "      <td>4980</td>\n",
       "      <td>8.513386</td>\n",
       "      <td>15381.0</td>\n",
       "      <td>9.640953</td>\n",
       "      <td>21.038944</td>\n",
       "      <td>27.059359</td>\n",
       "      <td>35.114752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15381.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Cortex_1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cortex_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTGGCTCGCATGAGAC-1</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>37</td>\n",
       "      <td>4620</td>\n",
       "      <td>8.438366</td>\n",
       "      <td>13193.0</td>\n",
       "      <td>9.487517</td>\n",
       "      <td>20.609414</td>\n",
       "      <td>26.445842</td>\n",
       "      <td>34.063519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13193.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Cortex_3</td>\n",
       "      <td>1</td>\n",
       "      <td>Cortex_3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTGTATCACACAGAAT-1</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>74</td>\n",
       "      <td>6120</td>\n",
       "      <td>8.719481</td>\n",
       "      <td>21951.0</td>\n",
       "      <td>9.996614</td>\n",
       "      <td>18.199626</td>\n",
       "      <td>24.235798</td>\n",
       "      <td>32.440436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21951.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Cortex_1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cortex_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTGTGGCCCTGACAGT-1</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>60</td>\n",
       "      <td>4971</td>\n",
       "      <td>8.511577</td>\n",
       "      <td>14779.0</td>\n",
       "      <td>9.601030</td>\n",
       "      <td>21.381690</td>\n",
       "      <td>27.924758</td>\n",
       "      <td>36.213546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14779.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Cortex_1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cortex_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTGTTAGCAAATTCGA-1</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>42</td>\n",
       "      <td>4820</td>\n",
       "      <td>8.480737</td>\n",
       "      <td>14396.0</td>\n",
       "      <td>9.574775</td>\n",
       "      <td>20.595999</td>\n",
       "      <td>26.674076</td>\n",
       "      <td>34.655460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14396.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Cortex_3</td>\n",
       "      <td>1</td>\n",
       "      <td>Cortex_3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>324 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    in_tissue  array_row  array_col  n_genes_by_counts  \\\n",
       "AAATGGCATGTCTTGT-1          1         13         69               5191   \n",
       "AACAACTGGTAGTTGC-1          1         28         42               5252   \n",
       "AACAGGAAATCGAATA-1          1         15         67               6320   \n",
       "AACCCAGAGACGGAGA-1          1         15         39               3659   \n",
       "AACCGTTGTGTTTGCT-1          1         12         64               4512   \n",
       "...                       ...        ...        ...                ...   \n",
       "TTGGATTGGGTACCAC-1          1         17         55               4980   \n",
       "TTGGCTCGCATGAGAC-1          1         23         37               4620   \n",
       "TTGTATCACACAGAAT-1          1         12         74               6120   \n",
       "TTGTGGCCCTGACAGT-1          1         18         60               4971   \n",
       "TTGTTAGCAAATTCGA-1          1         22         42               4820   \n",
       "\n",
       "                    log1p_n_genes_by_counts  total_counts  log1p_total_counts  \\\n",
       "AAATGGCATGTCTTGT-1                 8.554874       15977.0            9.678968   \n",
       "AACAACTGGTAGTTGC-1                 8.566555       16649.0            9.720165   \n",
       "AACAGGAAATCGAATA-1                 8.751633       23375.0           10.059465   \n",
       "AACCCAGAGACGGAGA-1                 8.205218        9229.0            9.130215   \n",
       "AACCGTTGTGTTTGCT-1                 8.414717       12679.0            9.447782   \n",
       "...                                     ...           ...                 ...   \n",
       "TTGGATTGGGTACCAC-1                 8.513386       15381.0            9.640953   \n",
       "TTGGCTCGCATGAGAC-1                 8.438366       13193.0            9.487517   \n",
       "TTGTATCACACAGAAT-1                 8.719481       21951.0            9.996614   \n",
       "TTGTGGCCCTGACAGT-1                 8.511577       14779.0            9.601030   \n",
       "TTGTTAGCAAATTCGA-1                 8.480737       14396.0            9.574775   \n",
       "\n",
       "                    pct_counts_in_top_50_genes  pct_counts_in_top_100_genes  \\\n",
       "AAATGGCATGTCTTGT-1                   20.629655                    26.757213   \n",
       "AACAACTGGTAGTTGC-1                   20.481711                    26.277855   \n",
       "AACAGGAAATCGAATA-1                   17.929412                    23.850267   \n",
       "AACCCAGAGACGGAGA-1                   25.939972                    31.964460   \n",
       "AACCGTTGTGTTTGCT-1                   21.839262                    28.038489   \n",
       "...                                        ...                          ...   \n",
       "TTGGATTGGGTACCAC-1                   21.038944                    27.059359   \n",
       "TTGGCTCGCATGAGAC-1                   20.609414                    26.445842   \n",
       "TTGTATCACACAGAAT-1                   18.199626                    24.235798   \n",
       "TTGTGGCCCTGACAGT-1                   21.381690                    27.924758   \n",
       "TTGTTAGCAAATTCGA-1                   20.595999                    26.674076   \n",
       "\n",
       "                    pct_counts_in_top_200_genes  ...  total_counts_MT  \\\n",
       "AAATGGCATGTCTTGT-1                    34.743694  ...              0.0   \n",
       "AACAACTGGTAGTTGC-1                    34.092138  ...              0.0   \n",
       "AACAGGAAATCGAATA-1                    32.077005  ...              0.0   \n",
       "AACCCAGAGACGGAGA-1                    39.885145  ...              0.0   \n",
       "AACCGTTGTGTTTGCT-1                    36.209480  ...              0.0   \n",
       "...                                         ...  ...              ...   \n",
       "TTGGATTGGGTACCAC-1                    35.114752  ...              0.0   \n",
       "TTGGCTCGCATGAGAC-1                    34.063519  ...              0.0   \n",
       "TTGTATCACACAGAAT-1                    32.440436  ...              0.0   \n",
       "TTGTGGCCCTGACAGT-1                    36.213546  ...              0.0   \n",
       "TTGTTAGCAAATTCGA-1                    34.655460  ...              0.0   \n",
       "\n",
       "                    log1p_total_counts_MT  pct_counts_MT  n_counts  leiden  \\\n",
       "AAATGGCATGTCTTGT-1                    0.0            0.0   15977.0       0   \n",
       "AACAACTGGTAGTTGC-1                    0.0            0.0   16649.0       0   \n",
       "AACAGGAAATCGAATA-1                    0.0            0.0   23375.0       0   \n",
       "AACCCAGAGACGGAGA-1                    0.0            0.0    9229.0       1   \n",
       "AACCGTTGTGTTTGCT-1                    0.0            0.0   12679.0       0   \n",
       "...                                   ...            ...       ...     ...   \n",
       "TTGGATTGGGTACCAC-1                    0.0            0.0   15381.0       0   \n",
       "TTGGCTCGCATGAGAC-1                    0.0            0.0   13193.0       5   \n",
       "TTGTATCACACAGAAT-1                    0.0            0.0   21951.0       0   \n",
       "TTGTGGCCCTGACAGT-1                    0.0            0.0   14779.0       0   \n",
       "TTGTTAGCAAATTCGA-1                    0.0            0.0   14396.0       5   \n",
       "\n",
       "                     cluster batch  celltype str_batch batch_id  \n",
       "AAATGGCATGTCTTGT-1  Cortex_1     1  Cortex_1         1        0  \n",
       "AACAACTGGTAGTTGC-1  Cortex_1     1  Cortex_1         1        0  \n",
       "AACAGGAAATCGAATA-1  Cortex_1     1  Cortex_1         1        0  \n",
       "AACCCAGAGACGGAGA-1  Cortex_2     1  Cortex_2         1        0  \n",
       "AACCGTTGTGTTTGCT-1  Cortex_1     1  Cortex_1         1        0  \n",
       "...                      ...   ...       ...       ...      ...  \n",
       "TTGGATTGGGTACCAC-1  Cortex_1     1  Cortex_1         1        0  \n",
       "TTGGCTCGCATGAGAC-1  Cortex_3     1  Cortex_3         1        0  \n",
       "TTGTATCACACAGAAT-1  Cortex_1     1  Cortex_1         1        0  \n",
       "TTGTGGCCCTGACAGT-1  Cortex_1     1  Cortex_1         1        0  \n",
       "TTGTTAGCAAATTCGA-1  Cortex_3     1  Cortex_3         1        0  \n",
       "\n",
       "[324 rows x 21 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b27d3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Filtering genes by counts ...\n",
      "scGPT - INFO - Filtering cells by counts ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Log1p transforming ...\n",
      "scGPT - WARNING - The input data seems to be already log1p transformed. Set `log1p=False` to avoid double log1p transform.\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/gibbs/project/zhao/tl688/conda_envs/scgpt/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py:249: ImplicitModificationWarning: Trying to modify attribute `.var` of view, initializing view as actual.\n",
      "  adata.var['n_counts'] = number\n"
     ]
    }
   ],
   "source": [
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=3,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=config.n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "preprocessor(adata, batch_key=\"batch\" if dataset_name != \"heart_cell\" else None)\n",
    "\n",
    "# %%\n",
    "if per_seq_batch_sample:\n",
    "    # sort the adata by batch_id in advance\n",
    "    adata_sorted = adata[adata.obs[\"batch_id\"].argsort()].copy()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Tokenize input\n",
    "\n",
    "# %%\n",
    "input_layer_key = \"X_binned\"\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].A\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "celltypes_labels = adata.obs[\"celltype\"].tolist()  # make sure count from 0\n",
    "num_types = len(set(celltypes_labels))\n",
    "celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "num_batch_types = len(set(batch_ids))\n",
    "batch_ids = np.array(batch_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6460cdcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 324 × 11968\n",
       "    obs: 'in_tissue', 'array_row', 'array_col', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes', 'total_counts_MT', 'log1p_total_counts_MT', 'pct_counts_MT', 'n_counts', 'leiden', 'cluster', 'batch', 'celltype', 'str_batch', 'batch_id'\n",
       "    var: 'gene_ids', 'feature_types', 'genome', 'MT', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm', 'gene_name', 'id_in_vocab', 'n_counts'\n",
       "    uns: 'cluster_colors', 'hvg', 'leiden', 'leiden_colors', 'neighbors', 'pca', 'spatial', 'umap', 'log1p'\n",
       "    obsm: 'X_pca', 'X_umap', 'spatial', 'bin_edges'\n",
       "    varm: 'PCs'\n",
       "    layers: 'X_normed', 'X_log1p', 'X_binned'\n",
       "    obsp: 'connectivities', 'distances'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f67c332c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to save/dev_PBMC_10K-Jul25-18-17\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# settings for input and preprocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_ratio = config.mask_ratio\n",
    "mask_value = -1\n",
    "pad_value = -2\n",
    "n_input_bins = config.n_bins\n",
    "\n",
    "n_hvg = len(adata.var_names)  # number of highly variable genes\n",
    "max_seq_len = n_hvg + 1\n",
    "per_seq_batch_sample = False\n",
    "DSBN = True  # Domain-spec batchnorm\n",
    "explicit_zero_prob = True  # whether explicit bernoulli for zeros\n",
    "\n",
    "# %%\n",
    "dataset_name = config.dataset_name\n",
    "save_dir = Path(f\"./save/dev_{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"save to {save_dir}\")\n",
    "# save the whole script to the dir\n",
    "# os.system(f\"cp {__file__} {save_dir}\")\n",
    "\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf7697fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "input_layer_key = \"X_binned\"\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].A\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "celltypes_labels = adata.obs[\"celltype\"].tolist()  # make sure count from 0\n",
    "num_types = len(set(celltypes_labels))\n",
    "celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "num_batch_types = len(set(batch_ids))\n",
    "batch_ids = np.array(batch_ids)\n",
    "\n",
    "(\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_celltype_labels,\n",
    "    valid_celltype_labels,\n",
    "    train_batch_labels,\n",
    "    valid_batch_labels,\n",
    ") = train_test_split(\n",
    "    all_counts, celltypes_labels, batch_ids, test_size=0.001, shuffle=True\n",
    ")\n",
    "\n",
    "# %%\n",
    "if config.load_model is None:\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(vocab(genes), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27754159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - train set number of samples: 323, \n",
      "\t feature length: 11969\n",
      "scGPT - INFO - valid set number of samples: 1, \n",
      "\t feature length: 11969\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "if config.load_model is None:\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(vocab(genes), dtype=int)\n",
    "\n",
    "# %%\n",
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=True,\n",
    ")\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=True,\n",
    ")\n",
    "logger.info(\n",
    "    f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    ")\n",
    "\n",
    "\n",
    "# %%\n",
    "def prepare_data(sort_seq_batch=False) -> Tuple[Dict[str, torch.Tensor]]:\n",
    "    masked_values_train = random_mask_value(\n",
    "        tokenized_train[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    masked_values_valid = random_mask_value(\n",
    "        tokenized_valid[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    print(\n",
    "        f\"random masking at epoch {epoch:3d}, ratio of masked values in train: \",\n",
    "        f\"{(masked_values_train == mask_value).sum() / (masked_values_train - pad_value).count_nonzero():.4f}\",\n",
    "    )\n",
    "\n",
    "    input_gene_ids_train, input_gene_ids_valid = (\n",
    "        tokenized_train[\"genes\"],\n",
    "        tokenized_valid[\"genes\"],\n",
    "    )\n",
    "    input_values_train, input_values_valid = masked_values_train, masked_values_valid\n",
    "    target_values_train, target_values_valid = (\n",
    "        tokenized_train[\"values\"],\n",
    "        tokenized_valid[\"values\"],\n",
    "    )\n",
    "\n",
    "    tensor_batch_labels_train = torch.from_numpy(train_batch_labels).long()\n",
    "    tensor_batch_labels_valid = torch.from_numpy(valid_batch_labels).long()\n",
    "\n",
    "    if sort_seq_batch:\n",
    "        train_sort_ids = np.argsort(train_batch_labels)\n",
    "        input_gene_ids_train = input_gene_ids_train[train_sort_ids]\n",
    "        input_values_train = input_values_train[train_sort_ids]\n",
    "        target_values_train = target_values_train[train_sort_ids]\n",
    "        tensor_batch_labels_train = tensor_batch_labels_train[train_sort_ids]\n",
    "\n",
    "        valid_sort_ids = np.argsort(valid_batch_labels)\n",
    "        input_gene_ids_valid = input_gene_ids_valid[valid_sort_ids]\n",
    "        input_values_valid = input_values_valid[valid_sort_ids]\n",
    "        target_values_valid = target_values_valid[valid_sort_ids]\n",
    "        tensor_batch_labels_valid = tensor_batch_labels_valid[valid_sort_ids]\n",
    "\n",
    "    train_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_train,\n",
    "        \"values\": input_values_train,\n",
    "        \"target_values\": target_values_train,\n",
    "        \"batch_labels\": tensor_batch_labels_train,\n",
    "    }\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_valid,\n",
    "        \"values\": input_values_valid,\n",
    "        \"target_values\": target_values_valid,\n",
    "        \"batch_labels\": tensor_batch_labels_valid,\n",
    "    }\n",
    "\n",
    "    return train_data_pt, valid_data_pt\n",
    "\n",
    "\n",
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    intra_domain_shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    if per_seq_batch_sample:\n",
    "        # find the indices of samples in each seq batch\n",
    "        subsets = []\n",
    "        batch_labels_array = data_pt[\"batch_labels\"].numpy()\n",
    "        for batch_label in np.unique(batch_labels_array):\n",
    "            batch_indices = np.where(batch_labels_array == batch_label)[0].tolist()\n",
    "            subsets.append(batch_indices)\n",
    "        data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_sampler=SubsetsBatchSampler(\n",
    "                subsets,\n",
    "                batch_size,\n",
    "                intra_subset_shuffle=intra_domain_shuffle,\n",
    "                inter_subset_shuffle=shuffle,\n",
    "                drop_last=drop_last,\n",
    "            ),\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return data_loader\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23e3546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa1b766c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 324 × 11968\n",
       "    obs: 'in_tissue', 'array_row', 'array_col', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes', 'total_counts_MT', 'log1p_total_counts_MT', 'pct_counts_MT', 'n_counts', 'leiden', 'cluster', 'batch', 'celltype', 'str_batch', 'batch_id'\n",
       "    var: 'gene_ids', 'feature_types', 'genome', 'MT', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm', 'gene_name', 'id_in_vocab', 'n_counts'\n",
       "    uns: 'cluster_colors', 'hvg', 'leiden', 'leiden_colors', 'neighbors', 'pca', 'spatial', 'umap', 'log1p'\n",
       "    obsm: 'X_pca', 'X_umap', 'spatial', 'bin_edges'\n",
       "    varm: 'PCs'\n",
       "    layers: 'X_normed', 'X_log1p', 'X_binned'\n",
       "    obsp: 'connectivities', 'distances'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7fdef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdict = simulation_data(best_model,\n",
    "    adata_t=adata_sorted if per_seq_batch_sample else adata,\n",
    "    include_types=[\"cls\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86ba57dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([4.35546875, 4.16015625, 4.33984375, ..., 4.58984375, 4.4140625 ,\n",
       "        4.54296875]),\n",
       " array([4.3515625 , 4.16796875, 4.34765625, ..., 4.59375   , 4.4296875 ,\n",
       "        4.54296875]),\n",
       " array([4.328125  , 4.19140625, 4.33984375, ..., 4.70703125, 4.3359375 ,\n",
       "        4.52734375]),\n",
       " array([4.43359375, 4.19921875, 4.40234375, ..., 4.77734375, 4.37890625,\n",
       "        4.6484375 ]),\n",
       " array([4.38671875, 4.16015625, 4.359375  , ..., 4.62109375, 4.4609375 ,\n",
       "        4.578125  ]),\n",
       " array([4.375     , 4.1640625 , 4.4921875 , ..., 4.60546875, 4.32421875,\n",
       "        4.5546875 ]),\n",
       " array([4.34765625, 4.1640625 , 4.33984375, ..., 4.59375   , 4.3046875 ,\n",
       "        4.54296875]),\n",
       " array([4.359375  , 4.16015625, 4.34765625, ..., 4.70703125, 4.3203125 ,\n",
       "        4.546875  ]),\n",
       " array([4.37890625, 4.15234375, 4.3515625 , ..., 4.6171875 , 4.3203125 ,\n",
       "        4.56640625]),\n",
       " array([4.3828125 , 4.15625   , 4.3515625 , ..., 4.62890625, 4.32421875,\n",
       "        4.57421875]),\n",
       " array([4.34375   , 4.1640625 , 4.33984375, ..., 4.58203125, 4.30859375,\n",
       "        4.5390625 ]),\n",
       " array([4.33203125, 4.234375  , 4.3828125 , ..., 4.56640625, 4.31640625,\n",
       "        4.96484375]),\n",
       " array([4.34375   , 4.16015625, 4.33203125, ..., 4.671875  , 4.30859375,\n",
       "        4.53515625]),\n",
       " array([4.36328125, 4.16796875, 4.3515625 , ..., 4.59765625, 4.31640625,\n",
       "        4.55859375]),\n",
       " array([4.33984375, 4.17578125, 4.33984375, ..., 4.578125  , 4.32421875,\n",
       "        4.5390625 ]),\n",
       " array([4.44921875, 4.2109375 , 4.4140625 , ..., 4.703125  , 4.39453125,\n",
       "        4.65625   ]),\n",
       " array([4.33984375, 4.1796875 , 4.34375   , ..., 4.578125  , 4.31640625,\n",
       "        4.5390625 ]),\n",
       " array([4.328125  , 4.171875  , 4.33203125, ..., 4.640625  , 4.32421875,\n",
       "        4.52734375]),\n",
       " array([4.359375  , 4.1640625 , 4.34765625, ..., 4.59375   , 4.3125    ,\n",
       "        4.546875  ]),\n",
       " array([4.3515625 , 4.1640625 , 4.33984375, ..., 4.578125  , 4.30859375,\n",
       "        4.5390625 ]),\n",
       " array([4.34375   , 4.15234375, 4.33984375, ..., 4.58984375, 4.3046875 ,\n",
       "        4.53515625]),\n",
       " array([4.359375  , 4.171875  , 4.34765625, ..., 4.59765625, 4.31640625,\n",
       "        4.546875  ]),\n",
       " array([4.3671875 , 4.16015625, 4.3515625 , ..., 4.6015625 , 4.3203125 ,\n",
       "        4.5546875 ]),\n",
       " array([4.359375  , 4.1640625 , 4.3515625 , ..., 4.6875    , 4.390625  ,\n",
       "        4.55859375]),\n",
       " array([4.34765625, 4.171875  , 4.34375   , ..., 4.58203125, 4.31640625,\n",
       "        4.5390625 ]),\n",
       " array([4.3359375 , 4.1796875 , 4.3359375 , ..., 4.57421875, 4.328125  ,\n",
       "        4.5390625 ]),\n",
       " array([4.390625  , 4.1640625 , 4.3671875 , ..., 4.63671875, 4.45703125,\n",
       "        4.58984375]),\n",
       " array([4.33984375, 4.16796875, 4.328125  , ..., 4.57421875, 4.36328125,\n",
       "        4.53125   ]),\n",
       " array([4.35546875, 4.2734375 , 4.34375   , ..., 4.59375   , 4.30859375,\n",
       "        4.546875  ]),\n",
       " array([4.3515625 , 4.16015625, 4.33984375, ..., 4.6875    , 4.3125    ,\n",
       "        4.5390625 ]),\n",
       " array([4.34375   , 4.16796875, 4.3359375 , ..., 4.5703125 , 4.31640625,\n",
       "        4.53515625]),\n",
       " array([4.40625   , 4.16796875, 4.37890625, ..., 4.6484375 , 4.375     ,\n",
       "        4.6015625 ]),\n",
       " array([4.34375   , 4.16015625, 4.33203125, ..., 4.58203125, 4.30078125,\n",
       "        4.53125   ]),\n",
       " array([4.33984375, 4.2421875 , 4.3359375 , ..., 4.578125  , 4.3203125 ,\n",
       "        4.53125   ]),\n",
       " array([4.32421875, 4.1875    , 4.3359375 , ..., 4.5546875 , 4.3359375 ,\n",
       "        4.52734375]),\n",
       " array([4.33203125, 4.1875    , 4.33984375, ..., 4.56640625, 4.3359375 ,\n",
       "        4.5390625 ]),\n",
       " array([4.359375  , 4.21484375, 4.3515625 , ..., 4.60546875, 4.3125    ,\n",
       "        4.55078125]),\n",
       " array([4.34765625, 4.16796875, 4.33984375, ..., 4.58203125, 4.3125    ,\n",
       "        4.5390625 ]),\n",
       " array([4.34765625, 4.17578125, 4.34765625, ..., 4.58984375, 4.32421875,\n",
       "        4.546875  ]),\n",
       " array([4.3515625 , 4.16796875, 4.34765625, ..., 4.59375   , 4.31640625,\n",
       "        4.54296875]),\n",
       " array([4.33984375, 4.17578125, 4.25      , ..., 4.69140625, 4.3203125 ,\n",
       "        4.5390625 ]),\n",
       " array([4.43359375, 4.1953125 , 4.39453125, ..., 4.6796875 , 4.37890625,\n",
       "        4.63671875]),\n",
       " array([4.33984375, 4.16015625, 4.33203125, ..., 4.5703125 , 4.3046875 ,\n",
       "        4.52734375]),\n",
       " array([4.36328125, 4.16015625, 4.35546875, ..., 4.60546875, 4.45703125,\n",
       "        4.5546875 ]),\n",
       " array([4.34765625, 4.16796875, 4.34765625, ..., 4.69140625, 4.3203125 ,\n",
       "        4.5390625 ]),\n",
       " array([4.484375  , 4.2421875 , 4.4453125 , ..., 4.73046875, 4.44140625,\n",
       "        4.69140625]),\n",
       " array([4.33984375, 4.17578125, 4.3359375 , ..., 4.57421875, 4.328125  ,\n",
       "        4.5390625 ]),\n",
       " array([4.3515625 , 4.16796875, 4.34375   , ..., 4.5859375 , 4.31640625,\n",
       "        4.5390625 ]),\n",
       " array([4.375     , 4.16796875, 4.47265625, ..., 4.6171875 , 4.32421875,\n",
       "        4.5625    ]),\n",
       " array([4.36328125, 4.16015625, 4.3515625 , ..., 4.73828125, 4.453125  ,\n",
       "        4.5546875 ]),\n",
       " array([4.35546875, 4.171875  , 4.34375   , ..., 4.58984375, 4.3125    ,\n",
       "        4.546875  ]),\n",
       " array([4.38671875, 4.1640625 , 4.3671875 , ..., 4.7109375 , 4.39453125,\n",
       "        4.58984375]),\n",
       " array([4.35546875, 4.18359375, 4.3515625 , ..., 4.58984375, 4.328125  ,\n",
       "        4.546875  ]),\n",
       " array([4.3359375 , 4.1796875 , 4.23828125, ..., 4.6796875 , 4.32421875,\n",
       "        4.53515625]),\n",
       " array([4.35546875, 4.16796875, 4.34375   , ..., 4.66796875, 4.31640625,\n",
       "        4.54296875]),\n",
       " array([4.36328125, 4.15234375, 4.34765625, ..., 4.6015625 , 4.30859375,\n",
       "        4.55078125]),\n",
       " array([4.359375  , 4.1640625 , 4.34765625, ..., 4.6875    , 4.3125    ,\n",
       "        4.5546875 ]),\n",
       " array([4.359375  , 4.171875  , 4.34765625, ..., 4.60546875, 4.3203125 ,\n",
       "        4.5546875 ]),\n",
       " array([4.34375   , 4.17578125, 4.33984375, ..., 4.58203125, 4.3203125 ,\n",
       "        4.53515625]),\n",
       " array([4.34765625, 4.16796875, 4.34375   , ..., 4.58984375, 4.30859375,\n",
       "        4.546875  ]),\n",
       " array([4.34765625, 4.171875  , 4.34375   , ..., 4.58203125, 4.3125    ,\n",
       "        4.54296875]),\n",
       " array([4.35546875, 4.16796875, 4.265625  , ..., 4.6640625 , 4.31640625,\n",
       "        4.546875  ]),\n",
       " array([4.3515625 , 4.1640625 , 4.34765625, ..., 4.6953125 , 4.3046875 ,\n",
       "        4.55078125]),\n",
       " array([4.36328125, 4.15625   , 4.34375   , ..., 4.59375   , 4.30859375,\n",
       "        4.546875  ]),\n",
       " array([4.359375  , 4.16015625, 4.34375   , ..., 4.59765625, 4.30859375,\n",
       "        4.55078125]),\n",
       " array([4.3828125 , 4.15234375, 4.35546875, ..., 4.625     , 4.3203125 ,\n",
       "        4.5703125 ]),\n",
       " array([4.3515625 , 4.17578125, 4.34765625, ..., 4.671875  , 4.3203125 ,\n",
       "        4.54296875]),\n",
       " array([4.359375  , 4.171875  , 4.3515625 , ..., 4.59765625, 4.3125    ,\n",
       "        4.55078125]),\n",
       " array([4.3359375 , 4.17578125, 4.33203125, ..., 4.66796875, 4.32421875,\n",
       "        4.53515625]),\n",
       " array([4.375     , 4.16015625, 4.35546875, ..., 4.69140625, 4.3203125 ,\n",
       "        4.5625    ]),\n",
       " array([4.375     , 4.15234375, 4.34765625, ..., 4.69921875, 4.30859375,\n",
       "        4.5625    ]),\n",
       " array([4.3515625 , 4.1640625 , 4.34375   , ..., 4.69140625, 4.3046875 ,\n",
       "        4.5390625 ]),\n",
       " array([4.33203125, 4.1796875 , 4.3359375 , ..., 4.5625    , 4.3359375 ,\n",
       "        4.53125   ]),\n",
       " array([4.35546875, 4.1796875 , 4.35546875, ..., 4.70703125, 4.3671875 ,\n",
       "        4.5546875 ]),\n",
       " array([4.35546875, 4.1640625 , 4.34375   , ..., 4.6015625 , 4.30859375,\n",
       "        4.5546875 ]),\n",
       " array([4.3515625 , 4.16015625, 4.33984375, ..., 4.5859375 , 4.3671875 ,\n",
       "        4.546875  ]),\n",
       " array([4.359375  , 4.16015625, 4.34375   , ..., 4.59375   , 4.453125  ,\n",
       "        4.54296875]),\n",
       " array([4.3515625 , 4.171875  , 4.34375   , ..., 4.58203125, 4.32421875,\n",
       "        4.546875  ]),\n",
       " array([4.34765625, 4.171875  , 4.3046875 , ..., 4.58984375, 4.3203125 ,\n",
       "        4.55078125]),\n",
       " array([4.35546875, 4.16796875, 4.3359375 , ..., 4.58203125, 4.3203125 ,\n",
       "        4.5390625 ]),\n",
       " array([4.3671875 , 4.15625   , 4.34765625, ..., 4.61328125, 4.3125    ,\n",
       "        4.55859375]),\n",
       " array([4.34375  , 4.1796875, 4.4921875, ..., 4.703125 , 4.328125 ,\n",
       "        4.546875 ]),\n",
       " array([4.4375    , 4.19140625, 4.3984375 , ..., 4.69921875, 4.37890625,\n",
       "        4.64453125]),\n",
       " array([4.42578125, 4.19140625, 4.3359375 , ..., 4.671875  , 4.375     ,\n",
       "        4.6328125 ]),\n",
       " array([4.40625   , 4.16796875, 4.375     , ..., 4.65234375, 4.34765625,\n",
       "        4.75      ]),\n",
       " array([4.37109375, 4.16015625, 4.3515625 , ..., 4.61328125, 4.30859375,\n",
       "        4.55859375]),\n",
       " array([4.375     , 4.16015625, 4.3515625 , ..., 4.609375  , 4.3203125 ,\n",
       "        4.5625    ]),\n",
       " array([4.33984375, 4.171875  , 4.33984375, ..., 4.671875  , 4.3203125 ,\n",
       "        4.53125   ]),\n",
       " array([4.37109375, 4.16796875, 4.359375  , ..., 4.69140625, 4.328125  ,\n",
       "        4.5703125 ]),\n",
       " array([4.36328125, 4.1640625 , 4.26171875, ..., 4.59765625, 4.3203125 ,\n",
       "        4.5546875 ]),\n",
       " array([4.34375   , 4.18359375, 4.34375   , ..., 4.578125  , 4.3359375 ,\n",
       "        4.54296875]),\n",
       " array([4.35546875, 4.1640625 , 4.34375   , ..., 4.67578125, 4.30859375,\n",
       "        4.546875  ]),\n",
       " array([4.3671875 , 4.28515625, 4.3515625 , ..., 4.60546875, 4.3203125 ,\n",
       "        4.5625    ]),\n",
       " array([4.33984375, 4.17578125, 4.33984375, ..., 4.578125  , 4.40234375,\n",
       "        4.53125   ]),\n",
       " array([4.34375   , 4.1484375 , 4.3359375 , ..., 4.69140625, 4.30859375,\n",
       "        4.54296875]),\n",
       " array([4.40625   , 4.16796875, 4.37109375, ..., 4.6484375 , 4.34375   ,\n",
       "        4.61328125]),\n",
       " array([4.42578125, 4.19140625, 4.390625  , ..., 4.67578125, 4.37109375,\n",
       "        4.640625  ]),\n",
       " array([4.35546875, 4.1640625 , 4.33984375, ..., 4.59375   , 4.30859375,\n",
       "        4.5390625 ]),\n",
       " array([4.3359375 , 4.18359375, 4.34375   , ..., 4.57421875, 4.3359375 ,\n",
       "        4.5390625 ]),\n",
       " array([4.3671875 , 4.16796875, 4.38671875, ..., 4.71484375, 4.31640625,\n",
       "        4.55859375]),\n",
       " array([4.35546875, 4.16796875, 4.3515625 , ..., 4.6796875 , 4.31640625,\n",
       "        4.5546875 ]),\n",
       " array([4.3515625 , 4.16015625, 4.33984375, ..., 4.58984375, 4.30859375,\n",
       "        4.54296875]),\n",
       " array([4.36328125, 4.15234375, 4.55078125, ..., 4.6015625 , 4.3046875 ,\n",
       "        4.546875  ]),\n",
       " array([4.37890625, 4.15625   , 4.35546875, ..., 4.68359375, 4.40234375,\n",
       "        4.5703125 ]),\n",
       " array([4.34765625, 4.17578125, 4.33984375, ..., 4.58984375, 4.32421875,\n",
       "        4.546875  ]),\n",
       " array([4.34765625, 4.1640625 , 4.33984375, ..., 4.5859375 , 4.3046875 ,\n",
       "        4.5390625 ]),\n",
       " array([4.40625   , 4.24609375, 4.37109375, ..., 4.71484375, 4.34765625,\n",
       "        4.609375  ]),\n",
       " array([4.3671875 , 4.15234375, 4.34375   , ..., 4.609375  , 4.30078125,\n",
       "        4.5546875 ]),\n",
       " array([4.35546875, 4.1640625 , 4.34765625, ..., 4.59375   , 4.30859375,\n",
       "        4.546875  ]),\n",
       " array([4.33203125, 4.171875  , 4.33203125, ..., 4.5625    , 4.31640625,\n",
       "        4.5234375 ]),\n",
       " array([4.4296875, 4.1875   , 4.3984375, ..., 4.6796875, 4.375    ,\n",
       "        4.640625 ]),\n",
       " array([4.37890625, 4.1640625 , 4.3515625 , ..., 4.609375  , 4.32421875,\n",
       "        4.5703125 ]),\n",
       " array([4.34765625, 4.1640625 , 4.33984375, ..., 4.578125  , 4.31640625,\n",
       "        4.5390625 ]),\n",
       " array([4.43359375, 4.1953125 , 4.3984375 , ..., 4.69921875, 4.3828125 ,\n",
       "        4.6484375 ]),\n",
       " array([4.34375   , 4.16796875, 4.34375   , ..., 4.68359375, 4.37890625,\n",
       "        4.5390625 ]),\n",
       " array([4.33203125, 4.1796875 , 4.3359375 , ..., 4.5703125 , 4.33984375,\n",
       "        4.53125   ]),\n",
       " array([4.37890625, 4.15625   , 4.3515625 , ..., 4.61328125, 4.31640625,\n",
       "        4.5625    ]),\n",
       " array([4.34375   , 4.1796875 , 4.34375   , ..., 4.57421875, 4.328125  ,\n",
       "        4.53515625]),\n",
       " array([4.3515625 , 4.1796875 , 4.34765625, ..., 4.5859375 , 4.32421875,\n",
       "        4.97265625]),\n",
       " array([4.3359375 , 4.1796875 , 4.33203125, ..., 4.5625    , 4.3203125 ,\n",
       "        4.53125   ]),\n",
       " array([4.3515625 , 4.16015625, 4.34375   , ..., 4.5859375 , 4.30078125,\n",
       "        5.015625  ]),\n",
       " array([4.33984375, 4.18359375, 4.33984375, ..., 4.5703125 , 4.328125  ,\n",
       "        4.53515625]),\n",
       " array([4.46484375, 4.22265625, 4.42578125, ..., 4.71484375, 4.4140625 ,\n",
       "        4.67578125]),\n",
       " array([4.3359375 , 4.17578125, 4.3359375 , ..., 4.6640625 , 4.32421875,\n",
       "        4.53125   ]),\n",
       " array([4.390625  , 4.17578125, 4.37109375, ..., 4.625     , 4.3359375 ,\n",
       "        4.5859375 ]),\n",
       " array([4.34375   , 4.15625   , 4.33203125, ..., 4.578125  , 4.296875  ,\n",
       "        4.5390625 ]),\n",
       " array([4.328125  , 4.1796875 , 4.3359375 , ..., 4.56640625, 4.33203125,\n",
       "        4.53125   ]),\n",
       " array([4.36328125, 4.15234375, 4.34375   , ..., 4.6015625 , 4.30859375,\n",
       "        4.546875  ]),\n",
       " array([4.359375  , 4.16796875, 4.34375   , ..., 4.58984375, 4.3203125 ,\n",
       "        4.54296875]),\n",
       " array([4.3515625 , 4.17578125, 4.3515625 , ..., 4.59375   , 4.37109375,\n",
       "        4.6953125 ]),\n",
       " array([4.375     , 4.16796875, 4.359375  , ..., 4.6171875 , 4.33203125,\n",
       "        4.57421875]),\n",
       " array([4.34375   , 4.1640625 , 4.3359375 , ..., 4.671875  , 4.31640625,\n",
       "        4.53515625]),\n",
       " array([4.3515625 , 4.1640625 , 4.34375   , ..., 4.58984375, 4.3046875 ,\n",
       "        4.546875  ]),\n",
       " array([4.34375   , 4.171875  , 4.33984375, ..., 4.58203125, 4.3203125 ,\n",
       "        4.5390625 ]),\n",
       " array([4.44140625, 4.203125  , 4.38671875, ..., 4.69921875, 4.48046875,\n",
       "        4.65234375]),\n",
       " array([4.34375   , 4.16796875, 4.26171875, ..., 4.57421875, 4.3125    ,\n",
       "        4.53515625]),\n",
       " array([4.36328125, 4.15234375, 4.2890625 , ..., 4.61328125, 4.30859375,\n",
       "        4.5546875 ]),\n",
       " array([4.33203125, 4.18359375, 4.3359375 , ..., 4.6875    , 4.453125  ,\n",
       "        4.53515625]),\n",
       " array([4.34375   , 4.171875  , 4.51171875, ..., 4.58203125, 4.3203125 ,\n",
       "        4.5390625 ]),\n",
       " array([4.35546875, 4.15625   , 4.33984375, ..., 4.59765625, 4.3046875 ,\n",
       "        4.546875  ]),\n",
       " array([4.36328125, 4.16015625, 4.34375   , ..., 4.6015625 , 4.30859375,\n",
       "        4.55078125]),\n",
       " array([4.3515625 , 4.17578125, 4.35546875, ..., 4.6953125 , 4.32421875,\n",
       "        4.5546875 ]),\n",
       " array([4.33984375, 4.171875  , 4.33984375, ..., 4.5859375 , 4.3203125 ,\n",
       "        4.54296875]),\n",
       " array([4.3671875 , 4.16796875, 4.3515625 , ..., 4.60546875, 4.32421875,\n",
       "        4.55859375]),\n",
       " array([4.3515625 , 4.16015625, 4.34375   , ..., 4.59375   , 4.3046875 ,\n",
       "        4.5390625 ]),\n",
       " array([4.375     , 4.15625   , 4.3515625 , ..., 4.609375  , 4.3125    ,\n",
       "        4.55859375]),\n",
       " array([4.4375    , 4.1953125 , 4.3984375 , ..., 4.71484375, 4.3828125 ,\n",
       "        4.640625  ]),\n",
       " array([4.37109375, 4.1640625 , 4.3515625 , ..., 4.60546875, 4.3203125 ,\n",
       "        4.56640625]),\n",
       " array([4.46484375, 4.22265625, 4.421875  , ..., 4.71484375, 4.41796875,\n",
       "        4.671875  ]),\n",
       " array([4.43359375, 4.1953125 , 4.40625   , ..., 4.68359375, 4.3828125 ,\n",
       "        4.64453125]),\n",
       " array([4.40625   , 4.1640625 , 4.375     , ..., 4.703125  , 4.34375   ,\n",
       "        4.60546875]),\n",
       " array([4.3671875 , 4.1640625 , 4.28125   , ..., 4.6015625 , 4.40234375,\n",
       "        4.5546875 ]),\n",
       " array([4.36328125, 4.15625   , 4.34765625, ..., 4.60546875, 4.30859375,\n",
       "        4.5546875 ]),\n",
       " array([4.36328125, 4.1484375 , 4.34375   , ..., 4.59765625, 4.30859375,\n",
       "        4.546875  ]),\n",
       " array([4.37890625, 4.15625   , 4.35546875, ..., 4.6875    , 4.31640625,\n",
       "        4.56640625]),\n",
       " array([4.3671875 , 4.15625   , 4.3515625 , ..., 4.609375  , 4.3125    ,\n",
       "        4.55859375]),\n",
       " array([4.359375  , 4.1640625 , 4.34765625, ..., 4.69140625, 4.31640625,\n",
       "        4.55078125]),\n",
       " array([4.34765625, 4.16015625, 4.33984375, ..., 4.67578125, 4.30859375,\n",
       "        4.53125   ]),\n",
       " array([4.328125  , 4.171875  , 4.27734375, ..., 4.69921875, 4.3203125 ,\n",
       "        4.90625   ]),\n",
       " array([4.40625   , 4.1640625 , 4.37109375, ..., 4.64453125, 4.33984375,\n",
       "        4.59765625]),\n",
       " array([4.3359375 , 4.18359375, 4.52734375, ..., 4.5703125 , 4.32421875,\n",
       "        4.53515625]),\n",
       " array([4.44140625, 4.19921875, 4.40625   , ..., 4.69140625, 4.390625  ,\n",
       "        4.65234375]),\n",
       " array([4.3671875 , 4.15625   , 4.34765625, ..., 4.609375  , 4.3125    ,\n",
       "        4.55859375]),\n",
       " array([4.359375  , 4.1640625 , 4.3515625 , ..., 4.60546875, 4.3125    ,\n",
       "        4.55078125]),\n",
       " array([4.3359375 , 4.17578125, 4.34375   , ..., 4.578125  , 4.3203125 ,\n",
       "        4.53515625]),\n",
       " array([4.34375   , 4.16796875, 4.3359375 , ..., 4.58203125, 4.31640625,\n",
       "        4.5390625 ]),\n",
       " array([4.35546875, 4.16015625, 4.34375   , ..., 4.59375   , 4.30859375,\n",
       "        4.546875  ]),\n",
       " array([4.34375   , 4.16796875, 4.34375   , ..., 4.58984375, 4.3203125 ,\n",
       "        4.5390625 ]),\n",
       " array([4.3828125 , 4.15234375, 4.3515625 , ..., 4.625     , 4.31640625,\n",
       "        4.5703125 ]),\n",
       " array([4.3359375 , 4.17578125, 4.3359375 , ..., 4.57421875, 4.32421875,\n",
       "        4.5390625 ]),\n",
       " array([4.32421875, 4.18359375, 4.33203125, ..., 4.55859375, 4.3828125 ,\n",
       "        4.5234375 ]),\n",
       " array([4.33984375, 4.171875  , 4.33203125, ..., 4.57421875, 4.421875  ,\n",
       "        4.53515625]),\n",
       " array([4.34765625, 4.1875    , 4.34375   , ..., 4.58984375, 4.3046875 ,\n",
       "        4.5390625 ]),\n",
       " array([4.34765625, 4.16796875, 4.421875  , ..., 4.7109375 , 4.3125    ,\n",
       "        4.55078125]),\n",
       " array([4.359375  , 4.16015625, 4.3515625 , ..., 4.60546875, 4.30859375,\n",
       "        4.5546875 ]),\n",
       " array([4.38671875, 4.15625   , 4.35546875, ..., 4.62890625, 4.328125  ,\n",
       "        4.578125  ]),\n",
       " array([4.35546875, 4.15625   , 4.43359375, ..., 4.5859375 , 4.3046875 ,\n",
       "        4.54296875]),\n",
       " array([4.34375   , 4.15625   , 4.33203125, ..., 4.58203125, 4.30078125,\n",
       "        4.53125   ]),\n",
       " array([4.37109375, 4.1484375 , 4.34375   , ..., 4.609375  , 4.3125    ,\n",
       "        4.5546875 ]),\n",
       " array([4.33984375, 4.17578125, 4.34375   , ..., 4.578125  , 4.3203125 ,\n",
       "        4.53125   ]),\n",
       " array([4.3359375 , 4.19140625, 4.34375   , ..., 4.5703125 , 4.3515625 ,\n",
       "        4.546875  ]),\n",
       " array([4.33984375, 4.16015625, 4.33203125, ..., 4.58203125, 4.3046875 ,\n",
       "        4.53125   ]),\n",
       " array([4.47265625, 4.234375  , 4.4375    , ..., 4.7265625 , 4.43359375,\n",
       "        4.68359375]),\n",
       " array([4.328125  , 4.1875    , 4.2578125 , ..., 4.65234375, 4.33984375,\n",
       "        4.53125   ]),\n",
       " array([4.33984375, 4.16796875, 4.33203125, ..., 4.67578125, 4.31640625,\n",
       "        4.53515625]),\n",
       " array([4.359375  , 4.1640625 , 4.34765625, ..., 4.59765625, 4.3125    ,\n",
       "        4.546875  ]),\n",
       " array([4.37109375, 4.15625   , 4.35546875, ..., 4.61328125, 4.3203125 ,\n",
       "        4.55859375]),\n",
       " array([4.34765625, 4.1796875 , 4.26171875, ..., 4.5859375 , 4.33203125,\n",
       "        4.546875  ]),\n",
       " array([4.37890625, 4.16015625, 4.34765625, ..., 4.6171875 , 4.3203125 ,\n",
       "        4.5703125 ]),\n",
       " array([4.34375   , 4.17578125, 4.34375   , ..., 4.68359375, 4.32421875,\n",
       "        4.54296875]),\n",
       " array([4.3515625, 4.1796875, 4.3515625, ..., 4.5859375, 4.328125 ,\n",
       "        4.546875 ]),\n",
       " array([4.390625  , 4.1640625 , 4.359375  , ..., 4.625     , 4.328125  ,\n",
       "        4.58203125]),\n",
       " array([4.3515625 , 4.1640625 , 4.34375   , ..., 4.59765625, 4.3125    ,\n",
       "        4.55078125]),\n",
       " array([4.34765625, 4.15625   , 4.33203125, ..., 4.5859375 , 4.3046875 ,\n",
       "        4.53125   ]),\n",
       " array([4.3984375 , 4.16015625, 4.3671875 , ..., 4.6953125 , 4.33203125,\n",
       "        4.59765625]),\n",
       " array([4.35546875, 4.1640625 , 4.34765625, ..., 4.59375   , 4.3828125 ,\n",
       "        4.55078125]),\n",
       " array([4.33984375, 4.17578125, 4.34375   , ..., 4.57421875, 4.3203125 ,\n",
       "        4.5390625 ]),\n",
       " array([4.46875   , 4.2265625 , 4.43359375, ..., 4.7265625 , 4.421875  ,\n",
       "        4.6796875 ]),\n",
       " array([4.34375   , 4.26953125, 4.3359375 , ..., 4.58203125, 4.3125    ,\n",
       "        4.53515625]),\n",
       " array([4.3515625 , 4.15625   , 4.33984375, ..., 4.66015625, 4.3046875 ,\n",
       "        4.54296875]),\n",
       " array([4.3359375 , 4.17578125, 4.3359375 , ..., 4.6640625 , 4.32421875,\n",
       "        4.53515625]),\n",
       " array([4.35546875, 4.171875  , 4.34375   , ..., 4.58984375, 4.3125    ,\n",
       "        4.546875  ]),\n",
       " array([4.4765625 , 4.234375  , 4.44140625, ..., 4.72265625, 4.43359375,\n",
       "        4.69140625]),\n",
       " array([4.38671875, 4.1484375 , 4.3515625 , ..., 4.6328125 , 4.31640625,\n",
       "        4.578125  ]),\n",
       " array([4.3515625 , 4.15625   , 4.3828125 , ..., 4.5859375 , 4.30078125,\n",
       "        4.54296875]),\n",
       " array([4.3359375 , 4.16796875, 4.328125  , ..., 4.5703125 , 4.31640625,\n",
       "        4.53515625]),\n",
       " array([4.3671875 , 4.16796875, 4.35546875, ..., 4.6953125 , 4.3125    ,\n",
       "        4.55859375]),\n",
       " array([4.34375   , 4.1640625 , 4.33984375, ..., 4.58203125, 4.421875  ,\n",
       "        4.53515625]),\n",
       " array([4.3515625 , 4.15625   , 4.33984375, ..., 4.59375   , 4.3046875 ,\n",
       "        4.54296875]),\n",
       " array([4.35546875, 4.16796875, 4.4453125 , ..., 4.58984375, 4.31640625,\n",
       "        4.546875  ]),\n",
       " array([4.32421875, 4.18359375, 4.3359375 , ..., 4.6640625 , 4.33203125,\n",
       "        4.53125   ]),\n",
       " array([4.49609375, 4.25390625, 4.453125  , ..., 4.74609375, 4.45703125,\n",
       "        4.70703125]),\n",
       " array([4.41015625, 4.16796875, 4.38671875, ..., 4.6640625 , 4.34375   ,\n",
       "        4.625     ]),\n",
       " array([4.3671875 , 4.15234375, 4.3515625 , ..., 4.609375  , 4.30859375,\n",
       "        4.5546875 ]),\n",
       " array([4.3515625 , 4.1640625 , 4.34375   , ..., 4.5859375 , 4.31640625,\n",
       "        4.546875  ]),\n",
       " array([4.47265625, 4.234375  , 4.4375    , ..., 4.72265625, 4.4296875 ,\n",
       "        4.6953125 ]),\n",
       " array([4.34765625, 4.16796875, 4.33984375, ..., 4.578125  , 4.3125    ,\n",
       "        4.5390625 ]),\n",
       " array([4.4140625 , 4.17578125, 4.3828125 , ..., 4.65625   , 4.35546875,\n",
       "        4.6171875 ]),\n",
       " array([4.33984375, 4.1796875 , 4.3359375 , ..., 4.6875    , 4.32421875,\n",
       "        4.66015625]),\n",
       " array([4.48046875, 4.23828125, 4.44921875, ..., 4.74609375, 4.4375    ,\n",
       "        4.6953125 ]),\n",
       " array([4.36328125, 4.16796875, 4.3515625 , ..., 4.74609375, 4.3203125 ,\n",
       "        4.55859375]),\n",
       " array([4.36328125, 4.16015625, 4.3515625 , ..., 4.69921875, 4.3515625 ,\n",
       "        4.5546875 ]),\n",
       " array([4.36328125, 4.15234375, 4.33203125, ..., 4.59765625, 4.3046875 ,\n",
       "        4.546875  ]),\n",
       " array([4.3671875 , 4.15234375, 4.34765625, ..., 4.59765625, 4.3046875 ,\n",
       "        4.55078125]),\n",
       " array([4.34375   , 4.1796875 , 4.34765625, ..., 4.578125  , 4.33203125,\n",
       "        4.546875  ]),\n",
       " array([4.34375   , 4.171875  , 4.33984375, ..., 4.57421875, 4.3203125 ,\n",
       "        4.5390625 ]),\n",
       " array([4.35546875, 4.16015625, 4.33984375, ..., 4.58984375, 4.3046875 ,\n",
       "        4.55078125]),\n",
       " array([4.359375  , 4.17578125, 4.34765625, ..., 4.59375   , 4.3203125 ,\n",
       "        4.5546875 ]),\n",
       " array([4.359375  , 4.15625   , 4.34375   , ..., 4.6015625 , 4.30078125,\n",
       "        4.546875  ]),\n",
       " array([4.3515625 , 4.16796875, 4.34765625, ..., 4.59765625, 4.30859375,\n",
       "        4.55078125]),\n",
       " array([4.34765625, 4.23828125, 4.34375   , ..., 4.58984375, 4.3125    ,\n",
       "        4.54296875]),\n",
       " array([4.375     , 4.15625   , 4.3203125 , ..., 4.609375  , 4.32421875,\n",
       "        4.56640625]),\n",
       " array([4.35546875, 4.1640625 , 4.34765625, ..., 4.6953125 , 4.3125    ,\n",
       "        4.55078125]),\n",
       " array([4.43359375, 4.1875    , 4.3984375 , ..., 4.68359375, 4.37109375,\n",
       "        4.63671875]),\n",
       " array([4.359375  , 4.15625   , 4.34375   , ..., 4.68359375, 4.30859375,\n",
       "        4.55078125]),\n",
       " array([4.36328125, 4.16796875, 4.265625  , ..., 4.60546875, 4.3125    ,\n",
       "        4.5546875 ]),\n",
       " array([4.3515625 , 4.16796875, 4.33984375, ..., 4.5859375 , 4.30859375,\n",
       "        4.5390625 ]),\n",
       " array([4.375     , 4.16015625, 4.3515625 , ..., 4.6171875 , 4.31640625,\n",
       "        4.5625    ]),\n",
       " array([4.34765625, 4.17578125, 4.34765625, ..., 4.6953125 , 4.3984375 ,\n",
       "        4.5390625 ]),\n",
       " array([4.34375   , 4.171875  , 4.33984375, ..., 4.58203125, 4.3125    ,\n",
       "        4.5390625 ]),\n",
       " array([4.36328125, 4.15625   , 4.3515625 , ..., 4.61328125, 4.30859375,\n",
       "        4.55859375]),\n",
       " array([4.35546875, 4.1640625 , 4.3515625 , ..., 4.6015625 , 4.30859375,\n",
       "        4.91796875]),\n",
       " array([4.5       , 4.25390625, 4.453125  , ..., 4.74609375, 4.453125  ,\n",
       "        4.69921875]),\n",
       " array([4.37890625, 4.15234375, 4.34765625, ..., 4.625     , 4.3125    ,\n",
       "        4.5703125 ]),\n",
       " array([4.35546875, 4.16015625, 4.51171875, ..., 4.69140625, 4.3125    ,\n",
       "        4.546875  ]),\n",
       " array([4.375     , 4.16015625, 4.34765625, ..., 4.703125  , 4.3203125 ,\n",
       "        4.5625    ]),\n",
       " array([4.39453125, 4.1640625 , 4.3671875 , ..., 4.63671875, 4.328125  ,\n",
       "        4.58984375]),\n",
       " array([4.34765625, 4.16015625, 4.34765625, ..., 4.69921875, 4.31640625,\n",
       "        4.5390625 ]),\n",
       " array([4.390625  , 4.16015625, 4.515625  , ..., 4.62890625, 4.328125  ,\n",
       "        4.58203125]),\n",
       " array([4.33203125, 4.1875    , 4.33984375, ..., 4.6796875 , 4.33984375,\n",
       "        4.52734375]),\n",
       " array([4.359375  , 4.16015625, 4.34765625, ..., 4.59375   , 4.30859375,\n",
       "        4.5546875 ]),\n",
       " array([4.3984375 , 4.1640625 , 4.3671875 , ..., 4.72265625, 4.328125  ,\n",
       "        4.59765625]),\n",
       " array([4.3671875 , 4.2265625 , 4.35546875, ..., 4.61328125, 4.31640625,\n",
       "        4.5625    ]),\n",
       " array([4.3671875 , 4.15234375, 4.35546875, ..., 4.60546875, 4.3203125 ,\n",
       "        4.5625    ]),\n",
       " array([4.359375  , 4.26171875, 4.34765625, ..., 4.6015625 , 4.3203125 ,\n",
       "        4.55078125]),\n",
       " array([4.34375   , 4.16796875, 4.5546875 , ..., 4.6875    , 4.31640625,\n",
       "        4.53515625]),\n",
       " array([4.38671875, 4.1640625 , 4.36328125, ..., 4.73828125, 4.3359375 ,\n",
       "        4.578125  ]),\n",
       " array([4.390625  , 4.16796875, 4.36328125, ..., 4.6328125 , 4.3359375 ,\n",
       "        4.5859375 ]),\n",
       " array([4.33984375, 4.18359375, 4.33984375, ..., 4.671875  , 4.33203125,\n",
       "        4.53515625]),\n",
       " array([4.35546875, 4.16796875, 4.35546875, ..., 4.70703125, 4.31640625,\n",
       "        4.5546875 ]),\n",
       " array([4.359375  , 4.171875  , 4.34765625, ..., 4.59765625, 4.3203125 ,\n",
       "        4.5546875 ]),\n",
       " array([4.34765625, 4.16796875, 4.34375   , ..., 4.59375   , 4.31640625,\n",
       "        4.54296875]),\n",
       " array([4.3203125 , 4.203125  , 4.33984375, ..., 4.5546875 , 4.3515625 ,\n",
       "        4.52734375]),\n",
       " array([4.3515625 , 4.15625   , 4.33984375, ..., 4.58984375, 4.30078125,\n",
       "        4.546875  ]),\n",
       " array([4.37890625, 4.16015625, 4.35546875, ..., 4.6171875 , 4.32421875,\n",
       "        4.5703125 ]),\n",
       " array([4.359375  , 4.15234375, 4.3359375 , ..., 4.59375   , 4.3046875 ,\n",
       "        4.546875  ]),\n",
       " array([4.359375  , 4.1640625 , 4.34765625, ..., 4.69140625, 4.31640625,\n",
       "        4.55078125]),\n",
       " array([4.38671875, 4.1640625 , 4.36328125, ..., 4.7109375 , 4.32421875,\n",
       "        4.58203125]),\n",
       " array([4.40625   , 4.1640625 , 4.37109375, ..., 4.6484375 , 4.3359375 ,\n",
       "        4.60546875]),\n",
       " array([4.34375   , 4.16796875, 4.33984375, ..., 4.6953125 , 4.31640625,\n",
       "        4.53515625]),\n",
       " array([4.359375  , 4.1640625 , 4.35546875, ..., 4.60546875, 4.4140625 ,\n",
       "        4.5625    ]),\n",
       " array([4.59765625, 4.32421875, 4.55859375, ..., 4.81640625, 4.58203125,\n",
       "        4.79296875]),\n",
       " array([4.375    , 4.15625  , 4.3515625, ..., 4.609375 , 4.3203125,\n",
       "        4.5625   ]),\n",
       " array([4.3359375 , 4.171875  , 4.33203125, ..., 4.6875    , 4.3203125 ,\n",
       "        4.52734375]),\n",
       " array([4.359375  , 4.15625   , 4.34765625, ..., 4.703125  , 4.31640625,\n",
       "        4.5546875 ]),\n",
       " array([4.390625  , 4.1640625 , 4.36328125, ..., 4.73828125, 4.33203125,\n",
       "        4.5859375 ]),\n",
       " array([4.35546875, 4.1640625 , 4.34375   , ..., 4.68359375, 4.3203125 ,\n",
       "        4.55078125]),\n",
       " array([4.3515625 , 4.1796875 , 4.34765625, ..., 4.703125  , 4.33203125,\n",
       "        4.55859375]),\n",
       " array([4.39453125, 4.16796875, 4.3671875 , ..., 4.70703125, 4.33203125,\n",
       "        4.58984375]),\n",
       " array([4.390625  , 4.28125   , 4.36328125, ..., 4.6328125 , 4.3203125 ,\n",
       "        4.58984375]),\n",
       " array([4.37109375, 4.17578125, 4.36328125, ..., 4.67578125, 4.32421875,\n",
       "        4.5703125 ]),\n",
       " array([4.3515625 , 4.16015625, 4.34765625, ..., 4.69140625, 4.31640625,\n",
       "        4.55078125]),\n",
       " array([4.37109375, 4.15234375, 4.34765625, ..., 4.6953125 , 4.3125    ,\n",
       "        4.56640625]),\n",
       " array([4.34375   , 4.1640625 , 4.33984375, ..., 4.578125  , 4.30859375,\n",
       "        4.53125   ]),\n",
       " array([4.36328125, 4.171875  , 4.34765625, ..., 4.59765625, 4.31640625,\n",
       "        4.55859375]),\n",
       " array([4.3671875 , 4.1640625 , 4.34765625, ..., 4.6875    , 4.3203125 ,\n",
       "        4.55859375]),\n",
       " array([4.3515625 , 4.16796875, 4.34765625, ..., 4.58984375, 4.30859375,\n",
       "        4.54296875]),\n",
       " array([4.34765625, 4.1640625 , 4.34375   , ..., 4.5859375 , 4.31640625,\n",
       "        4.53515625]),\n",
       " array([4.3515625 , 4.18359375, 4.35546875, ..., 4.59765625, 4.328125  ,\n",
       "        4.5546875 ]),\n",
       " array([4.36328125, 4.16796875, 4.34765625, ..., 4.609375  , 4.31640625,\n",
       "        4.5625    ]),\n",
       " array([4.375     , 4.1484375 , 4.34765625, ..., 4.609375  , 4.30859375,\n",
       "        4.55859375]),\n",
       " array([4.3671875 , 4.15625   , 4.3515625 , ..., 4.60546875, 4.30859375,\n",
       "        4.5546875 ]),\n",
       " array([4.4375    , 4.19921875, 4.40234375, ..., 4.69140625, 4.38671875,\n",
       "        4.64453125]),\n",
       " array([4.33984375, 4.16796875, 4.33984375, ..., 4.69921875, 4.30859375,\n",
       "        4.53515625]),\n",
       " array([4.359375  , 4.15625   , 4.34375   , ..., 4.6015625 , 4.30859375,\n",
       "        4.55078125]),\n",
       " array([4.36328125, 4.16796875, 4.34765625, ..., 4.703125  , 4.30859375,\n",
       "        4.546875  ]),\n",
       " array([4.33984375, 4.19140625, 4.46875   , ..., 4.56640625, 4.3359375 ,\n",
       "        4.5390625 ]),\n",
       " array([4.36328125, 4.1640625 , 4.3515625 , ..., 4.6015625 , 4.31640625,\n",
       "        4.76171875]),\n",
       " array([4.34765625, 4.1640625 , 4.33203125, ..., 4.58203125, 4.3125    ,\n",
       "        4.5390625 ]),\n",
       " array([4.34765625, 4.17578125, 4.33984375, ..., 4.640625  , 4.328125  ,\n",
       "        4.5390625 ]),\n",
       " array([4.34375   , 4.17578125, 4.34375   , ..., 4.57421875, 4.32421875,\n",
       "        4.53515625]),\n",
       " array([4.36328125, 4.15234375, 4.34375   , ..., 4.7265625 , 4.30859375,\n",
       "        4.55078125]),\n",
       " array([4.37109375, 4.1640625 , 4.3515625 , ..., 4.609375  , 4.3125    ,\n",
       "        4.55859375]),\n",
       " array([4.34375   , 4.171875  , 4.34375   , ..., 4.58203125, 4.3203125 ,\n",
       "        4.5390625 ]),\n",
       " array([4.3515625, 4.1640625, 4.34375  , ..., 4.71875  , 4.3203125,\n",
       "        4.546875 ]),\n",
       " array([4.34765625, 4.17578125, 4.34765625, ..., 4.58984375, 4.328125  ,\n",
       "        4.546875  ]),\n",
       " array([4.34765625, 4.1796875 , 4.33984375, ..., 4.578125  , 4.32421875,\n",
       "        4.54296875]),\n",
       " array([4.375     , 4.15625   , 4.40625   , ..., 4.62109375, 4.31640625,\n",
       "        4.56640625]),\n",
       " array([4.3515625 , 4.16015625, 4.34375   , ..., 4.59375   , 4.30859375,\n",
       "        4.54296875]),\n",
       " array([4.36328125, 4.1640625 , 4.34765625, ..., 4.59375   , 4.31640625,\n",
       "        4.5546875 ]),\n",
       " array([4.48828125, 4.25      , 4.4453125 , ..., 4.73828125, 4.44921875,\n",
       "        4.6953125 ]),\n",
       " array([4.359375  , 4.1640625 , 4.3515625 , ..., 4.69921875, 4.3125    ,\n",
       "        4.5546875 ]),\n",
       " array([4.45703125, 4.22265625, 4.41796875, ..., 4.69921875, 4.4140625 ,\n",
       "        4.65625   ]),\n",
       " array([4.3515625 , 4.17578125, 4.34765625, ..., 4.5859375 , 4.31640625,\n",
       "        4.546875  ]),\n",
       " array([4.375     , 4.16015625, 4.35546875, ..., 4.72265625, 4.30859375,\n",
       "        4.56640625]),\n",
       " array([4.3671875 , 4.16796875, 4.34765625, ..., 4.73046875, 4.32421875,\n",
       "        4.5546875 ]),\n",
       " array([4.390625  , 4.15625   , 4.36328125, ..., 4.71484375, 4.328125  ,\n",
       "        4.58984375]),\n",
       " array([4.359375 , 4.171875 , 4.3515625, ..., 4.6015625, 4.3125   ,\n",
       "        4.5546875]),\n",
       " array([4.3359375 , 4.16796875, 4.3359375 , ..., 4.5703125 , 4.3125    ,\n",
       "        4.53125   ]),\n",
       " array([4.37109375, 4.15625   , 4.44921875, ..., 4.61328125, 4.32421875,\n",
       "        4.5625    ]),\n",
       " array([4.37890625, 4.16015625, 4.35546875, ..., 4.70703125, 4.3203125 ,\n",
       "        4.5703125 ]),\n",
       " array([4.3359375 , 4.18359375, 4.33984375, ..., 4.65625   , 4.3359375 ,\n",
       "        4.53515625]),\n",
       " array([4.36328125, 4.16796875, 4.34765625, ..., 4.59765625, 4.31640625,\n",
       "        4.546875  ]),\n",
       " array([4.3671875 , 4.15625   , 4.34765625, ..., 4.69140625, 4.31640625,\n",
       "        4.55078125])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32c1e2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_new = sc.AnnData(np.array(outdict)[:,1:], obs=adata.obs, var = adata.var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7124b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 324 × 11968\n",
       "    obs: 'in_tissue', 'array_row', 'array_col', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes', 'total_counts_MT', 'log1p_total_counts_MT', 'pct_counts_MT', 'n_counts', 'leiden', 'cluster', 'batch', 'celltype', 'str_batch', 'batch_id'\n",
       "    var: 'gene_ids', 'feature_types', 'genome', 'MT', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm', 'gene_name', 'id_in_vocab', 'n_counts'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3f10538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import count_nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "044845ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.16015625, 4.33984375, 4.73046875, ..., 4.58984375, 4.4140625 ,\n",
       "        4.54296875],\n",
       "       [4.16796875, 4.34765625, 5.2734375 , ..., 4.59375   , 4.4296875 ,\n",
       "        4.54296875],\n",
       "       [4.19140625, 4.33984375, 4.6953125 , ..., 4.70703125, 4.3359375 ,\n",
       "        4.52734375],\n",
       "       ...,\n",
       "       [4.18359375, 4.33984375, 4.69921875, ..., 4.65625   , 4.3359375 ,\n",
       "        4.53515625],\n",
       "       [4.16796875, 4.34765625, 5.14453125, ..., 4.59765625, 4.31640625,\n",
       "        4.546875  ],\n",
       "       [4.15625   , 4.34765625, 4.890625  , ..., 4.69140625, 4.31640625,\n",
       "        4.55078125]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_new.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d9b2253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - count_nonzero(adata_new.X) / float(adata_new.X.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f30fe87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_new.write_h5ad(\"spatial_imputation.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0458c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eb4368c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_list = []\n",
    "pval_list = []\n",
    "for item in adata.var_names:\n",
    "    adata1 = adata[:,item]\n",
    "    adata2 = adata_new[:,item]\n",
    "    cor, pval = scipy.stats.pearsonr(np.array(adata1.X.todense().T)[0], np.array(adata2.X.T)[0])\n",
    "    cor_list.append(cor)\n",
    "    pval_list.append(pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "91dcdf98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29276355298057793,\n",
       " 0.06364279694947046,\n",
       " 0.8980683546273069,\n",
       " 0.8854411550146363,\n",
       " 0.8019262094519675,\n",
       " -0.7502244537010024,\n",
       " 0.3712056935388391,\n",
       " -0.4059514394723756,\n",
       " 0.35108149606479644,\n",
       " 0.6631847272852014,\n",
       " -0.3161655447586613,\n",
       " 0.6560774758977889,\n",
       " 0.4627860016478608,\n",
       " 0.5722303857834256,\n",
       " 0.531061342746727,\n",
       " -0.7896652972697417,\n",
       " 0.3255529950717632,\n",
       " 0.6360763191218959,\n",
       " 0.5850618531274792,\n",
       " -0.27082775045670654,\n",
       " 0.8068808601739471,\n",
       " 0.12497535724908855,\n",
       " 0.9051800136957762,\n",
       " 0.7216033987616801,\n",
       " 0.5543028651916058,\n",
       " -0.38737139750098926,\n",
       " -0.26197595650440025,\n",
       " -0.016537888329892707,\n",
       " 0.724860976834234,\n",
       " 0.8278312271266361,\n",
       " 0.1860608985932782,\n",
       " -0.35221368842270756,\n",
       " 0.16169084231195094,\n",
       " -0.07300443668853751,\n",
       " 0.6096803050095367,\n",
       " -0.07666016996540188,\n",
       " 0.08088371197380358,\n",
       " 0.9136873959871202,\n",
       " 0.8277552915976989,\n",
       " -0.1175753644456426,\n",
       " 0.8942058712263915,\n",
       " -0.09487078011198649,\n",
       " -0.7261064793480408,\n",
       " -0.6204740721997084,\n",
       " 0.04386197866922077,\n",
       " 0.25474394629976393,\n",
       " -0.509141506780486,\n",
       " 0.6200976185897545,\n",
       " -0.021496403705718925,\n",
       " -0.39755767612804577,\n",
       " -0.7959300392275946,\n",
       " 0.7036746559463715,\n",
       " 0.35349920937598156,\n",
       " 0.8063593095080493,\n",
       " -0.4853225162991654,\n",
       " 0.004368631024739029,\n",
       " 0.2651713833079835,\n",
       " -0.10587277152025125,\n",
       " -0.33044541877856126,\n",
       " 0.5277305437850635,\n",
       " 0.5587784957147371,\n",
       " 0.5343973307503016,\n",
       " 0.10136095261345418,\n",
       " 0.6017028024134178,\n",
       " -0.8171472471059931,\n",
       " -0.7892161852610304,\n",
       " 0.8049845645318,\n",
       " 0.5771594506645973,\n",
       " 0.48730596496968437,\n",
       " -0.42780535296148836,\n",
       " -0.7369999473858805,\n",
       " 0.6512071775900297,\n",
       " 0.4853330465249281,\n",
       " -0.20296752493424272,\n",
       " 0.20705980933960733,\n",
       " 0.4057746460808524,\n",
       " 0.3315315769858536,\n",
       " 0.744229748617705,\n",
       " 0.044047810996235136,\n",
       " -0.5177057601458804,\n",
       " -0.2759957717798472,\n",
       " 0.6671966468178079,\n",
       " 0.4142767031149103,\n",
       " -0.5411372047868588,\n",
       " 0.01512771274312955,\n",
       " 0.2994207021877596,\n",
       " 0.6305230048331236,\n",
       " -0.7573968410741739,\n",
       " 0.8336832355145558,\n",
       " 0.8179207557441325,\n",
       " 0.7476495853357596,\n",
       " -0.021006505673251373,\n",
       " 0.4592070685784967,\n",
       " 0.5668762410238619,\n",
       " 0.3303498535213654,\n",
       " 0.8352236675541779,\n",
       " 0.7539261570930818,\n",
       " 0.3137409514512849,\n",
       " 0.6575503232782333,\n",
       " -0.663709618798319,\n",
       " 0.19694491385332138,\n",
       " 0.814316652494451,\n",
       " 0.0772379973348306,\n",
       " -0.2751446419080078,\n",
       " 0.6474243623534967,\n",
       " 0.6772566036560907,\n",
       " 0.32415888979780455,\n",
       " -0.2340363464488268,\n",
       " 0.7894552615135827,\n",
       " 0.11313023035121202,\n",
       " 0.04088367483826676,\n",
       " -0.1011029250269943,\n",
       " -0.2133178885550215,\n",
       " -0.5887294138225827,\n",
       " 0.8215352955842303,\n",
       " 0.513326626680767,\n",
       " 0.5368436805167354,\n",
       " 0.13167747496160428,\n",
       " 0.7035187812772676,\n",
       " 0.03666696822562149,\n",
       " 0.6427483706502024,\n",
       " 0.8392480654783308,\n",
       " 0.6072418807851159,\n",
       " 0.4576919624929276,\n",
       " 0.043210023434707694,\n",
       " 0.7428029540785969,\n",
       " 0.12805384488605753,\n",
       " 0.22255964703764133,\n",
       " 0.22235947516378826,\n",
       " 0.35068650738368695,\n",
       " 0.26569375348160584,\n",
       " 0.48700279387926726,\n",
       " -0.24427177284189525,\n",
       " 0.3956604915466763,\n",
       " 0.21576300781310473,\n",
       " 0.675195236922847,\n",
       " 0.17660463060880754,\n",
       " -0.36557292102493955,\n",
       " -0.549378340247753,\n",
       " -0.13859924476347613,\n",
       " 0.410912546850806,\n",
       " 0.03250880614011255,\n",
       " 0.3459437629728252,\n",
       " 0.6238253440560066,\n",
       " 0.30009233795921253,\n",
       " 0.589655887770965,\n",
       " 0.4410430237030537,\n",
       " 0.7371258474282631,\n",
       " 0.49867589323310896,\n",
       " -0.17964092806647677,\n",
       " 0.30817067113457486,\n",
       " 0.26385535824119405,\n",
       " -0.03423280975431683,\n",
       " 0.6957500990811779,\n",
       " 0.7952570012987553,\n",
       " 0.6474514091867997,\n",
       " -0.04017145866936908,\n",
       " 0.8620733287092075,\n",
       " 0.31596709733795253,\n",
       " -0.42482905661958054,\n",
       " 0.05674700262332862,\n",
       " -0.6444137900685881,\n",
       " 0.15607725952937274,\n",
       " 0.36309256110885507,\n",
       " -0.12474526123190442,\n",
       " 0.5160346988141733,\n",
       " -0.020035152961713595,\n",
       " 0.6177878057902981,\n",
       " 0.5098012324419721,\n",
       " 0.8165801878144587,\n",
       " 0.179002165963171,\n",
       " 0.005204277440510831,\n",
       " 0.7329608186876452,\n",
       " -0.28324470974702104,\n",
       " 0.13986773941588693,\n",
       " 0.2532509646682994,\n",
       " 0.056923723398876566,\n",
       " 0.727326781100564,\n",
       " 0.29353306737827367,\n",
       " 0.2144647559310475,\n",
       " 0.1940870160367497,\n",
       " 0.6186205214274695,\n",
       " 0.7393155972618631,\n",
       " -0.09354880344372625,\n",
       " 0.5528567425504861,\n",
       " 0.11792067822097424,\n",
       " 0.5510139726865855,\n",
       " -0.11881062683036998,\n",
       " 0.6349420701007419,\n",
       " 0.5477307112379243,\n",
       " 0.5411720846563145,\n",
       " 0.10438427873122479,\n",
       " -0.25642113690920615,\n",
       " 0.16809752019375396,\n",
       " 0.5203226554916822,\n",
       " 0.5963417816090932,\n",
       " 0.37484367974726057,\n",
       " 0.2115021655025675,\n",
       " 0.45271090283007925,\n",
       " -0.2717703247459205,\n",
       " 0.6783346360664403,\n",
       " 0.03211732303152507,\n",
       " 0.7809214756967813,\n",
       " 0.6152343767957926,\n",
       " 0.44120742329126283,\n",
       " 0.00047992847291755675,\n",
       " 0.4194693702619321,\n",
       " 0.5454383486794827,\n",
       " 0.20382218596123888,\n",
       " -0.5031929546395055,\n",
       " -0.6515529199054759,\n",
       " 0.43908818989796694,\n",
       " 0.4077001420119075,\n",
       " 0.19526785868816768,\n",
       " 0.5547284248796782,\n",
       " -0.12658611962500133,\n",
       " 0.31569196562675417,\n",
       " 0.34685109985707835,\n",
       " 0.5929793575042711,\n",
       " 0.7258991748122005,\n",
       " 0.0960016158252977,\n",
       " 0.47058244286667233,\n",
       " 0.047191972597432616,\n",
       " 0.21311520670364445,\n",
       " 0.14602152233920843,\n",
       " -0.5509529639656066,\n",
       " 0.7221847331059335,\n",
       " 0.527638497713434,\n",
       " 0.48715248977135844,\n",
       " 0.28094342945163714,\n",
       " 0.6018148415820984,\n",
       " 0.7949262549340552,\n",
       " 0.7936076405101359,\n",
       " 0.08282149543546106,\n",
       " 0.2841470919042373,\n",
       " 0.6557435640039028,\n",
       " 0.4914703802641117,\n",
       " 0.7488687472155263,\n",
       " -0.46086904091637826,\n",
       " 0.6708645745140581,\n",
       " 0.6255643061967143,\n",
       " 0.14903755485796838,\n",
       " 0.17122448583414204,\n",
       " 0.32881160036149776,\n",
       " -0.19946943279037996,\n",
       " 0.3287328311274407,\n",
       " -0.2351226548563419,\n",
       " -0.29006065019654687,\n",
       " 0.4092742336676094,\n",
       " 0.21914548895679478,\n",
       " 0.7331387379454106,\n",
       " 0.7020642528708044,\n",
       " 0.7539723242594236,\n",
       " 0.5726541722912502,\n",
       " -0.11201907540785133,\n",
       " 0.21517463203190873,\n",
       " 0.2658106287684004,\n",
       " 0.812096845110801,\n",
       " -0.8358859820717142,\n",
       " 0.618501854794246,\n",
       " -0.5697616207454075,\n",
       " -0.0526293793643597,\n",
       " 0.8121251185503882,\n",
       " 0.09936868456715715,\n",
       " 0.12664433530917404,\n",
       " 0.6913655136261969,\n",
       " -0.2147225656778867,\n",
       " -0.32855954073143717,\n",
       " 0.4279925335372064,\n",
       " -0.15100461365468393,\n",
       " -0.44832237983991724,\n",
       " 0.6246890302241548,\n",
       " -0.5376551382313752,\n",
       " -0.7228382568020499,\n",
       " -0.5398169968216291,\n",
       " 0.7631091390013347,\n",
       " 0.07311719412497678,\n",
       " -0.0978002471077235,\n",
       " -0.7459173858379198,\n",
       " 0.27762264178555346,\n",
       " 0.8136866683881021,\n",
       " 0.7847327198759203,\n",
       " -0.5182279005118673,\n",
       " 0.6612496600069347,\n",
       " 0.5001970683255503,\n",
       " -0.47174742789889307,\n",
       " -0.25496538170926275,\n",
       " 0.13740184306998096,\n",
       " 0.09439805185659868,\n",
       " 0.4379944154303745,\n",
       " -0.25383159406712524,\n",
       " 0.6162937552657308,\n",
       " 0.6822665944051169,\n",
       " -0.8241944540752788,\n",
       " 0.7267023153568376,\n",
       " -0.1730650251035596,\n",
       " 0.12195238533139284,\n",
       " 0.06423395165525032,\n",
       " 0.1375907151904791,\n",
       " -0.485606230313938,\n",
       " 0.8844734621558854,\n",
       " 0.46321753993461506,\n",
       " 0.23659738592906004,\n",
       " -0.46262015186757993,\n",
       " 0.7032685697329372,\n",
       " 0.6387718064458533,\n",
       " 0.6063163034726733,\n",
       " 0.6763212290683973,\n",
       " 0.23318506066373976,\n",
       " 0.8188736390619519,\n",
       " 0.26783310187668397,\n",
       " 0.8257736947774917,\n",
       " -0.3572994496810526,\n",
       " 0.27260326788731326,\n",
       " 0.4527957996362357,\n",
       " 0.374613543997841,\n",
       " 0.2781026012772492,\n",
       " 0.7876927339146517,\n",
       " 0.1840954524272068,\n",
       " 0.40836872773713667,\n",
       " 0.8106025708285571,\n",
       " 0.008604970077368962,\n",
       " 0.1581874774384703,\n",
       " 0.34115821983512135,\n",
       " 0.886415952675617,\n",
       " 0.85405657303726,\n",
       " -0.5921438297658794,\n",
       " 0.8623368033474026,\n",
       " 0.5480133274856226,\n",
       " 0.7981080244960144,\n",
       " 0.6929751277529836,\n",
       " 0.3734244799233059,\n",
       " 0.07214331631228606,\n",
       " 0.718684096375652,\n",
       " 0.24984922212788216,\n",
       " 0.20850973041357207,\n",
       " 0.7517170012578236,\n",
       " -0.31222408246973327,\n",
       " 0.12344797157580566,\n",
       " 0.31686311317857657,\n",
       " 0.3333721611981377,\n",
       " 0.8159650917273467,\n",
       " 0.6039873775302306,\n",
       " 0.17449884154538786,\n",
       " 0.08823838964056904,\n",
       " 0.6815124677010936,\n",
       " 0.16165164115831387,\n",
       " 0.7690413512257589,\n",
       " 0.8453662417831714,\n",
       " 0.410636981485884,\n",
       " 0.6366477479326424,\n",
       " 0.6523486133079792,\n",
       " 0.7234367081801222,\n",
       " 0.8149955543270063,\n",
       " 0.8692919861414348,\n",
       " 0.5201017940122781,\n",
       " 0.4665979701497326,\n",
       " 0.14849412894471303,\n",
       " -0.4657197411594526,\n",
       " 0.5868514678716888,\n",
       " 0.38672777508382805,\n",
       " 0.43875687543946107,\n",
       " -0.46367964000319084,\n",
       " 0.3560570810291774,\n",
       " 0.14195431295629,\n",
       " 0.2916492946401659,\n",
       " -0.37248050821319656,\n",
       " 0.3416656674992869,\n",
       " 0.4040201678929726,\n",
       " 0.5652383666093133,\n",
       " 0.5626958669167754,\n",
       " 0.45424897147359256,\n",
       " 0.38499101067419456,\n",
       " 0.7453467242114944,\n",
       " 0.26425301751459607,\n",
       " 0.09304489232800034,\n",
       " -0.41268838969533966,\n",
       " 0.10749607934041551,\n",
       " 0.8312862565196137,\n",
       " 0.5763542876794125,\n",
       " 0.7319600189944675,\n",
       " -0.09162215110055749,\n",
       " 0.316875751518448,\n",
       " 0.11619525390713088,\n",
       " 0.22832702293904122,\n",
       " 0.2920676795410867,\n",
       " 0.3853254205254262,\n",
       " 0.4905272427795632,\n",
       " 0.17656842390086316,\n",
       " 0.1500044053348718,\n",
       " -0.759868582180685,\n",
       " 0.26661975201034227,\n",
       " -0.329317868254591,\n",
       " 0.31038322130027945,\n",
       " 0.4310721240630413,\n",
       " 0.6487786989602305,\n",
       " -0.6745330526748679,\n",
       " 0.5133183021932688,\n",
       " 0.41353403334560607,\n",
       " -0.7557343641895694,\n",
       " 0.2257233771984213,\n",
       " -0.3882895855481202,\n",
       " 0.6182985199791621,\n",
       " 0.5366189105377678,\n",
       " 0.2273253793192339,\n",
       " 0.39988468247023695,\n",
       " -0.37887852205900385,\n",
       " 0.8179263501075088,\n",
       " 0.0025418290387867934,\n",
       " 0.29929410758114955,\n",
       " 0.28270208640803157,\n",
       " 0.32455012115668797,\n",
       " -0.11559201168560423,\n",
       " 0.7395198809803551,\n",
       " 0.6875659039962243,\n",
       " 0.6459722406052927,\n",
       " 0.6724417608866298,\n",
       " 0.7797000215592828,\n",
       " 0.7803060067889573,\n",
       " 0.09773664985983499,\n",
       " 0.0727894434960448,\n",
       " 0.34460129459929595,\n",
       " 0.32983372502666924,\n",
       " 0.45306541301593484,\n",
       " -0.013980209626008856,\n",
       " 0.610897273210713,\n",
       " -0.584970681637957,\n",
       " 0.7209551835820642,\n",
       " 0.254846419279157,\n",
       " 0.28909316773622185,\n",
       " 0.49777936574081205,\n",
       " 0.3814610662666728,\n",
       " 0.2636796338639438,\n",
       " 0.07093471402940181,\n",
       " -0.2697222001793145,\n",
       " -0.8201824158562425,\n",
       " -0.0429182832780009,\n",
       " -0.005781949416719212,\n",
       " 0.43651543184604685,\n",
       " 0.038548718443814814,\n",
       " -0.5361633345932999,\n",
       " 0.4282143952311455,\n",
       " 0.35201014321399204,\n",
       " 0.8585884272351784,\n",
       " 0.4481405355376892,\n",
       " -0.13797160134521375,\n",
       " 0.3451720386324873,\n",
       " -0.3757559659871039,\n",
       " -0.6859240079173001,\n",
       " 0.14806142318725857,\n",
       " 0.900819200881633,\n",
       " 0.6539106400371372,\n",
       " 0.9094519521355819,\n",
       " 0.47636060728870416,\n",
       " 0.759819641454861,\n",
       " 0.649873510242811,\n",
       " -0.26748280146528064,\n",
       " 0.12335335604435889,\n",
       " 0.2108896164622023,\n",
       " 0.3594307383814781,\n",
       " 0.3188335793017913,\n",
       " -0.5715427197547178,\n",
       " 0.7809521074963003,\n",
       " 0.8695845664021422,\n",
       " -0.7038729064720817,\n",
       " 0.8216653383359934,\n",
       " -0.38624493576219204,\n",
       " 0.3398390413772257,\n",
       " 0.19891892043522508,\n",
       " -0.3807063742850359,\n",
       " -0.14190385693264157,\n",
       " 0.16792343700858292,\n",
       " 0.5867676013816556,\n",
       " 0.5008739698048164,\n",
       " -0.2606769424927865,\n",
       " -0.6749279680379072,\n",
       " 0.46230689947766845,\n",
       " 0.33230712438873494,\n",
       " 0.7199957169583285,\n",
       " 0.24755940526786438,\n",
       " 0.5577096053152736,\n",
       " -0.04119278653132565,\n",
       " -0.6380167163499393,\n",
       " 0.1976950933541883,\n",
       " -0.8214475274554578,\n",
       " 0.7733229127343221,\n",
       " 0.880625419940772,\n",
       " 0.85208773035289,\n",
       " 0.8861852803996119,\n",
       " -0.7625795325917877,\n",
       " 0.348425909028746,\n",
       " 0.4391088676055087,\n",
       " 0.13127088899554892,\n",
       " 0.8113355915633953,\n",
       " 0.014011874590150075,\n",
       " -0.053551366383851114,\n",
       " 0.6939130004518672,\n",
       " 0.4430379942864395,\n",
       " -0.20843190544390547,\n",
       " 0.5056124227812074,\n",
       " 0.2691782993699847,\n",
       " 0.8199262923687017,\n",
       " 0.06243733182731951,\n",
       " 0.11056412045924519,\n",
       " 0.6621650392364719,\n",
       " 0.28107469184559647,\n",
       " 0.6899159505106539,\n",
       " -0.5346183114153971,\n",
       " 0.42780392658617916,\n",
       " 0.54177666797389,\n",
       " -0.11029592585532334,\n",
       " 0.7886481309642104,\n",
       " 0.260683775871149,\n",
       " -0.2075475077943926,\n",
       " 0.45056612653140427,\n",
       " -0.012495909919442771,\n",
       " -0.3102992135097773,\n",
       " -0.8880365088579897,\n",
       " -0.7028900826974848,\n",
       " 0.8584079704812742,\n",
       " 0.32720384059895935,\n",
       " 0.8885580794446823,\n",
       " 0.311846870927833,\n",
       " 0.7529926338204062,\n",
       " 0.6842278968924534,\n",
       " 0.28719233700020436,\n",
       " 0.6271699191843213,\n",
       " 0.6713098912218001,\n",
       " -0.077449584849408,\n",
       " 0.6034520177780373,\n",
       " 0.0542556569088446,\n",
       " 0.37813408759429973,\n",
       " 0.26327414244439656,\n",
       " 0.7056130247729834,\n",
       " -0.4156570568925654,\n",
       " -0.31619661483292033,\n",
       " -0.4349823682774838,\n",
       " -0.6658868750010498,\n",
       " 0.24239398048879968,\n",
       " -0.779498549534932,\n",
       " 0.9164377355819076,\n",
       " 0.16860097690480225,\n",
       " 0.6316184025344841,\n",
       " 0.775324203539236,\n",
       " 0.33396317581039403,\n",
       " -0.3897810185385679,\n",
       " -0.1426994263658177,\n",
       " 0.6181683280062377,\n",
       " -0.2923869494845765,\n",
       " 0.7041109299299115,\n",
       " 0.10074821274131837,\n",
       " 0.5463568280837809,\n",
       " -0.6398906594682903,\n",
       " 0.44448919221273087,\n",
       " 0.7132675538589248,\n",
       " -0.052657106971119666,\n",
       " 0.7561098921607009,\n",
       " -0.16887865429824023,\n",
       " 0.27816977605242243,\n",
       " 0.005180021464012217,\n",
       " -0.4574177768183788,\n",
       " 0.14315476644666036,\n",
       " 0.3690426039477091,\n",
       " 0.115606198836964,\n",
       " 0.6317122077787403,\n",
       " 0.154261352330772,\n",
       " 0.3769720600875553,\n",
       " 0.5906674954931368,\n",
       " -0.33915810227527426,\n",
       " -0.2131310019830123,\n",
       " 0.8911566750787426,\n",
       " 0.4484654835091469,\n",
       " 0.716470785153782,\n",
       " -0.44078491414568904,\n",
       " 0.6231750324871538,\n",
       " 0.36445039079158453,\n",
       " 0.711351530256166,\n",
       " 0.29919282214398807,\n",
       " -0.5622875006373804,\n",
       " 0.7265593637168715,\n",
       " 0.007833512080343984,\n",
       " 0.08874711824983803,\n",
       " 0.6974156234299006,\n",
       " 0.9014501886496171,\n",
       " 0.6790651591460672,\n",
       " -0.8453080735172447,\n",
       " -0.25905523008044773,\n",
       " 0.06867002262298892,\n",
       " -0.3022165579630968,\n",
       " 0.3271546273827075,\n",
       " 0.43832105276499267,\n",
       " 0.31553356187485837,\n",
       " 0.2392584290467461,\n",
       " 0.7019049976074802,\n",
       " 0.1628790734707052,\n",
       " 0.338474176360908,\n",
       " 0.6594046614623499,\n",
       " 0.6723501599613133,\n",
       " 0.09320266808523188,\n",
       " 0.9415724401091804,\n",
       " -0.18876121540072985,\n",
       " -0.1486869937063849,\n",
       " 0.16227408858565406,\n",
       " 0.7689830387050369,\n",
       " 0.24166406009662453,\n",
       " 0.2457703761142657,\n",
       " 0.6528579026353852,\n",
       " 0.6037092957663927,\n",
       " 0.452272062083468,\n",
       " 0.7929075209898918,\n",
       " 0.41501993007373184,\n",
       " 0.3145856862062897,\n",
       " 0.6040893071965044,\n",
       " -0.05055099971880432,\n",
       " -0.17926218820494877,\n",
       " 0.2631504955395256,\n",
       " 0.2653568993574041,\n",
       " 0.5290711282682974,\n",
       " -0.17501277257220457,\n",
       " 0.4686076966870413,\n",
       " 0.7941073613570058,\n",
       " 0.7564119539793176,\n",
       " 0.7983223503394862,\n",
       " 0.5430829957049145,\n",
       " -0.5306756775274346,\n",
       " -0.004521877835556456,\n",
       " -0.5805268212586234,\n",
       " -0.5315071176757045,\n",
       " 0.17037848499733807,\n",
       " 0.029018693739633856,\n",
       " -0.017406530373827438,\n",
       " 0.47253829669992603,\n",
       " -0.2572495654879191,\n",
       " 0.4489872406878956,\n",
       " 0.7521516864995335,\n",
       " -0.7541566595058156,\n",
       " -0.60543612257305,\n",
       " 0.2111965111697231,\n",
       " 0.24517953654377417,\n",
       " 0.269004378093767,\n",
       " 0.5764859440512369,\n",
       " 0.38972683231932703,\n",
       " 0.7774025125207766,\n",
       " 0.6049062620348424,\n",
       " -0.3651092940172527,\n",
       " -0.6338813172458545,\n",
       " 0.7322253482832426,\n",
       " 0.9235100242761818,\n",
       " -0.6453431825560886,\n",
       " 0.230463235508265,\n",
       " -0.3680370537772411,\n",
       " 0.667181606188529,\n",
       " 0.7948998352254011,\n",
       " 0.6823071086009515,\n",
       " 0.8432032046726625,\n",
       " 0.005467339182309818,\n",
       " 0.14805093402920458,\n",
       " 0.8184689388627835,\n",
       " 0.5279793109774306,\n",
       " 0.46447661440261995,\n",
       " -0.8038175456225349,\n",
       " 0.7402953258384594,\n",
       " 0.2582409664457027,\n",
       " 0.7868911193245511,\n",
       " 0.6172934282017328,\n",
       " -0.03323403640014538,\n",
       " 0.7806734722449058,\n",
       " -0.3788046249633919,\n",
       " -0.20020527104582958,\n",
       " 0.6541523649765603,\n",
       " 0.7612451012135483,\n",
       " 0.5225982537909631,\n",
       " 0.7844384132475481,\n",
       " -0.4633450014019061,\n",
       " 0.13506396754135533,\n",
       " 0.5936117742227377,\n",
       " -0.6981661621376568,\n",
       " 0.48557120748494254,\n",
       " 0.668537534362181,\n",
       " 0.8028701060685134,\n",
       " 0.7292153447966605,\n",
       " -0.7270888389752338,\n",
       " 0.3814381072026498,\n",
       " 0.6686097226879247,\n",
       " 0.8736324880011387,\n",
       " 0.6834774428354105,\n",
       " -0.4309830628355095,\n",
       " 0.4446515495528306,\n",
       " -0.4201854173897688,\n",
       " 0.7224830761267171,\n",
       " 0.8461963568369979,\n",
       " 0.5876227241659181,\n",
       " 0.07229683830189616,\n",
       " 0.35988598589994736,\n",
       " -0.1505388819559788,\n",
       " 0.04726048138391488,\n",
       " 0.45950150525015643,\n",
       " -0.5972914341421773,\n",
       " -0.2260838284047685,\n",
       " -0.08728002876067789,\n",
       " -0.015075594286536825,\n",
       " 0.24572912345661346,\n",
       " -0.02600395006208265,\n",
       " -0.42929592070222045,\n",
       " -0.18696776008050334,\n",
       " 0.47661707249059065,\n",
       " -0.23267306953543465,\n",
       " 0.015290151777313254,\n",
       " -0.054781617227725045,\n",
       " 0.23371456383161837,\n",
       " 0.26648983055890985,\n",
       " 0.8749098806432721,\n",
       " 0.5388395532152394,\n",
       " 0.6838805153914873,\n",
       " -0.08684185130121516,\n",
       " 0.35069403880880623,\n",
       " 0.39054789172535526,\n",
       " 0.6072905368618727,\n",
       " -0.3814671790098202,\n",
       " 0.1946699135853853,\n",
       " -0.024101876503968323,\n",
       " 0.44825898728186697,\n",
       " -0.2781994820277631,\n",
       " 0.6321231451720264,\n",
       " 0.5535817942665039,\n",
       " -0.7346575731288949,\n",
       " -0.6143894749910986,\n",
       " 0.4791869248957552,\n",
       " 0.423963164201868,\n",
       " -0.21106699276360186,\n",
       " 0.3291276743693816,\n",
       " 0.631193742063745,\n",
       " -0.3653929414425648,\n",
       " -0.24045177213709926,\n",
       " 0.3720777045942259,\n",
       " 0.376854949628311,\n",
       " 0.41117010609247423,\n",
       " 0.3421184731472302,\n",
       " 0.503536021295618,\n",
       " 0.8285374424545549,\n",
       " 0.5681584017742602,\n",
       " -0.47626338839280136,\n",
       " 0.5279979386916939,\n",
       " 0.47114383810433597,\n",
       " -0.05336518704635093,\n",
       " 0.8106668191044513,\n",
       " 0.7500554513276578,\n",
       " 0.4606407362200709,\n",
       " 0.37400966724496515,\n",
       " 0.8852402475337385,\n",
       " -0.09234855315116312,\n",
       " 0.6419290322548926,\n",
       " -0.48363555976718836,\n",
       " 0.6544482235598763,\n",
       " 0.5393568549801375,\n",
       " 0.041194124823456656,\n",
       " 9.561352920441715e-05,\n",
       " 0.5287104770543949,\n",
       " 0.5984974050561576,\n",
       " 0.4537528978393588,\n",
       " 0.2494008109398166,\n",
       " 0.6153692141254344,\n",
       " 0.37823008741613406,\n",
       " -0.22056500401998774,\n",
       " 0.24820027522324822,\n",
       " -0.08827322211014743,\n",
       " -0.5351334023327143,\n",
       " 0.6933543717814509,\n",
       " 0.3622629679114403,\n",
       " 0.5671416394516889,\n",
       " -0.04512022164254078,\n",
       " 0.76026862822394,\n",
       " 0.21321395984563724,\n",
       " -0.2590993687091407,\n",
       " -0.6671892671586797,\n",
       " 0.47027257002547096,\n",
       " 0.6859339116559185,\n",
       " 0.7362612059583118,\n",
       " 0.053401967560008624,\n",
       " 0.31359209893178164,\n",
       " 0.7562899219053538,\n",
       " 0.8431776575284952,\n",
       " 0.8141176299298686,\n",
       " 0.7512599667768375,\n",
       " -0.3466308413268144,\n",
       " 0.930109872606289,\n",
       " 0.7419880814155124,\n",
       " 0.7092803991029425,\n",
       " 0.5040408474894955,\n",
       " 0.13344163475408297,\n",
       " -0.16010705226939448,\n",
       " -0.7456594244589078,\n",
       " -0.6619600108406326,\n",
       " 0.4547271548470399,\n",
       " 0.5390021729355676,\n",
       " 0.9035683089761568,\n",
       " 0.8538291653997612,\n",
       " 0.8594314085340041,\n",
       " -0.37169693738279636,\n",
       " 0.6905594630933294,\n",
       " 0.735411630765475,\n",
       " 0.23176966157858897,\n",
       " -0.4204613290099094,\n",
       " 0.7813179508691492,\n",
       " 0.5958124802184824,\n",
       " 0.5702982200656339,\n",
       " 0.635226779162021,\n",
       " 0.6796036657911168,\n",
       " 0.40715882576344337,\n",
       " -0.6380159901263028,\n",
       " 0.4309844092398109,\n",
       " 0.28419336995335664,\n",
       " 0.42215315339815823,\n",
       " -0.6482448216586881,\n",
       " -0.7025855487609058,\n",
       " 0.6453413328664136,\n",
       " 0.6472635726498662,\n",
       " -0.5956015687027525,\n",
       " 0.9215178258757774,\n",
       " 0.24553930129849844,\n",
       " -0.2463914406417642,\n",
       " 0.7722418519533092,\n",
       " 0.6895129643805178,\n",
       " 0.2532098076449443,\n",
       " 0.20008121998671577,\n",
       " -0.7172624876033421,\n",
       " -0.6682188182277019,\n",
       " 0.8643895272948089,\n",
       " -0.44800167225819476,\n",
       " 0.26701541964550907,\n",
       " 0.9042628994534163,\n",
       " 0.5038975099179114,\n",
       " -0.4205986307397163,\n",
       " 0.8120669762970103,\n",
       " 0.09140765540888325,\n",
       " 0.7215833861522679,\n",
       " 0.6334980562931806,\n",
       " -0.676053911242338,\n",
       " 0.13827274728427724,\n",
       " -0.39258081973708947,\n",
       " 0.7486085223924842,\n",
       " 0.6899886628600598,\n",
       " 0.10275115918908428,\n",
       " 0.07174144288747816,\n",
       " -0.16579950547731492,\n",
       " 0.7633177897341096,\n",
       " 0.6123244560420855,\n",
       " 0.700077679774312,\n",
       " 0.02176756399372964,\n",
       " 0.437965977652074,\n",
       " 0.34459874977236243,\n",
       " 0.10109241016128812,\n",
       " 0.13905742352803868,\n",
       " -0.3170012487806059,\n",
       " 0.4921291842679593,\n",
       " 0.7985905849450822,\n",
       " 0.5910381576332175,\n",
       " 0.8861326768605261,\n",
       " 0.8228379756695134,\n",
       " 0.3138401972938004,\n",
       " 0.5909610626585093,\n",
       " -0.1336809933821165,\n",
       " 0.2652559249247125,\n",
       " -0.7023552545863717,\n",
       " 0.07339817554784,\n",
       " 0.3723781077047875,\n",
       " -0.03383381911726056,\n",
       " 0.8626354958891335,\n",
       " 0.4978876295551315,\n",
       " 0.20827228329591144,\n",
       " 0.5349403182699085,\n",
       " -0.10523425131397555,\n",
       " -0.32900861202206383,\n",
       " 0.7568848859076764,\n",
       " 0.7921798651488986,\n",
       " 0.679971128219235,\n",
       " 0.5623033243082862,\n",
       " -0.10164853945493901,\n",
       " 0.34150081636651963,\n",
       " 0.4588529829008549,\n",
       " 0.1113324053554676,\n",
       " 0.7879568658322024,\n",
       " 0.6530180210795051,\n",
       " 0.7059562083817965,\n",
       " 0.8413355594329078,\n",
       " -0.350094852361927,\n",
       " 0.8581880066517713,\n",
       " 0.3399741741748342,\n",
       " 0.7432251484246537,\n",
       " -0.33147188586668713,\n",
       " -0.6771285124095977,\n",
       " -0.07504114498201221,\n",
       " -0.013451437403227106,\n",
       " -0.49818643466044843,\n",
       " 0.44554923986694894,\n",
       " 0.7385477861199294,\n",
       " 0.7593985416666096,\n",
       " 0.3514493706314797,\n",
       " 0.3820866517643823,\n",
       " 0.32725079697536297,\n",
       " 0.6186756243065884,\n",
       " 0.3506032239662501,\n",
       " -0.37820157740379556,\n",
       " 0.6652238000719799,\n",
       " 0.5627946017776393,\n",
       " 0.36024369600679285,\n",
       " 0.6215322340017541,\n",
       " 0.723521563463058,\n",
       " 0.8519130336564903,\n",
       " 0.792224643839181,\n",
       " -0.004340321779009023,\n",
       " 0.20578872233686832,\n",
       " 0.42724006089646843,\n",
       " 0.8276012782680107,\n",
       " 0.3457876352514161,\n",
       " 0.8306674644202421,\n",
       " 0.5718780847248048,\n",
       " -0.6602106122359358,\n",
       " 0.4081733245153663,\n",
       " 0.3049021127387052,\n",
       " 0.03578986288783648,\n",
       " 0.602206472073504,\n",
       " 0.5814431363650565,\n",
       " 0.23392347217509357,\n",
       " -0.3334822912723271,\n",
       " 0.6084255223749163,\n",
       " -0.17491994152509724,\n",
       " 0.23799642950027858,\n",
       " -0.474377058936362,\n",
       " 0.4987712601342953,\n",
       " 0.7104344968849056,\n",
       " -0.6334688168826003,\n",
       " 0.7738486105599959,\n",
       " -0.17901729273790348,\n",
       " 0.7726916803053746,\n",
       " 0.8149539050821022,\n",
       " 0.7492993853454513,\n",
       " 0.3846231878548445,\n",
       " -0.0904115410957339,\n",
       " 0.9080022162344,\n",
       " 0.3841522630265214,\n",
       " 0.6182572189223268,\n",
       " 0.5663726029887948,\n",
       " -0.009401950848238505,\n",
       " 0.7072844078639586,\n",
       " 0.5173035387625594,\n",
       " 0.8177255820419328,\n",
       " 0.6838603521245389,\n",
       " 0.7705556258885183,\n",
       " 0.3530513742941498,\n",
       " -0.29849122831554686,\n",
       " 0.5590967478884368,\n",
       " -0.0414833938060299,\n",
       " 0.6767984883365544,\n",
       " -0.006306036864316806,\n",
       " 0.008642384431176452,\n",
       " 0.17236602214431654,\n",
       " 0.46942509693487156,\n",
       " 0.7070516906025579,\n",
       " 0.3073960836174861,\n",
       " 0.47229324525931576,\n",
       " 0.8612292699169519,\n",
       " 0.8392416008338657,\n",
       " 0.502319992979444,\n",
       " 0.6359459626584691,\n",
       " 0.7811324394769056,\n",
       " 0.4213913241963056,\n",
       " 0.03690981834678145,\n",
       " -0.2687525845995386,\n",
       " 0.6851374257197308,\n",
       " 0.42491635440799547,\n",
       " 0.18526655392855024,\n",
       " -0.4228056781866933,\n",
       " 0.3549707203647534,\n",
       " 0.2154797333035421,\n",
       " -0.16944588388397108,\n",
       " -0.21459058419913102,\n",
       " 0.75462781724364,\n",
       " 0.3780690596523212,\n",
       " 0.5183778742449959,\n",
       " 0.7996666310101442,\n",
       " 0.8843603409691079,\n",
       " -0.38358456290064025,\n",
       " 0.23609749473407615,\n",
       " 0.551526122899849,\n",
       " -0.8321394588990743,\n",
       " -0.16579352686094467,\n",
       " -0.24288679795512078,\n",
       " 0.095228144009022,\n",
       " 0.10628935033857985,\n",
       " 0.6596208030584539,\n",
       " 0.5358547823504624,\n",
       " 0.4246259978434461,\n",
       " 0.2915141054914974,\n",
       " 0.8687520878484482,\n",
       " -0.266731782920324,\n",
       " 0.7336549811907501,\n",
       " 0.8337414759460374,\n",
       " 0.3338897499973755,\n",
       " 0.7350983220520118,\n",
       " ...]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6baad4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27afdbf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x148f8b59cd60>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 320x320 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 288,
       "width": 317
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e99555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_new.var['pval'] = pval_list\n",
    "adata_new.var['cor'] = cor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b17e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_new.write_h5ad(\"spatial_imputation.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d25ac71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 324 × 11968\n",
       "    obs: 'in_tissue', 'array_row', 'array_col', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes', 'total_counts_MT', 'log1p_total_counts_MT', 'pct_counts_MT', 'n_counts', 'leiden', 'cluster', 'batch', 'celltype', 'str_batch', 'batch_id'\n",
       "    var: 'gene_ids', 'feature_types', 'genome', 'MT', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm', 'gene_name', 'id_in_vocab', 'n_counts', 'pval', 'cor'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c237cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04585cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76ade565",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_new.write_h5ad(\"spatial_impuration.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb97ce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.scale(adata_new) \n",
    "sc.tl.pca(adata_new)\n",
    "sc.pp.neighbors(adata_new)\n",
    "sc.tl.umap(adata_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf9868d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d33d5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f632f421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce86b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b5d98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
